{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Dataset: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 50 are the primary variables including all of the dummy variables. The next 560 are interactions with the dummy variables. The final variables are the adjacency variables that aren't in the primary variable section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"7d531d82-6efc-4898-8579-dc8748fdde87\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(global) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = \"1\";\n",
       "\n",
       "  if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force !== \"\") {\n",
       "    window._bokeh_onload_callbacks = [];\n",
       "    window._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "\n",
       "  \n",
       "  if (typeof (window._bokeh_timeout) === \"undefined\" || force !== \"\") {\n",
       "    window._bokeh_timeout = Date.now() + 5000;\n",
       "    window._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    if (window.Bokeh !== undefined) {\n",
       "      Bokeh.$(\"#7d531d82-6efc-4898-8579-dc8748fdde87\").text(\"BokehJS successfully loaded.\");\n",
       "    } else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    delete window._bokeh_onload_callbacks\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    window._bokeh_onload_callbacks.push(callback);\n",
       "    if (window._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    window._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        window._bokeh_is_loading--;\n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"7d531d82-6efc-4898-8579-dc8748fdde87\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '7d531d82-6efc-4898-8579-dc8748fdde87' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = ['https://cdn.pydata.org/bokeh/release/bokeh-0.12.2.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.2.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-compiler-0.12.2.min.js'];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "      Bokeh.$(\"#7d531d82-6efc-4898-8579-dc8748fdde87\").text(\"BokehJS is loading...\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.2.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.2.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((window.Bokeh !== undefined) || (force === \"1\")) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i](window.Bokeh);\n",
       "      }if (force === \"1\") {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!window._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      window._bokeh_failed_load = true;\n",
       "    } else if (!force) {\n",
       "      var cell = $(\"#7d531d82-6efc-4898-8579-dc8748fdde87\").parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (window._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(this));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "import bokeh\n",
    "\n",
    "from bokeh.io import show, output_notebook, output_file\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.charts import Bar\n",
    "from bokeh.charts.attributes import cat\n",
    "from bokeh.models import HoverTool\n",
    "output_notebook ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124011, 825)\n"
     ]
    }
   ],
   "source": [
    "combined = pd.read_csv(\"new data/Final_Project_Variables - combined.csv\")\n",
    "print(combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_all = pd.read_csv(\"new data/Final_Project_Variables - train.csv\")\n",
    "locations = pd.read_csv(\"new data/Google Locations - train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>observation</th>\n",
       "      <th>listing_id</th>\n",
       "      <th>interest_code</th>\n",
       "      <th>days_between_adj_5</th>\n",
       "      <th>log_price_adj_3</th>\n",
       "      <th>interest_adj_13</th>\n",
       "      <th>feature_count_adj_25</th>\n",
       "      <th>bathrooms_adj_4</th>\n",
       "      <th>descript_len_adj_11</th>\n",
       "      <th>bedrooms_adj_3</th>\n",
       "      <th>...</th>\n",
       "      <th>bathrooms_adj_22</th>\n",
       "      <th>bathrooms_adj_23</th>\n",
       "      <th>bathrooms_adj_24</th>\n",
       "      <th>bathrooms_adj_25</th>\n",
       "      <th>bathrooms_adj_26</th>\n",
       "      <th>bathrooms_adj_27</th>\n",
       "      <th>bathrooms_adj_28</th>\n",
       "      <th>bathrooms_adj_29</th>\n",
       "      <th>bathrooms_adj_30</th>\n",
       "      <th>location_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>7170325</td>\n",
       "      <td>2</td>\n",
       "      <td>1318</td>\n",
       "      <td>23.485804</td>\n",
       "      <td>19</td>\n",
       "      <td>130</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5398</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>7092344</td>\n",
       "      <td>1</td>\n",
       "      <td>1173</td>\n",
       "      <td>24.941265</td>\n",
       "      <td>18</td>\n",
       "      <td>168</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7174</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>7158677</td>\n",
       "      <td>2</td>\n",
       "      <td>1170</td>\n",
       "      <td>24.302001</td>\n",
       "      <td>21</td>\n",
       "      <td>127</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6583</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>7211212</td>\n",
       "      <td>2</td>\n",
       "      <td>1189</td>\n",
       "      <td>23.654460</td>\n",
       "      <td>15</td>\n",
       "      <td>46</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5388</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>7225292</td>\n",
       "      <td>1</td>\n",
       "      <td>1207</td>\n",
       "      <td>25.326354</td>\n",
       "      <td>17</td>\n",
       "      <td>195</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6230</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>25.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 826 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   observation  listing_id  interest_code  days_between_adj_5  \\\n",
       "0            4     7170325              2                1318   \n",
       "1            6     7092344              1                1173   \n",
       "2            9     7158677              2                1170   \n",
       "3           10     7211212              2                1189   \n",
       "4           15     7225292              1                1207   \n",
       "\n",
       "   log_price_adj_3  interest_adj_13  feature_count_adj_25  bathrooms_adj_4  \\\n",
       "0        23.485804               19                   130              4.0   \n",
       "1        24.941265               18                   168              6.0   \n",
       "2        24.302001               21                   127              4.0   \n",
       "3        23.654460               15                    46              4.5   \n",
       "4        25.326354               17                   195              6.0   \n",
       "\n",
       "   descript_len_adj_11  bedrooms_adj_3      ...        bathrooms_adj_22  \\\n",
       "0                 5398               4      ...                    22.0   \n",
       "1                 7174               7      ...                    24.0   \n",
       "2                 6583               5      ...                    26.0   \n",
       "3                 5388               5      ...                    24.0   \n",
       "4                 6230               5      ...                    25.0   \n",
       "\n",
       "   bathrooms_adj_23  bathrooms_adj_24  bathrooms_adj_25  bathrooms_adj_26  \\\n",
       "0              23.0              24.0              25.0              26.0   \n",
       "1              26.0              27.0              28.0              29.0   \n",
       "2              27.0              28.0              31.0              34.0   \n",
       "3              25.0              26.0              27.0              28.0   \n",
       "4              26.0              27.0              28.0              29.0   \n",
       "\n",
       "   bathrooms_adj_27  bathrooms_adj_28  bathrooms_adj_29  bathrooms_adj_30  \\\n",
       "0              27.0              28.0              29.0              30.0   \n",
       "1              30.0              31.0              33.0              34.0   \n",
       "2              35.0              36.0              37.0              38.0   \n",
       "3              29.0              30.0              31.0              32.0   \n",
       "4              30.0              31.0              32.0              33.0   \n",
       "\n",
       "   location_code  \n",
       "0              8  \n",
       "1              2  \n",
       "2             15  \n",
       "3              8  \n",
       "4              2  \n",
       "\n",
       "[5 rows x 826 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all = train_all.merge(locations[[\"observation\",\"location_code\"]], on=\"observation\")\n",
    "train_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_all[\"observation.1\"]\n",
    "del train_all[\"listing_id.1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 824)\n"
     ]
    }
   ],
   "source": [
    "print(train_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74659, 824)\n"
     ]
    }
   ],
   "source": [
    "holdout = pd.read_csv(\"new data/Final_Project_Variables - test.csv\")\n",
    "locations_test = pd.read_csv(\"new data/Google Locations - test.csv\")\n",
    "holdout = holdout.merge(locations_test[[\"observation\",\"location_code\"]], on=\"observation\")\n",
    "del holdout[\"observation.1\"]\n",
    "del holdout[\"listing_id.1\"]\n",
    "print(holdout.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build dataset with reduced adjacencies and no interaction terms: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_all_small = pd.read_csv(\"new data/no_corr_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_all_small = train_all_small .merge(locations[[\"observation\",\"location_code\"]], on=\"observation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>observation</th>\n",
       "      <th>listing_id</th>\n",
       "      <th>interest_code</th>\n",
       "      <th>mgmt_list_count</th>\n",
       "      <th>feature_count</th>\n",
       "      <th>descript_len</th>\n",
       "      <th>photo_count</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>log_price</th>\n",
       "      <th>...</th>\n",
       "      <th>photo_count_adj_30</th>\n",
       "      <th>log_price_adj_3</th>\n",
       "      <th>log_price_adj_12</th>\n",
       "      <th>bedrooms_adj_1</th>\n",
       "      <th>bedrooms_adj_3</th>\n",
       "      <th>bathrooms_adj_1</th>\n",
       "      <th>bathrooms_adj_4</th>\n",
       "      <th>bathrooms_adj_7</th>\n",
       "      <th>bathrooms_adj_25</th>\n",
       "      <th>location_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>7170325</td>\n",
       "      <td>2</td>\n",
       "      <td>294</td>\n",
       "      <td>7</td>\n",
       "      <td>530</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.783224</td>\n",
       "      <td>...</td>\n",
       "      <td>235</td>\n",
       "      <td>23.485804</td>\n",
       "      <td>93.244562</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>7092344</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>6</td>\n",
       "      <td>804</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.242756</td>\n",
       "      <td>...</td>\n",
       "      <td>131</td>\n",
       "      <td>24.941265</td>\n",
       "      <td>98.428382</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>7158677</td>\n",
       "      <td>2</td>\n",
       "      <td>265</td>\n",
       "      <td>6</td>\n",
       "      <td>776</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.159089</td>\n",
       "      <td>...</td>\n",
       "      <td>136</td>\n",
       "      <td>24.302001</td>\n",
       "      <td>98.417272</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>7211212</td>\n",
       "      <td>2</td>\n",
       "      <td>235</td>\n",
       "      <td>0</td>\n",
       "      <td>565</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>8.006368</td>\n",
       "      <td>...</td>\n",
       "      <td>148</td>\n",
       "      <td>23.654460</td>\n",
       "      <td>93.360586</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>7.5</td>\n",
       "      <td>27.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>7225292</td>\n",
       "      <td>1</td>\n",
       "      <td>135</td>\n",
       "      <td>4</td>\n",
       "      <td>321</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.935587</td>\n",
       "      <td>...</td>\n",
       "      <td>153</td>\n",
       "      <td>25.326354</td>\n",
       "      <td>98.639056</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   observation  listing_id  interest_code  mgmt_list_count  feature_count  \\\n",
       "0            4     7170325              2              294              7   \n",
       "1            6     7092344              1               64              6   \n",
       "2            9     7158677              2              265              6   \n",
       "3           10     7211212              2              235              0   \n",
       "4           15     7225292              1              135              4   \n",
       "\n",
       "   descript_len  photo_count  bedrooms  bathrooms  log_price      ...        \\\n",
       "0           530           12         1        1.0   7.783224      ...         \n",
       "1           804            6         2        1.0   8.242756      ...         \n",
       "2           776            6         2        1.0   8.159089      ...         \n",
       "3           565            5         3        1.5   8.006368      ...         \n",
       "4           321            4         0        1.0   7.935587      ...         \n",
       "\n",
       "   photo_count_adj_30  log_price_adj_3  log_price_adj_12  bedrooms_adj_1  \\\n",
       "0                 235        23.485804         93.244562               1   \n",
       "1                 131        24.941265         98.428382               2   \n",
       "2                 136        24.302001         98.417272               2   \n",
       "3                 148        23.654460         93.360586               1   \n",
       "4                 153        25.326354         98.639056               2   \n",
       "\n",
       "   bedrooms_adj_3  bathrooms_adj_1  bathrooms_adj_4  bathrooms_adj_7  \\\n",
       "0               4              1.0              4.0              7.0   \n",
       "1               7              1.0              6.0              9.0   \n",
       "2               5              1.0              4.0              7.0   \n",
       "3               5              1.0              4.5              7.5   \n",
       "4               5              2.0              6.0             10.0   \n",
       "\n",
       "   bathrooms_adj_25  location_code  \n",
       "0              25.0              8  \n",
       "1              28.0              2  \n",
       "2              31.0             15  \n",
       "3              27.0              8  \n",
       "4              28.0              2  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#delete random extra column: \n",
    "del train_all_small[\"Unnamed: 0\"]\n",
    "train_all_small.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "holdout_small = pd.read_csv(\"new data/no_corr_test.csv\")\n",
    "holdout_small = holdout_small.merge(locations_test[[\"observation\",\"location_code\"]], on=\"observation\")\n",
    "#delete random extra column: \n",
    "del holdout_small[\"Unnamed: 0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Outliers* (all numbers expressed as indices in the training set, not as observation ids)\n",
    "\n",
    "- Feature_count         27648 28418\n",
    "- Description length    48522\n",
    "- Bedrooms              4440 10673 11387 41667    (have 7 or 8 bedrooms)\n",
    "- Bathrooms             41576 (has 10 bathrooms)\n",
    "- log price             12948\n",
    "- Distance adj1         6479 29013 31264 41723      6479(definitely remove!)\n",
    "- Desciption len2       13436 17719"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove outliers:\n",
    "outliers = [  27648, 28418,48522, 4440 ,10673, 11387 ,41667, 41576 ,12948,6479, 29013, 31264, 41723, 6479,13436, 17719 ]\n",
    "outliers = [x-1 for x in outliers]\n",
    "\n",
    "train_all_small = train_all_small.drop(outliers)\n",
    "train_all = train_all.drop(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#list(train_all.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All training samples Class Distribution: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of Lows:  69.4711879522468\n",
      "percentage of Mediums:  22.747633621825404\n",
      "percentage of Highs:  7.781178425927802\n"
     ]
    }
   ],
   "source": [
    "interest = train_all[\"interest_code\"]\n",
    "print(\"percentage of Lows: \", len(interest[interest== 1])/len(interest) * 100)\n",
    "print(\"percentage of Mediums: \", len(interest[interest== 2])/len(interest) * 100)\n",
    "print(\"percentage of Highs: \", len(interest[interest== 3])/len(interest) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data prep:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, recode binary variables as -1 vs 1 instead of as 0 vs 1 to center the predictor around 0. \n",
    "\n",
    "\"The difference is due to simple geometry. The initial hyperplanes pass fairly near the origin. If the data are centered near the origin (as with {-1,1} coding), the initial hyperplanes will cut through the data in a variety of directions. If the data are offset from the origin (as with {0,1} coding), many of the initial hyperplanes will miss the data entirely, and those that pass through the data will provide a only a limited range of directions, making it difficult to find local optima that use hyperplanes that go in different directions.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_all_small.loc[train_all_small[\"one\"]==0, \"one\"] = -1\n",
    "train_all_small.loc[train_all_small[\"two\"]==0, \"two\"] = -1\n",
    "train_all_small.loc[train_all_small[\"three\"]==0, \"three\"] = -1\n",
    "train_all_small.loc[train_all_small[\"five\"]==0, \"five\"] = -1\n",
    "train_all_small.loc[train_all_small[\"six\"]==0, \"six\"] = -1\n",
    "train_all_small.loc[train_all_small[\"seven\"]==0, \"seven\"] = -1\n",
    "train_all_small.loc[train_all_small[\"Doorman\"]==0, \"Doorman\"] = -1\n",
    "train_all_small.loc[train_all_small[ \"Dogs_Allowed\"]==0,  \"Dogs_Allowed\"] = -1\n",
    "train_all_small.loc[train_all_small[\"Cats_Allowed\"]==0, \"Cats_Allowed\"] = -1\n",
    "train_all_small.loc[train_all_small[\"Elevator\"  ]==0, \"Elevator\"  ] = -1\n",
    "train_all_small.loc[train_all_small[\"Balcony\" ]==0,\"Balcony\" ] = -1\n",
    "train_all_small.loc[train_all_small[\"Outdoors\"]==0, \"Outdoors\"] = -1\n",
    "train_all_small.loc[train_all_small[\"Hardwood\" ]==0, \"Hardwood\" ] = -1\n",
    "train_all_small.loc[train_all_small[\"Internet_Mention\" ]==0, \"Internet_Mention\" ] = -1\n",
    "train_all_small.loc[train_all_small[\"Loft\" ]==0,\"Loft\"  ] = -1\n",
    "train_all_small.loc[train_all_small[\"New\" ]==0,\"New\"  ] = -1\n",
    "train_all_small.loc[train_all_small[\"No_fee\"  ]==0, \"No_fee\"  ] = -1\n",
    "train_all_small.loc[train_all_small[\"Gym\"  ]==0,\"Gym\"   ] = -1\n",
    "train_all_small.loc[train_all_small[\"Parking\"]==0, \"Parking\"] = -1\n",
    "train_all_small.loc[train_all_small[\"Storage\"]==0, \"Storage\"  ] = -1\n",
    "train_all_small.loc[train_all_small[\"Sunlight\" ]==0,\"Sunlight\"  ] = -1\n",
    "train_all_small.loc[train_all_small[\"Pool\"   ]==0, \"Pool\"   ] = -1\n",
    "train_all_small.loc[train_all_small[ \"Laundry_Room\"  ]==0, \"Laundry_Room\"   ] = -1\n",
    "train_all_small.loc[train_all_small[\"Wash_Dry\"   ]==0, \"Wash_Dry\"   ] = -1\n",
    "train_all_small.loc[train_all_small[\"Dishwasher\" ]==0,\"Dishwasher\"  ] = -1\n",
    "train_all_small.loc[train_all_small[ \"Prewar\" ]==0,  \"Prewar\" ] = -1\n",
    "train_all_small.loc[train_all_small[\"AirCon\"]==0, \"AirCon\"   ] = -1\n",
    "\n",
    "\n",
    "holdout_small.loc[holdout_small[\"one\"]==0, \"one\"] = -1\n",
    "holdout_small.loc[holdout_small[\"two\"]==0, \"two\"] = -1\n",
    "holdout_small.loc[holdout_small[\"three\"]==0, \"three\"] = -1\n",
    "holdout_small.loc[holdout_small[\"five\"]==0, \"five\"] = -1\n",
    "holdout_small.loc[holdout_small[\"six\"]==0, \"six\"] = -1\n",
    "holdout_small.loc[holdout_small[\"seven\"]==0, \"seven\"] = -1\n",
    "holdout_small.loc[holdout_small[\"Doorman\"]==0, \"Doorman\"] = -1\n",
    "holdout_small.loc[holdout_small[ \"Dogs_Allowed\"]==0,  \"Dogs_Allowed\"] = -1\n",
    "holdout_small.loc[holdout_small[\"Cats_Allowed\"]==0, \"Cats_Allowed\"] = -1\n",
    "holdout_small.loc[holdout_small[\"Elevator\"  ]==0, \"Elevator\"  ] = -1\n",
    "holdout_small.loc[holdout_small[\"Balcony\" ]==0,\"Balcony\" ] = -1\n",
    "holdout_small.loc[holdout_small[\"Outdoors\"]==0, \"Outdoors\"] = -1\n",
    "holdout_small.loc[holdout_small[\"Hardwood\" ]==0, \"Hardwood\" ] = -1\n",
    "holdout_small.loc[holdout_small[\"Internet_Mention\" ]==0, \"Internet_Mention\" ] = -1\n",
    "holdout_small.loc[holdout_small[\"Loft\" ]==0,\"Loft\"  ] = -1\n",
    "holdout_small.loc[holdout_small[\"New\" ]==0,\"New\"  ] = -1\n",
    "holdout_small.loc[holdout_small[\"No_fee\"  ]==0, \"No_fee\"  ] = -1\n",
    "holdout_small.loc[holdout_small[\"Gym\"  ]==0,\"Gym\"   ] = -1\n",
    "holdout_small.loc[holdout_small[\"Parking\"]==0, \"Parking\"] = -1\n",
    "holdout_small.loc[holdout_small[\"Storage\"]==0, \"Storage\"  ] = -1\n",
    "holdout_small.loc[holdout_small[\"Sunlight\" ]==0,\"Sunlight\"  ] = -1\n",
    "holdout_small.loc[holdout_small[\"Pool\"   ]==0, \"Pool\"   ] = -1\n",
    "holdout_small.loc[holdout_small[ \"Laundry_Room\"  ]==0, \"Laundry_Room\"   ] = -1\n",
    "holdout_small.loc[holdout_small[\"Wash_Dry\"   ]==0, \"Wash_Dry\"   ] = -1\n",
    "holdout_small.loc[holdout_small[\"Dishwasher\" ]==0,\"Dishwasher\"  ] = -1\n",
    "holdout_small.loc[holdout_small[ \"Prewar\" ]==0,  \"Prewar\" ] = -1\n",
    "holdout_small.loc[holdout_small[\"AirCon\"]==0, \"AirCon\"   ] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#do the -1/1 encoding for the full dataset: \n",
    "\n",
    "train_all.loc[train_all[\"one\"]==0, \"one\"] = -1\n",
    "train_all.loc[train_all[\"two\"]==0, \"two\"] = -1\n",
    "train_all.loc[train_all[\"three\"]==0, \"three\"] = -1\n",
    "train_all.loc[train_all[\"five\"]==0, \"five\"] = -1\n",
    "train_all.loc[train_all[\"six\"]==0, \"six\"] = -1\n",
    "train_all.loc[train_all[\"seven\"]==0, \"seven\"] = -1\n",
    "train_all.loc[train_all[\"Doorman\"]==0, \"Doorman\"] = -1\n",
    "train_all.loc[train_all[ \"Dogs_Allowed\"]==0,  \"Dogs_Allowed\"] = -1\n",
    "train_all.loc[train_all[\"Cats_Allowed\"]==0, \"Cats_Allowed\"] = -1\n",
    "train_all.loc[train_all[\"Elevator\"  ]==0, \"Elevator\"  ] = -1\n",
    "train_all.loc[train_all[\"Balcony\" ]==0,\"Balcony\" ] = -1\n",
    "train_all.loc[train_all[\"Outdoors\"]==0, \"Outdoors\"] = -1\n",
    "train_all.loc[train_all[\"Hardwood\" ]==0, \"Hardwood\" ] = -1\n",
    "train_all.loc[train_all[\"Internet_Mention\" ]==0, \"Internet_Mention\" ] = -1\n",
    "train_all.loc[train_all[\"Loft\" ]==0,\"Loft\"  ] = -1\n",
    "train_all.loc[train_all[\"New\" ]==0,\"New\"  ] = -1\n",
    "train_all.loc[train_all[\"No_fee\"  ]==0, \"No_fee\"  ] = -1\n",
    "train_all.loc[train_all[\"Gym\"  ]==0,\"Gym\"   ] = -1\n",
    "train_all.loc[train_all[\"Parking\"]==0, \"Parking\"] = -1\n",
    "train_all.loc[train_all[\"Storage\"]==0, \"Storage\"  ] = -1\n",
    "train_all.loc[train_all[\"Sunlight\" ]==0,\"Sunlight\"  ] = -1\n",
    "train_all.loc[train_all[\"Pool\"   ]==0, \"Pool\"   ] = -1\n",
    "train_all.loc[train_all[ \"Laundry_Room\"  ]==0, \"Laundry_Room\"   ] = -1\n",
    "train_all.loc[train_all[\"Wash_Dry\"   ]==0, \"Wash_Dry\"   ] = -1\n",
    "train_all.loc[train_all[\"Dishwasher\" ]==0,\"Dishwasher\"  ] = -1\n",
    "train_all.loc[train_all[ \"Prewar\" ]==0,  \"Prewar\" ] = -1\n",
    "train_all.loc[train_all[\"AirCon\"]==0, \"AirCon\"   ] = -1\n",
    "\n",
    "\n",
    "holdout.loc[holdout[\"one\"]==0, \"one\"] = -1\n",
    "holdout.loc[holdout[\"two\"]==0, \"two\"] = -1\n",
    "holdout.loc[holdout[\"three\"]==0, \"three\"] = -1\n",
    "holdout.loc[holdout[\"five\"]==0, \"five\"] = -1\n",
    "holdout.loc[holdout[\"six\"]==0, \"six\"] = -1\n",
    "holdout.loc[holdout[\"seven\"]==0, \"seven\"] = -1\n",
    "holdout.loc[holdout[\"Doorman\"]==0, \"Doorman\"] = -1\n",
    "holdout.loc[holdout[ \"Dogs_Allowed\"]==0,  \"Dogs_Allowed\"] = -1\n",
    "holdout.loc[holdout[\"Cats_Allowed\"]==0, \"Cats_Allowed\"] = -1\n",
    "holdout.loc[holdout[\"Elevator\"  ]==0, \"Elevator\"  ] = -1\n",
    "holdout.loc[holdout[\"Balcony\" ]==0,\"Balcony\" ] = -1\n",
    "holdout.loc[holdout[\"Outdoors\"]==0, \"Outdoors\"] = -1\n",
    "holdout.loc[holdout[\"Hardwood\" ]==0, \"Hardwood\" ] = -1\n",
    "holdout.loc[holdout[\"Internet_Mention\" ]==0, \"Internet_Mention\" ] = -1\n",
    "holdout.loc[holdout[\"Loft\" ]==0,\"Loft\"  ] = -1\n",
    "holdout.loc[holdout[\"New\" ]==0,\"New\"  ] = -1\n",
    "holdout.loc[holdout[\"No_fee\"  ]==0, \"No_fee\"  ] = -1\n",
    "holdout.loc[holdout[\"Gym\"  ]==0,\"Gym\"   ] = -1\n",
    "holdout.loc[holdout[\"Parking\"]==0, \"Parking\"] = -1\n",
    "holdout.loc[holdout[\"Storage\"]==0, \"Storage\"  ] = -1\n",
    "holdout.loc[holdout[\"Sunlight\" ]==0,\"Sunlight\"  ] = -1\n",
    "holdout.loc[holdout[\"Pool\"   ]==0, \"Pool\"   ] = -1\n",
    "holdout.loc[holdout[ \"Laundry_Room\"  ]==0, \"Laundry_Room\"   ] = -1\n",
    "holdout.loc[holdout[\"Wash_Dry\"   ]==0, \"Wash_Dry\"   ] = -1\n",
    "holdout.loc[holdout[\"Dishwasher\" ]==0,\"Dishwasher\"  ] = -1\n",
    "holdout.loc[holdout[ \"Prewar\" ]==0,  \"Prewar\" ] = -1\n",
    "holdout.loc[holdout[\"AirCon\"]==0, \"AirCon\"   ] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Because MLP is sensitive to scaling, scale all the continuous predictors \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "#This custom function will scale only the continuous predictors:\n",
    "class CustomScaler(BaseEstimator,TransformerMixin): \n",
    "    def __init__(self,columns,copy=True,with_mean=True,with_std=True):\n",
    "        self.scaler = StandardScaler(copy,with_mean,with_std)\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler.fit(X[self.columns], y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, copy=None):\n",
    "        init_col_order = X.columns\n",
    "        X_scaled = pd.DataFrame(self.scaler.transform(X[self.columns]), columns=self.columns, index=X.index)\n",
    "        X_not_scaled = X.ix[:,~X.columns.isin(self.columns)]\n",
    "        return pd.concat([X_not_scaled, X_scaled], axis=1)[init_col_order]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scale all the predictors in the train and holdout data:\n",
    "\n",
    "continuous_cols =  ['mgmt_list_count','feature_count','descript_len','photo_count','bedrooms','bathrooms','log_price',\n",
    " 'log_days_between','website','interest_adj_13','interest_adj_1', 'interest_adj_2','interest_adj_3',\n",
    " 'distance_adj_1','distance_adj_2','distance_adj_30','days_between_adj_2','days_between_adj_3',\n",
    " 'days_between_adj_30','feature_count_adj_25','descript_len_adj_1','descript_len_adj_2','descript_len_adj_11',\n",
    " 'descript_len_adj_27','photo_count_adj_1','photo_count_adj_30','log_price_adj_3','log_price_adj_12',\n",
    " 'bedrooms_adj_1','bedrooms_adj_3','bathrooms_adj_1','bathrooms_adj_4','bathrooms_adj_7','bathrooms_adj_25']\n",
    "\n",
    "scale = CustomScaler(columns=continuous_cols )\n",
    "\n",
    "train_all_scale_small = scale.fit_transform(train_all_small)\n",
    "holdout_scale_small = scale.fit_transform(holdout_small)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "continuous_cols2 = ['days_between_adj_5','log_price_adj_3','interest_adj_13','feature_count_adj_25','bathrooms_adj_4',\n",
    " 'descript_len_adj_11','bedrooms_adj_3','mgmt_list_count','feature_count','descript_len','photo_count','bedrooms','bathrooms','log_price','log_price_sq',\n",
    " 'log_price__interest_adj_13', 'log_days_between','website','days_between_adj_5__website',\n",
    " 'days_between_adj_5__one',  'days_between_adj_5__two', 'days_between_adj_5__three',\n",
    " 'days_between_adj_5__four',\n",
    " 'days_between_adj_5__five',\n",
    " 'days_between_adj_5__six',\n",
    " 'days_between_adj_5__seven',\n",
    " 'days_between_adj_5__Doorman',\n",
    " 'days_between_adj_5__Dogs_Allowed',\n",
    " 'days_between_adj_5__Cats_Allowed',\n",
    " 'days_between_adj_5__Pets_Allowed',\n",
    " 'days_between_adj_5__Elevator',\n",
    " 'days_between_adj_5__Balcony',\n",
    " 'days_between_adj_5__Outdoors',\n",
    " 'days_between_adj_5__Hardwood',\n",
    " 'days_between_adj_5__Internet_Mention',\n",
    " 'days_between_adj_5__Loft',\n",
    " 'days_between_adj_5__New',\n",
    " 'days_between_adj_5__No_fee',\n",
    " 'days_between_adj_5__Gym',\n",
    " 'days_between_adj_5__Parking',\n",
    " 'days_between_adj_5__Storage',\n",
    " 'days_between_adj_5__Sunlight',\n",
    " 'days_between_adj_5__Pool',\n",
    " 'days_between_adj_5__Laundry_Room',\n",
    " 'days_between_adj_5__Wash_Dry',\n",
    " 'days_between_adj_5__Dishwasher',\n",
    " 'days_between_adj_5__Prewar',\n",
    " 'days_between_adj_5__AirCon',\n",
    " 'log_price_adj_3__website',\n",
    " 'log_price_adj_3__one',\n",
    " 'log_price_adj_3__two',\n",
    " 'log_price_adj_3__three',\n",
    " 'log_price_adj_3__four',\n",
    " 'log_price_adj_3__five',\n",
    " 'log_price_adj_3__six', 'log_price_adj_3__seven', 'log_price_adj_3__Doorman', 'log_price_adj_3__Dogs_Allowed', 'log_price_adj_3__Cats_Allowed',\n",
    " 'log_price_adj_3__Pets_Allowed', 'log_price_adj_3__Elevator',\n",
    " 'log_price_adj_3__Balcony',\n",
    " 'log_price_adj_3__Outdoors',\n",
    " 'log_price_adj_3__Hardwood',\n",
    " 'log_price_adj_3__Internet_Mention',\n",
    " 'log_price_adj_3__Loft',\n",
    " 'log_price_adj_3__New',\n",
    " 'log_price_adj_3__No_fee',\n",
    " 'log_price_adj_3__Gym',\n",
    " 'log_price_adj_3__Parking',\n",
    " 'log_price_adj_3__Storage',\n",
    " 'log_price_adj_3__Sunlight',\n",
    " 'log_price_adj_3__Pool',\n",
    " 'log_price_adj_3__Laundry_Room',\n",
    " 'log_price_adj_3__Wash_Dry',\n",
    " 'log_price_adj_3__Dishwasher',\n",
    " 'log_price_adj_3__Prewar',\n",
    " 'log_price_adj_3__AirCon',\n",
    " 'interest_adj_13__website',\n",
    " 'interest_adj_13__one',\n",
    " 'interest_adj_13__two',\n",
    " 'interest_adj_13__three',\n",
    " 'interest_adj_13__four',\n",
    " 'interest_adj_13__five',\n",
    " 'interest_adj_13__six',\n",
    " 'interest_adj_13__seven',\n",
    " 'interest_adj_13__Doorman',\n",
    " 'interest_adj_13__Dogs_Allowed',\n",
    " 'interest_adj_13__Cats_Allowed',\n",
    " 'interest_adj_13__Pets_Allowed',\n",
    " 'interest_adj_13__Elevator',\n",
    " 'interest_adj_13__Balcony',\n",
    " 'interest_adj_13__Outdoors',\n",
    " 'interest_adj_13__Hardwood',\n",
    " 'interest_adj_13__Internet_Mention',\n",
    " 'interest_adj_13__Loft',\n",
    " 'interest_adj_13__New',\n",
    " 'interest_adj_13__No_fee',\n",
    " 'interest_adj_13__Gym',\n",
    " 'interest_adj_13__Parking',\n",
    " 'interest_adj_13__Storage',\n",
    " 'interest_adj_13__Sunlight',\n",
    " 'interest_adj_13__Pool',\n",
    " 'interest_adj_13__Laundry_Room',\n",
    " 'interest_adj_13__Wash_Dry',\n",
    " 'interest_adj_13__Dishwasher',\n",
    " 'interest_adj_13__Prewar',\n",
    " 'interest_adj_13__AirCon',\n",
    " 'feature_count_adj_25__website',\n",
    " 'feature_count_adj_25__one',\n",
    " 'feature_count_adj_25__two',\n",
    " 'feature_count_adj_25__three',\n",
    " 'feature_count_adj_25__four',\n",
    " 'feature_count_adj_25__five',\n",
    " 'feature_count_adj_25__six',\n",
    " 'feature_count_adj_25__seven',\n",
    " 'feature_count_adj_25__Doorman',\n",
    " 'feature_count_adj_25__Dogs_Allowed',\n",
    " 'feature_count_adj_25__Cats_Allowed',\n",
    " 'feature_count_adj_25__Pets_Allowed',\n",
    " 'feature_count_adj_25__Elevator',\n",
    " 'feature_count_adj_25__Balcony',\n",
    " 'feature_count_adj_25__Outdoors',\n",
    " 'feature_count_adj_25__Hardwood',\n",
    " 'feature_count_adj_25__Internet_Mention',\n",
    " 'feature_count_adj_25__Loft',\n",
    " 'feature_count_adj_25__New',\n",
    " 'feature_count_adj_25__No_fee',\n",
    " 'feature_count_adj_25__Gym',\n",
    " 'feature_count_adj_25__Parking',\n",
    " 'feature_count_adj_25__Storage',\n",
    " 'feature_count_adj_25__Sunlight',\n",
    " 'feature_count_adj_25__Pool',\n",
    " 'feature_count_adj_25__Laundry_Room',\n",
    " 'feature_count_adj_25__Wash_Dry',\n",
    " 'feature_count_adj_25__Dishwasher',\n",
    " 'feature_count_adj_25__Prewar',\n",
    " 'feature_count_adj_25__AirCon',\n",
    " 'bathrooms_adj_4__website',\n",
    " 'bathrooms_adj_4__one',\n",
    " 'bathrooms_adj_4__two',\n",
    " 'bathrooms_adj_4__three',\n",
    " 'bathrooms_adj_4__four',\n",
    " 'bathrooms_adj_4__five',\n",
    " 'bathrooms_adj_4__six',\n",
    " 'bathrooms_adj_4__seven',\n",
    " 'bathrooms_adj_4__Doorman',\n",
    " 'bathrooms_adj_4__Dogs_Allowed',\n",
    " 'bathrooms_adj_4__Cats_Allowed',\n",
    " 'bathrooms_adj_4__Pets_Allowed',\n",
    " 'bathrooms_adj_4__Elevator',\n",
    " 'bathrooms_adj_4__Balcony',\n",
    " 'bathrooms_adj_4__Outdoors',\n",
    " 'bathrooms_adj_4__Hardwood',\n",
    " 'bathrooms_adj_4__Internet_Mention',\n",
    " 'bathrooms_adj_4__Loft',\n",
    " 'bathrooms_adj_4__New',\n",
    " 'bathrooms_adj_4__No_fee',\n",
    " 'bathrooms_adj_4__Gym',\n",
    " 'bathrooms_adj_4__Parking',\n",
    " 'bathrooms_adj_4__Storage',\n",
    " 'bathrooms_adj_4__Sunlight',\n",
    " 'bathrooms_adj_4__Pool',\n",
    " 'bathrooms_adj_4__Laundry_Room',\n",
    " 'bathrooms_adj_4__Wash_Dry',\n",
    " 'bathrooms_adj_4__Dishwasher',\n",
    " 'bathrooms_adj_4__Prewar',\n",
    " 'bathrooms_adj_4__AirCon',\n",
    " 'descript_len_adj_11__website',\n",
    " 'descript_len_adj_11__one',\n",
    " 'descript_len_adj_11__two',\n",
    " 'descript_len_adj_11__three',\n",
    " 'descript_len_adj_11__four',\n",
    " 'descript_len_adj_11__five',\n",
    " 'descript_len_adj_11__six',\n",
    " 'descript_len_adj_11__seven',\n",
    " 'descript_len_adj_11__Doorman',\n",
    " 'descript_len_adj_11__Dogs_Allowed',\n",
    " 'descript_len_adj_11__Cats_Allowed',\n",
    " 'descript_len_adj_11__Pets_Allowed',\n",
    " 'descript_len_adj_11__Elevator',\n",
    " 'descript_len_adj_11__Balcony',\n",
    " 'descript_len_adj_11__Outdoors',\n",
    " 'descript_len_adj_11__Hardwood',\n",
    " 'descript_len_adj_11__Internet_Mention',\n",
    " 'descript_len_adj_11__Loft',\n",
    " 'descript_len_adj_11__New',\n",
    " 'descript_len_adj_11__No_fee',\n",
    " 'descript_len_adj_11__Gym',\n",
    " 'descript_len_adj_11__Parking',\n",
    " 'descript_len_adj_11__Storage',\n",
    " 'descript_len_adj_11__Sunlight',\n",
    " 'descript_len_adj_11__Pool',\n",
    " 'descript_len_adj_11__Laundry_Room',\n",
    " 'descript_len_adj_11__Wash_Dry',\n",
    " 'descript_len_adj_11__Dishwasher',\n",
    " 'descript_len_adj_11__Prewar',\n",
    " 'descript_len_adj_11__AirCon',\n",
    " 'bedrooms_adj_3__website',\n",
    " 'bedrooms_adj_3__one',\n",
    " 'bedrooms_adj_3__two',\n",
    " 'bedrooms_adj_3__three',\n",
    " 'bedrooms_adj_3__four',\n",
    " 'bedrooms_adj_3__five',\n",
    " 'bedrooms_adj_3__six',\n",
    " 'bedrooms_adj_3__seven',\n",
    " 'bedrooms_adj_3__Doorman',\n",
    " 'bedrooms_adj_3__Dogs_Allowed',\n",
    " 'bedrooms_adj_3__Cats_Allowed',\n",
    " 'bedrooms_adj_3__Pets_Allowed',\n",
    " 'bedrooms_adj_3__Elevator',\n",
    " 'bedrooms_adj_3__Balcony',\n",
    " 'bedrooms_adj_3__Outdoors',\n",
    " 'bedrooms_adj_3__Hardwood',\n",
    " 'bedrooms_adj_3__Internet_Mention',\n",
    " 'bedrooms_adj_3__Loft',\n",
    " 'bedrooms_adj_3__New',\n",
    " 'bedrooms_adj_3__No_fee',\n",
    " 'bedrooms_adj_3__Gym',\n",
    " 'bedrooms_adj_3__Parking',\n",
    " 'bedrooms_adj_3__Storage',\n",
    " 'bedrooms_adj_3__Sunlight',\n",
    " 'bedrooms_adj_3__Pool',\n",
    " 'bedrooms_adj_3__Laundry_Room',\n",
    " 'bedrooms_adj_3__Wash_Dry',\n",
    " 'bedrooms_adj_3__Dishwasher',\n",
    " 'bedrooms_adj_3__Prewar',\n",
    " 'bedrooms_adj_3__AirCon',\n",
    " 'mgmt_list_count__website',\n",
    " 'mgmt_list_count__one',\n",
    " 'mgmt_list_count__two',\n",
    " 'mgmt_list_count__three',\n",
    " 'mgmt_list_count__four',\n",
    " 'mgmt_list_count__five',\n",
    " 'mgmt_list_count__six',\n",
    " 'mgmt_list_count__seven',\n",
    " 'mgmt_list_count__Doorman',\n",
    " 'mgmt_list_count__Dogs_Allowed',\n",
    " 'mgmt_list_count__Cats_Allowed',\n",
    " 'mgmt_list_count__Pets_Allowed',\n",
    " 'mgmt_list_count__Elevator',\n",
    " 'mgmt_list_count__Balcony',\n",
    " 'mgmt_list_count__Outdoors',\n",
    " 'mgmt_list_count__Hardwood',\n",
    " 'mgmt_list_count__Internet_Mention',\n",
    " 'mgmt_list_count__Loft',\n",
    " 'mgmt_list_count__New',\n",
    " 'mgmt_list_count__No_fee',\n",
    " 'mgmt_list_count__Gym',\n",
    " 'mgmt_list_count__Parking',\n",
    " 'mgmt_list_count__Storage',\n",
    " 'mgmt_list_count__Sunlight',\n",
    " 'mgmt_list_count__Pool',\n",
    " 'mgmt_list_count__Laundry_Room',\n",
    " 'mgmt_list_count__Wash_Dry',\n",
    " 'mgmt_list_count__Dishwasher',\n",
    " 'mgmt_list_count__Prewar',\n",
    " 'mgmt_list_count__AirCon',\n",
    " 'feature_count__website',\n",
    " 'feature_count__one',\n",
    " 'feature_count__two',\n",
    " 'feature_count__three',\n",
    " 'feature_count__four',\n",
    " 'feature_count__five',\n",
    " 'feature_count__six',\n",
    " 'feature_count__seven',\n",
    " 'feature_count__Doorman',\n",
    " 'feature_count__Dogs_Allowed',\n",
    " 'feature_count__Cats_Allowed',\n",
    " 'feature_count__Pets_Allowed',\n",
    " 'feature_count__Elevator',\n",
    " 'feature_count__Balcony',\n",
    " 'feature_count__Outdoors',\n",
    " 'feature_count__Hardwood',\n",
    " 'feature_count__Internet_Mention',\n",
    " 'feature_count__Loft',\n",
    " 'feature_count__New',\n",
    " 'feature_count__No_fee',\n",
    " 'feature_count__Gym',\n",
    " 'feature_count__Parking',\n",
    " 'feature_count__Storage',\n",
    " 'feature_count__Sunlight',\n",
    " 'feature_count__Pool',\n",
    " 'feature_count__Laundry_Room',\n",
    " 'feature_count__Wash_Dry',\n",
    " 'feature_count__Dishwasher',\n",
    " 'feature_count__Prewar',\n",
    " 'feature_count__AirCon',\n",
    " 'descript_len__website',\n",
    " 'descript_len__one',\n",
    " 'descript_len__two',\n",
    " 'descript_len__three',\n",
    " 'descript_len__four',\n",
    " 'descript_len__five',\n",
    " 'descript_len__six',\n",
    " 'descript_len__seven',\n",
    " 'descript_len__Doorman',\n",
    " 'descript_len__Dogs_Allowed',\n",
    " 'descript_len__Cats_Allowed',\n",
    " 'descript_len__Pets_Allowed',\n",
    " 'descript_len__Elevator',\n",
    " 'descript_len__Balcony',\n",
    " 'descript_len__Outdoors',\n",
    " 'descript_len__Hardwood',\n",
    " 'descript_len__Internet_Mention',\n",
    " 'descript_len__Loft',\n",
    " 'descript_len__New',\n",
    " 'descript_len__No_fee',\n",
    " 'descript_len__Gym',\n",
    " 'descript_len__Parking',\n",
    " 'descript_len__Storage',\n",
    " 'descript_len__Sunlight',\n",
    " 'descript_len__Pool',\n",
    " 'descript_len__Laundry_Room',\n",
    " 'descript_len__Wash_Dry',\n",
    " 'descript_len__Dishwasher',\n",
    " 'descript_len__Prewar',\n",
    " 'descript_len__AirCon',\n",
    " 'photo_count__website',\n",
    " 'photo_count__one',\n",
    " 'photo_count__two',\n",
    " 'photo_count__three',\n",
    " 'photo_count__four',\n",
    " 'photo_count__five',\n",
    " 'photo_count__six',\n",
    " 'photo_count__seven',\n",
    " 'photo_count__Doorman',\n",
    " 'photo_count__Dogs_Allowed',\n",
    " 'photo_count__Cats_Allowed',\n",
    " 'photo_count__Pets_Allowed',\n",
    " 'photo_count__Elevator',\n",
    " 'photo_count__Balcony',\n",
    " 'photo_count__Outdoors',\n",
    " 'photo_count__Hardwood',\n",
    " 'photo_count__Internet_Mention',\n",
    " 'photo_count__Loft',\n",
    " 'photo_count__New',\n",
    " 'photo_count__No_fee',\n",
    " 'photo_count__Gym',\n",
    " 'photo_count__Parking',\n",
    " 'photo_count__Storage',\n",
    " 'photo_count__Sunlight',\n",
    " 'photo_count__Pool',\n",
    " 'photo_count__Laundry_Room',\n",
    " 'photo_count__Wash_Dry',\n",
    " 'photo_count__Dishwasher',\n",
    " 'photo_count__Prewar',\n",
    " 'photo_count__AirCon',\n",
    " 'bedrooms__website',\n",
    " 'bedrooms__one',\n",
    " 'bedrooms__two',\n",
    " 'bedrooms__three',\n",
    " 'bedrooms__four',\n",
    " 'bedrooms__five',\n",
    " 'bedrooms__six',\n",
    " 'bedrooms__seven',\n",
    " 'bedrooms__Doorman',\n",
    " 'bedrooms__Dogs_Allowed',\n",
    " 'bedrooms__Cats_Allowed',\n",
    " 'bedrooms__Pets_Allowed',\n",
    " 'bedrooms__Elevator',\n",
    " 'bedrooms__Balcony',\n",
    " 'bedrooms__Outdoors',\n",
    " 'bedrooms__Hardwood',\n",
    " 'bedrooms__Internet_Mention',\n",
    " 'bedrooms__Loft',\n",
    " 'bedrooms__New',\n",
    " 'bedrooms__No_fee',\n",
    " 'bedrooms__Gym',\n",
    " 'bedrooms__Parking',\n",
    " 'bedrooms__Storage',\n",
    " 'bedrooms__Sunlight',\n",
    " 'bedrooms__Pool',\n",
    " 'bedrooms__Laundry_Room',\n",
    " 'bedrooms__Wash_Dry',\n",
    " 'bedrooms__Dishwasher',\n",
    " 'bedrooms__Prewar',\n",
    " 'bedrooms__AirCon',\n",
    " 'bathrooms__website',\n",
    " 'bathrooms__one',\n",
    " 'bathrooms__two',\n",
    " 'bathrooms__three',\n",
    " 'bathrooms__four',\n",
    " 'bathrooms__five',\n",
    " 'bathrooms__six',\n",
    " 'bathrooms__seven',\n",
    " 'bathrooms__Doorman',\n",
    " 'bathrooms__Dogs_Allowed',\n",
    " 'bathrooms__Cats_Allowed',\n",
    " 'bathrooms__Pets_Allowed',\n",
    " 'bathrooms__Elevator',\n",
    " 'bathrooms__Balcony',\n",
    " 'bathrooms__Outdoors',\n",
    " 'bathrooms__Hardwood',\n",
    " 'bathrooms__Internet_Mention',\n",
    " 'bathrooms__Loft',\n",
    " 'bathrooms__New',\n",
    " 'bathrooms__No_fee',\n",
    " 'bathrooms__Gym',\n",
    " 'bathrooms__Parking',\n",
    " 'bathrooms__Storage',\n",
    " 'bathrooms__Sunlight',\n",
    " 'bathrooms__Pool',\n",
    " 'bathrooms__Laundry_Room',\n",
    " 'bathrooms__Wash_Dry',\n",
    " 'bathrooms__Dishwasher',\n",
    " 'bathrooms__Prewar',\n",
    " 'bathrooms__AirCon',\n",
    " 'log_price__website',\n",
    " 'log_price__one',\n",
    " 'log_price__two',\n",
    " 'log_price__three',\n",
    " 'log_price__four',\n",
    " 'log_price__five',\n",
    " 'log_price__six',\n",
    " 'log_price__seven',\n",
    " 'log_price__Doorman',\n",
    " 'log_price__Dogs_Allowed',\n",
    " 'log_price__Cats_Allowed',\n",
    " 'log_price__Pets_Allowed',\n",
    " 'log_price__Elevator',\n",
    " 'log_price__Balcony',\n",
    " 'log_price__Outdoors',\n",
    " 'log_price__Hardwood',\n",
    " 'log_price__Internet_Mention',\n",
    " 'log_price__Loft',\n",
    " 'log_price__New',\n",
    " 'log_price__No_fee',\n",
    " 'log_price__Gym',\n",
    " 'log_price__Parking',\n",
    " 'log_price__Storage',\n",
    " 'log_price__Sunlight',\n",
    " 'log_price__Pool',\n",
    " 'log_price__Laundry_Room',\n",
    " 'log_price__Wash_Dry',\n",
    " 'log_price__Dishwasher',\n",
    " 'log_price__Prewar',\n",
    " 'log_price__AirCon',\n",
    " 'log_price_sq__website',\n",
    " 'log_price_sq__one',\n",
    " 'log_price_sq__two',\n",
    " 'log_price_sq__three',\n",
    " 'log_price_sq__four',\n",
    " 'log_price_sq__five',\n",
    " 'log_price_sq__six',\n",
    " 'log_price_sq__seven',\n",
    " 'log_price_sq__Doorman',\n",
    " 'log_price_sq__Dogs_Allowed',\n",
    " 'log_price_sq__Cats_Allowed',\n",
    " 'log_price_sq__Pets_Allowed',\n",
    " 'log_price_sq__Elevator',\n",
    " 'log_price_sq__Balcony',\n",
    " 'log_price_sq__Outdoors',\n",
    " 'log_price_sq__Hardwood',\n",
    " 'log_price_sq__Internet_Mention',\n",
    " 'log_price_sq__Loft',\n",
    " 'log_price_sq__New',\n",
    " 'log_price_sq__No_fee',\n",
    " 'log_price_sq__Gym',\n",
    " 'log_price_sq__Parking',\n",
    " 'log_price_sq__Storage',\n",
    " 'log_price_sq__Sunlight',\n",
    " 'log_price_sq__Pool',\n",
    " 'log_price_sq__Laundry_Room',\n",
    " 'log_price_sq__Wash_Dry',\n",
    " 'log_price_sq__Dishwasher',\n",
    " 'log_price_sq__Prewar',\n",
    " 'log_price_sq__AirCon',\n",
    " 'log_price__interest_adj_13__website',\n",
    " 'log_price__interest_adj_13__one',\n",
    " 'log_price__interest_adj_13__two',\n",
    " 'log_price__interest_adj_13__three',\n",
    " 'log_price__interest_adj_13__four',\n",
    " 'log_price__interest_adj_13__five',\n",
    " 'log_price__interest_adj_13__six',\n",
    " 'log_price__interest_adj_13__seven',\n",
    " 'log_price__interest_adj_13__Doorman',\n",
    " 'log_price__interest_adj_13__Dogs_Allowed',\n",
    " 'log_price__interest_adj_13__Cats_Allowed',\n",
    " 'log_price__interest_adj_13__Pets_Allowed',\n",
    " 'log_price__interest_adj_13__Elevator',\n",
    " 'log_price__interest_adj_13__Balcony',\n",
    " 'log_price__interest_adj_13__Outdoors',\n",
    " 'log_price__interest_adj_13__Hardwood',\n",
    " 'log_price__interest_adj_13__Internet_Mention',\n",
    " 'log_price__interest_adj_13__Loft',\n",
    " 'log_price__interest_adj_13__New',\n",
    " 'log_price__interest_adj_13__No_fee',\n",
    " 'log_price__interest_adj_13__Gym',\n",
    " 'log_price__interest_adj_13__Parking',\n",
    " 'log_price__interest_adj_13__Storage',\n",
    " 'log_price__interest_adj_13__Sunlight',\n",
    " 'log_price__interest_adj_13__Pool',\n",
    " 'log_price__interest_adj_13__Laundry_Room',\n",
    " 'log_price__interest_adj_13__Wash_Dry',\n",
    " 'log_price__interest_adj_13__Dishwasher',\n",
    " 'log_price__interest_adj_13__Prewar',\n",
    " 'log_price__interest_adj_13__AirCon',\n",
    " 'log_days_between__website',\n",
    " 'log_days_between__one',\n",
    " 'log_days_between__two',\n",
    " 'log_days_between__three',\n",
    " 'log_days_between__four',\n",
    " 'log_days_between__five',\n",
    " 'log_days_between__six',\n",
    " 'log_days_between__seven',\n",
    " 'log_days_between__Doorman',\n",
    " 'log_days_between__Dogs_Allowed',\n",
    " 'log_days_between__Cats_Allowed',\n",
    " 'log_days_between__Pets_Allowed',\n",
    " 'log_days_between__Elevator',\n",
    " 'log_days_between__Balcony',\n",
    " 'log_days_between__Outdoors',\n",
    " 'log_days_between__Hardwood',\n",
    " 'log_days_between__Internet_Mention',\n",
    " 'log_days_between__Loft',\n",
    " 'log_days_between__New',\n",
    " 'log_days_between__No_fee',\n",
    " 'log_days_between__Gym',\n",
    " 'log_days_between__Parking',\n",
    " 'log_days_between__Storage',\n",
    " 'log_days_between__Sunlight',\n",
    " 'log_days_between__Pool',\n",
    " 'log_days_between__Laundry_Room',\n",
    " 'log_days_between__Wash_Dry',\n",
    " 'log_days_between__Dishwasher',\n",
    " 'log_days_between__Prewar',\n",
    " 'log_days_between__AirCon',\n",
    " 'interest_adj_1',\n",
    " 'interest_adj_2',\n",
    " 'interest_adj_3',\n",
    " 'interest_adj_4',\n",
    " 'interest_adj_5',\n",
    " 'interest_adj_6',\n",
    " 'interest_adj_7',\n",
    " 'interest_adj_8',\n",
    " 'interest_adj_9',\n",
    " 'interest_adj_10',\n",
    " 'interest_adj_11',\n",
    " 'interest_adj_12',\n",
    " 'interest_adj_14',\n",
    " 'interest_adj_15',\n",
    " 'interest_adj_16',\n",
    " 'interest_adj_17',\n",
    " 'interest_adj_18',\n",
    " 'interest_adj_19',\n",
    " 'interest_adj_20',\n",
    " 'interest_adj_21',\n",
    " 'interest_adj_22',\n",
    " 'interest_adj_23',\n",
    " 'interest_adj_24',\n",
    " 'interest_adj_25',\n",
    " 'interest_adj_26',\n",
    " 'interest_adj_27',\n",
    " 'interest_adj_28',\n",
    " 'interest_adj_29',\n",
    " 'interest_adj_30',\n",
    " 'distance_adj_1',\n",
    " 'distance_adj_2',\n",
    " 'distance_adj_3',\n",
    " 'distance_adj_4',\n",
    " 'distance_adj_5',\n",
    " 'distance_adj_6',\n",
    " 'distance_adj_7',\n",
    " 'distance_adj_8',\n",
    " 'distance_adj_9',\n",
    " 'distance_adj_10',\n",
    " 'distance_adj_11',\n",
    " 'distance_adj_12',\n",
    " 'distance_adj_13',\n",
    " 'distance_adj_14',\n",
    " 'distance_adj_15',\n",
    " 'distance_adj_16',\n",
    " 'distance_adj_17',\n",
    " 'distance_adj_18',\n",
    " 'distance_adj_19',\n",
    " 'distance_adj_20',\n",
    " 'distance_adj_21',\n",
    " 'distance_adj_22',\n",
    " 'distance_adj_23',\n",
    " 'distance_adj_24',\n",
    " 'distance_adj_25',\n",
    " 'distance_adj_26',\n",
    " 'distance_adj_27',\n",
    " 'distance_adj_28',\n",
    " 'distance_adj_29',\n",
    " 'distance_adj_30',\n",
    " 'days_between_adj_1',\n",
    " 'days_between_adj_2',\n",
    " 'days_between_adj_3',\n",
    " 'days_between_adj_4',\n",
    " 'days_between_adj_6',\n",
    " 'days_between_adj_7',\n",
    " 'days_between_adj_8',\n",
    " 'days_between_adj_9',\n",
    " 'days_between_adj_10',\n",
    " 'days_between_adj_11',\n",
    " 'days_between_adj_12',\n",
    " 'days_between_adj_13',\n",
    " 'days_between_adj_14',\n",
    " 'days_between_adj_15',\n",
    " 'days_between_adj_16',\n",
    " 'days_between_adj_17',\n",
    " 'days_between_adj_18',\n",
    " 'days_between_adj_19',\n",
    " 'days_between_adj_20',\n",
    " 'days_between_adj_21',\n",
    " 'days_between_adj_22',\n",
    " 'days_between_adj_23',\n",
    " 'days_between_adj_24',\n",
    " 'days_between_adj_25',\n",
    " 'days_between_adj_26',\n",
    " 'days_between_adj_27',\n",
    " 'days_between_adj_28',\n",
    " 'days_between_adj_29',\n",
    " 'days_between_adj_30',\n",
    " 'feature_count_adj_1',\n",
    " 'feature_count_adj_2',\n",
    " 'feature_count_adj_3',\n",
    " 'feature_count_adj_4',\n",
    " 'feature_count_adj_5',\n",
    " 'feature_count_adj_6',\n",
    " 'feature_count_adj_7',\n",
    " 'feature_count_adj_8',\n",
    " 'feature_count_adj_9',\n",
    " 'feature_count_adj_10',\n",
    " 'feature_count_adj_11',\n",
    " 'feature_count_adj_12',\n",
    " 'feature_count_adj_13',\n",
    " 'feature_count_adj_14',\n",
    " 'feature_count_adj_15',\n",
    " 'feature_count_adj_16',\n",
    " 'feature_count_adj_17',\n",
    " 'feature_count_adj_18',\n",
    " 'feature_count_adj_19',\n",
    " 'feature_count_adj_20',\n",
    " 'feature_count_adj_21',\n",
    " 'feature_count_adj_22',\n",
    " 'feature_count_adj_23',\n",
    " 'feature_count_adj_24',\n",
    " 'feature_count_adj_26',\n",
    " 'feature_count_adj_27',\n",
    " 'feature_count_adj_28',\n",
    " 'feature_count_adj_29',\n",
    " 'feature_count_adj_30',\n",
    " 'descript_len_adj_1',\n",
    " 'descript_len_adj_2',\n",
    " 'descript_len_adj_3',\n",
    " 'descript_len_adj_4',\n",
    " 'descript_len_adj_5',\n",
    " 'descript_len_adj_6',\n",
    " 'descript_len_adj_7',\n",
    " 'descript_len_adj_8',\n",
    " 'descript_len_adj_9',\n",
    " 'descript_len_adj_10',\n",
    " 'descript_len_adj_12',\n",
    " 'descript_len_adj_13',\n",
    " 'descript_len_adj_14',\n",
    " 'descript_len_adj_15',\n",
    " 'descript_len_adj_16',\n",
    " 'descript_len_adj_17',\n",
    " 'descript_len_adj_18',\n",
    " 'descript_len_adj_19',\n",
    " 'descript_len_adj_20',\n",
    " 'descript_len_adj_21',\n",
    " 'descript_len_adj_22',\n",
    " 'descript_len_adj_23',\n",
    " 'descript_len_adj_24',\n",
    " 'descript_len_adj_25',\n",
    " 'descript_len_adj_26',\n",
    " 'descript_len_adj_27',\n",
    " 'descript_len_adj_28',\n",
    " 'descript_len_adj_29',\n",
    " 'descript_len_adj_30',\n",
    " 'photo_count_adj_1',\n",
    " 'photo_count_adj_2',\n",
    " 'photo_count_adj_3',\n",
    " 'photo_count_adj_4',\n",
    " 'photo_count_adj_5',\n",
    " 'photo_count_adj_6',\n",
    " 'photo_count_adj_7',\n",
    " 'photo_count_adj_8',\n",
    " 'photo_count_adj_9',\n",
    " 'photo_count_adj_10',\n",
    " 'photo_count_adj_11',\n",
    " 'photo_count_adj_12',\n",
    " 'photo_count_adj_13',\n",
    " 'photo_count_adj_14',\n",
    " 'photo_count_adj_15',\n",
    " 'photo_count_adj_16',\n",
    " 'photo_count_adj_17',\n",
    " 'photo_count_adj_18',\n",
    " 'photo_count_adj_19',\n",
    " 'photo_count_adj_20',\n",
    " 'photo_count_adj_21',\n",
    " 'photo_count_adj_22',\n",
    " 'photo_count_adj_23',\n",
    " 'photo_count_adj_24',\n",
    " 'photo_count_adj_25',\n",
    " 'photo_count_adj_26',\n",
    " 'photo_count_adj_27',\n",
    " 'photo_count_adj_28',\n",
    " 'photo_count_adj_29',\n",
    " 'photo_count_adj_30',\n",
    " 'log_price_adj_1',\n",
    " 'log_price_adj_2',\n",
    " 'log_price_adj_4',\n",
    " 'log_price_adj_5',\n",
    " 'log_price_adj_6',\n",
    " 'log_price_adj_7',\n",
    " 'log_price_adj_8',\n",
    " 'log_price_adj_9',\n",
    " 'log_price_adj_10',\n",
    " 'log_price_adj_11',\n",
    " 'log_price_adj_12',\n",
    " 'log_price_adj_13',\n",
    " 'log_price_adj_14',\n",
    " 'log_price_adj_15',\n",
    " 'log_price_adj_16',\n",
    " 'log_price_adj_17',\n",
    " 'log_price_adj_18',\n",
    " 'log_price_adj_19',\n",
    " 'log_price_adj_20',\n",
    " 'log_price_adj_21',\n",
    " 'log_price_adj_22',\n",
    " 'log_price_adj_23',\n",
    " 'log_price_adj_24',\n",
    " 'log_price_adj_25',\n",
    " 'log_price_adj_26',\n",
    " 'log_price_adj_27',\n",
    " 'log_price_adj_28',\n",
    " 'log_price_adj_29',\n",
    " 'log_price_adj_30',\n",
    " 'bedrooms_adj_1',\n",
    " 'bedrooms_adj_2',\n",
    " 'bedrooms_adj_4',\n",
    " 'bedrooms_adj_5',\n",
    " 'bedrooms_adj_6',\n",
    " 'bedrooms_adj_7',\n",
    " 'bedrooms_adj_8',\n",
    " 'bedrooms_adj_9',\n",
    " 'bedrooms_adj_10',\n",
    " 'bedrooms_adj_11',\n",
    " 'bedrooms_adj_12',\n",
    " 'bedrooms_adj_13',\n",
    " 'bedrooms_adj_14',\n",
    " 'bedrooms_adj_15',\n",
    " 'bedrooms_adj_16',\n",
    " 'bedrooms_adj_17',\n",
    " 'bedrooms_adj_18',\n",
    " 'bedrooms_adj_19',\n",
    " 'bedrooms_adj_20',\n",
    " 'bedrooms_adj_21',\n",
    " 'bedrooms_adj_22',\n",
    " 'bedrooms_adj_23',\n",
    " 'bedrooms_adj_24',\n",
    " 'bedrooms_adj_25',\n",
    " 'bedrooms_adj_26',\n",
    " 'bedrooms_adj_27',\n",
    " 'bedrooms_adj_28',\n",
    " 'bedrooms_adj_29',\n",
    " 'bedrooms_adj_30',\n",
    " 'bathrooms_adj_1',\n",
    " 'bathrooms_adj_2',\n",
    " 'bathrooms_adj_3',\n",
    " 'bathrooms_adj_5',\n",
    " 'bathrooms_adj_6',\n",
    " 'bathrooms_adj_7',\n",
    " 'bathrooms_adj_8',\n",
    " 'bathrooms_adj_9',\n",
    " 'bathrooms_adj_10',\n",
    " 'bathrooms_adj_11',\n",
    " 'bathrooms_adj_12',\n",
    " 'bathrooms_adj_13',\n",
    " 'bathrooms_adj_14',\n",
    " 'bathrooms_adj_15',\n",
    " 'bathrooms_adj_16',\n",
    " 'bathrooms_adj_17',\n",
    " 'bathrooms_adj_18',\n",
    " 'bathrooms_adj_19',\n",
    " 'bathrooms_adj_20',\n",
    " 'bathrooms_adj_21',\n",
    " 'bathrooms_adj_22',\n",
    " 'bathrooms_adj_23',\n",
    " 'bathrooms_adj_24',\n",
    " 'bathrooms_adj_25',\n",
    " 'bathrooms_adj_26',\n",
    " 'bathrooms_adj_27',\n",
    " 'bathrooms_adj_28',\n",
    " 'bathrooms_adj_29',\n",
    " 'bathrooms_adj_30']\n",
    "\n",
    "scale = CustomScaler(columns=continuous_cols2 )\n",
    "\n",
    "train_all_scale = scale.fit_transform(train_all)\n",
    "holdout_scale = scale.fit_transform(holdout)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert from pandas to numpy arrays and take out the id columns at the beginning\n",
    "\n",
    "#For the whole dataset: \n",
    "mat_train = train_all_scale.as_matrix()\n",
    "X_train_all = mat_train[:,3:]\n",
    "y_train_all = mat_train[:, 2]\n",
    "mat_holdout = holdout_scale.as_matrix()\n",
    "X_holdout = mat_holdout[:,3:]\n",
    "\n",
    "#For the small data:\n",
    "mat_train_small = train_all_scale_small.as_matrix()\n",
    "X_train_all_small = mat_train_small[:,3:61]\n",
    "y_train_all_small = mat_train_small[:, 2]\n",
    "mat_holdout_small = holdout_scale_small.as_matrix()\n",
    "X_holdout_small = mat_holdout_small[:,3:61]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert the location column using one-hot encoding: \n",
    "\n",
    "#since this creates a lot of dummy variables, another way to do it would be to add a column for averge interest level for that\n",
    "#location code \n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(train_all_small[\"location_code\"].reshape(-1,1))\n",
    "loc_codes_train = enc.transform(train_all_small[\"location_code\"].reshape(-1,1)).toarray()\n",
    "#change for 0/1 encoding to -1/1 encoding: \n",
    "loc_codes_train[loc_codes_train ==0 ] = -1\n",
    "#add these new columns to the data array: \n",
    "X_train_all_small = np.concatenate((X_train_all_small, loc_codes_train),axis =1)\n",
    "#delete the 16th location because there is no location 16 in the test set\n",
    "X_train_all_small = X_train_all_small[:, :74]\n",
    "\n",
    "#do this for the holdout data too: \n",
    "enc.fit(holdout_small[\"location_code\"].reshape(-1,1))\n",
    "loc_codes_holdout = enc.transform(holdout_small[\"location_code\"].reshape(-1,1)).toarray()\n",
    "#change for 0/1 encoding to -1/1 encoding: \n",
    "loc_codes_holdout[loc_codes_holdout ==0 ] = -1\n",
    "#add these new columns to the data array: \n",
    "X_holdout_small = np.concatenate((X_holdout_small, loc_codes_holdout),axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#One-hot encoding for the full dataset: \n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(train_all[\"location_code\"].reshape(-1,1))\n",
    "loc_codes_train = enc.transform(train_all[\"location_code\"].reshape(-1,1)).toarray()\n",
    "#change for 0/1 encoding to -1/1 encoding: \n",
    "loc_codes_train[loc_codes_train ==0 ] = -1\n",
    "#add these new columns to the data array: \n",
    "X_train_all = np.concatenate((X_train_all, loc_codes_train),axis =1)\n",
    "#delete the 16th location because there is no location 16 in the test set\n",
    "X_train_all = X_train_all[:, :837]\n",
    "\n",
    "#do this for the holdout data too: \n",
    "enc.fit(holdout[\"location_code\"].reshape(-1,1))\n",
    "loc_codes_holdout = enc.transform(holdout[\"location_code\"].reshape(-1,1)).toarray()\n",
    "#change for 0/1 encoding to -1/1 encoding: \n",
    "loc_codes_holdout[loc_codes_holdout ==0 ] = -1\n",
    "#add these new columns to the data array: \n",
    "X_holdout = np.concatenate((X_holdout, loc_codes_holdout),axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49337, 17)\n",
      "(49337, 58)\n",
      "(74659, 16)\n",
      "(74659, 58)\n",
      "\n",
      "(74659, 837)\n",
      "(49337, 837)\n"
     ]
    }
   ],
   "source": [
    "print(loc_codes_train.shape)\n",
    "print(X_train_all_small.shape)\n",
    "print(loc_codes_holdout.shape)\n",
    "print(X_holdout_small.shape)\n",
    "print()\n",
    "print(X_holdout.shape)\n",
    "print(X_train_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test/train split (full dataset):   \n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_all, y_train_all)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test/train split (small dataset):\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train_small, X_test_small, y_train_small, y_test = train_test_split(X_train_all_small, y_train_all_small)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37014, 821)\n",
      "(12338, 821)\n",
      "(49352, 824)\n",
      "(37014, 62)\n",
      "(12338, 62)\n",
      "(49337, 62)\n"
     ]
    }
   ],
   "source": [
    "#full dataset: \n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(train_all.shape)\n",
    "\n",
    "#for the small dataset:\n",
    "print(X_train_no.shape)\n",
    "print(X_test_no.shape)\n",
    "print(X_train_all_scale_small.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline (Random) Classifier Performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline log loss:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.78391113651128785"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_y1 = len(y_train[y_train== 1])/len(y_train)\n",
    "per_y2 = len(y_train[y_train== 2])/len(y_train)\n",
    "per_y3 = len(y_train[y_train== 3])/len(y_train)\n",
    "\n",
    "y1 = len(y_train[y_train== 1])\n",
    "y2 = len(y_train[y_train== 2])\n",
    "y3 = len(y_train[y_train== 3])\n",
    "\n",
    "print(\"baseline log loss:\")\n",
    "- (1/X_train.shape[0])*(np.log(per_y1)*y1  + np.log(per_y2)*y2 + np.log(per_y3)*y3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log loss on the holdout set predicting all 1's = 10.63995"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron Neural Net:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multi-layer Perceptrons are:\n",
    "- The essential deep-learning model. We'd use an RNN for data with a time component and a CNN for image data (that's highly correlated between the pixel data points). But for a problem like this, a \"basic\" MLP is the way to go. \n",
    "- MLP's are also called deep feed-forward neural networks \n",
    "- Their goal is to learn the function mapping the input data to the classification labels (real function called f-star in the book. The approximation is just regular f(x))\n",
    "- Capability to learn non-linear models.\n",
    "- RNNs are the neural nets that can feed back to itself. \n",
    "- Neural networks are called networks because they are a chain of non-linear function compositions f3 (f2 (f1(x))). f1 is the first layer. f2 second layer, etc. \n",
    "\n",
    "##### Why Neural Nets are more powerful than linear models: \n",
    "- Linear and logistic regression are limited to linear functions, so the model cannot understand the interaction between any two input variables (without having to put those interaction explicitly into the model). \n",
    "- You don't have to transform any variables when using NN's because NN's are already transforming the predictors with nonlinear functions \n",
    "- However, the weights aren't interpretable like in logistic regression \n",
    "- Too many hidden units leads to overfitting \n",
    "\n",
    "##### The disadvantages of Multi-layer Perceptron are:\n",
    "- MLP with hidden layers have a non-convex loss function where there exists more than one local minimum. Therefore different random weight initializations can lead to different validation accuracy.\n",
    "- MLP requires tuning a number of hyperparameters such as the number of hidden neurons, layers, and iterations.\n",
    "- MLP is sensitive to feature scaling.\n",
    "\n",
    "##### How it works: \n",
    "- MLP trains using Backpropagation. \n",
    "- More precisely, it trains using some form of gradient descent and the gradients are calculated using Backpropagation. \n",
    "- For classification, it minimizes the Cross-Entropy loss function.\n",
    "\n",
    "##### Time complexity:\n",
    "Suppose there are n training samples, m features, k hidden layers, each containing h neurons - for simplicity, and o output neurons. The time complexity of backpropagation is $O(n \\cdot m \\cdot h^k \\cdot o \\cdot i) $, where i is the number of iterations.\n",
    "\n",
    "##### About the parameters: \n",
    "- Alpha: \n",
    " - (L2 regularization) term which helps in avoiding overfitting by penalizing weights with large magnitudes.\n",
    "- Activation Function for the output layer:\n",
    " - Softmax is the necessary choice so that no matter what values are plugged into predict_proba(), the output probability vector always sums up to 1. This can only be achieved by the Softmax activation function. Using an activation other that Softmax there is no guarantee that the sum of the activations in the final layer will be exactly one. \n",
    "- Activation Functions in the hidden layers:\n",
    " - tanh works better than logistic because the tanh function is symmetric about 0 so it is less prone to \"saturation in the later layers\". It also ensures that the input, output and hidden layers all have mean values of 0 and stand dev = 1. \n",
    " - ReLu has been shown to reach the same training error 6 times faster than tanh. However, relu isn't differentiable at zero and so it doesn't train a unit when it's \"zero active\". Leaky Relu and PReLu have been proposed to solve this problem but aren't implemented in sklearn yet. \n",
    "- Solvers: \n",
    "  - Empirically, we observed that L-BFGS converges faster and with better solutions on small datasets. \n",
    "  - For relatively large datasets, however, Adam is very robust. It usually converges quickly and gives pretty good performance. \n",
    "  - SGD with momentum or nesterov’s momentum, on the other hand, can perform better than those two algorithms if learning rate is correctly tuned.\n",
    "- Hidden Layers:\n",
    "  - hidden_layer_sizes=(5, 3) means there are 2 hidden layers, 5 hidden units in first layer and 3 hidden units in second layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the MLP classifier using Grid Search: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About GridSearchCV function:\n",
    "- by default does 3-fold cross validation \n",
    "- also does stratified Kfold by default, which preserves the percentage of samples for each class\n",
    "- use scoring = neg_log_loss (so it knows that lower is better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "gs = GridSearchCV (  \n",
    "    \n",
    "        MLPClassifier(),\n",
    "                  \n",
    "        param_grid={\n",
    "        #'learning_rate': [0.005, 0.001],    <- only when solver is 'sgd'\n",
    "        'alpha': 10.0 ** -np.arange(1, 7),\n",
    "        'hidden_layer_sizes': [(5,), (4,), (3,), (6,), ],\n",
    "        'solver': ['adam', 'lbfgs','sgd'],\n",
    "        'activation': [\"relu\", \"tanh\"]\n",
    "         }, \n",
    "                  \n",
    "        scoring = 'neg_log_loss' )         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#------------ Other things to try in grid search: -------------------------\n",
    "\n",
    "#MLPClassifier(solver='adam', random_state=1, activation = \"tanh\")    <- optimized for the old dataset (with all the correlations)\n",
    "        #\"random_state\" = 1,\n",
    "    #'hidden0__units': [4, 8, 12],\n",
    "    #'activation': [\"relu\", \"logistic\", \"tanh\"],\n",
    "        #'learning_rate': [0.005, 0.001],\n",
    "        #hidden_layer_sizes=(5, 2)\n",
    "        #'hidden_layer_sizes': [(5, 2), (5,), (10,), (2,), (7,)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the best results...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'activation': 'tanh', 'solver': 'sgd', 'alpha': 1.0000000000000001e-05, 'hidden_layer_sizes': (4,)}\n",
      "\n",
      "Best Mean log loss on the kth fold during training:\n",
      "0.624649655765\n",
      "Best Mean log loss on the training folds during training:\n",
      "0.531438061999\n",
      "Mean log loss on the training folds for the test set's best set of parameters\n",
      "0.590233290831\n",
      "\n",
      "top means: [0.62464965576462683, 0.62507566115723756, 0.6255623511494266, 0.62564019637260337, 0.62565603401020642]\n",
      "  and their std devs: [ 0.00218244  0.00185456  0.00070539  0.00321176  0.00179811]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(gs.best_params_)\n",
    "print()\n",
    "print(\"Best Mean log loss on the kth fold during training:\")\n",
    "print(sorted(-1* gs.cv_results_['mean_test_score'])[0])\n",
    "print(\"Best Mean log loss on the training folds during training:\")\n",
    "print(sorted(-1* gs.cv_results_['mean_train_score'])[0])\n",
    "print(\"Mean log loss on the training folds for the test set's best set of parameters\")\n",
    "print(-1* gs.cv_results_['mean_train_score'][np.argsort(-1* gs.cv_results_['mean_test_score'])][0])\n",
    "print()\n",
    "\n",
    "#print(\"All mean log losses testing (using 3 fold CV) :\")\n",
    "means_orig = -1 * gs.cv_results_['mean_test_score']\n",
    "stds  = gs.cv_results_['std_test_score']\n",
    "\n",
    "means = sorted(means_orig)\n",
    "order = np.argsort(means_orig)\n",
    "stds = stds[order]\n",
    "param_list = gs.cv_results_['params']  \n",
    "\n",
    "print(\"top means:\", means[0:5])\n",
    "print(\"  and their std devs:\", stds[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistically significant best results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T tests.....\n",
      "t-statistic & p-value for testing whether the differece in mean log losses is significantly different from 0: \n",
      "        -1.64148905887 0.242393044984\n"
     ]
    }
   ],
   "source": [
    "import scipy \n",
    "from scipy import stats\n",
    "\n",
    "print(\"T tests.....\")\n",
    "\n",
    "def calculate_t_stat(index1, index2):\n",
    "    t_stat = (means[index1] - means[index2] - 0)/(np.sqrt(stds[index1])**2/3 + np.sqrt(stds[index2])**2/3)\n",
    "    return t_stat\n",
    "\n",
    "t_stat = calculate_t_stat(0,25)\n",
    "p_val =  stats.t.sf(np.abs(t_stat), 3-1)*2     #divide by 2 for 1-tailed test? \n",
    "\n",
    "print(\"t-statistic & p-value for testing whether the differece in mean log losses is significantly different from 0: \") \n",
    "print(\"       \", t_stat,  p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best sets of parameters:\n",
      "\n",
      "Distance from best set: 0\n",
      "parameters: {'activation': 'tanh', 'solver': 'sgd', 'alpha': 1.0000000000000001e-05, 'hidden_layer_sizes': (4,)}\n",
      "T-stat that these params are better than the best set: 0.0 and p-value: 1.0\n",
      "\n",
      "Distance from best set: 1\n",
      "parameters: {'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.001, 'hidden_layer_sizes': (5,)}\n",
      "T-stat that these params are better than the best set: -0.316575190986 and p-value: 0.781553798158\n",
      "\n",
      "Distance from best set: 2\n",
      "parameters: {'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.01, 'hidden_layer_sizes': (4,)}\n",
      "T-stat that these params are better than the best set: -0.948144268788 and p-value: 0.443132338995\n",
      "\n",
      "Distance from best set: 3\n",
      "parameters: {'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.001, 'hidden_layer_sizes': (6,)}\n",
      "T-stat that these params are better than the best set: -0.550891232703 and p-value: 0.637027661561\n",
      "\n",
      "Distance from best set: 4\n",
      "parameters: {'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.10000000000000001, 'hidden_layer_sizes': (4,)}\n",
      "T-stat that these params are better than the best set: -0.758470890133 and p-value: 0.52736394271\n",
      "\n",
      "Distance from best set: 5\n",
      "parameters: {'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.10000000000000001, 'hidden_layer_sizes': (5,)}\n",
      "T-stat that these params are better than the best set: -0.786751866521 and p-value: 0.513848292587\n",
      "\n",
      "Distance from best set: 6\n",
      "parameters: {'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.01, 'hidden_layer_sizes': (3,)}\n",
      "T-stat that these params are better than the best set: -1.60980516752 and p-value: 0.248727806121\n",
      "\n",
      "Distance from best set: 7\n",
      "parameters: {'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.10000000000000001, 'hidden_layer_sizes': (6,)}\n",
      "T-stat that these params are better than the best set: -0.720001241142 and p-value: 0.546297956218\n",
      "\n",
      "Distance from best set: 8\n",
      "parameters: {'activation': 'tanh', 'solver': 'sgd', 'alpha': 1.0000000000000001e-05, 'hidden_layer_sizes': (3,)}\n",
      "T-stat that these params are better than the best set: -0.739220409989 and p-value: 0.536759369142\n",
      "\n",
      "Distance from best set: 9\n",
      "parameters: {'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.001, 'hidden_layer_sizes': (3,)}\n",
      "T-stat that these params are better than the best set: -1.64034049846 and p-value: 0.242619021537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Best sets of parameters:\")\n",
    "print()\n",
    "\n",
    "for i in range(10):         #change to range(len(mean))   to see the whole list\n",
    "    t_stat = calculate_t_stat(0,i)\n",
    "    print(\"Distance from best set:\", i)\n",
    "    print(\"parameters:\", gs.cv_results_['params'][order[i]])\n",
    "    print(\"T-stat that these params are better than the best set:\", t_stat, \"and p-value:\", stats.t.sf(np.abs(t_stat), 3-1)*2 )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results show that....\n",
    " - we can stop trying relu (tanh is better)\n",
    " - alpha doesn't matter much \n",
    " - sgd is the best solver (start adding learning rate to the parameters list)\n",
    " - number of units in first hidden layer doesn't appear to matter much either "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit on the best set of parameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'tanh',\n",
       " 'alpha': 1.0000000000000001e-05,\n",
       " 'hidden_layer_sizes': (4,),\n",
       " 'solver': 'sgd'}"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.cv_results_['params'][order[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'cv_results_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-341-0e6cbc7540df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# classifier with the best parameters from grid search:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m clf = MLPClassifier(activation = gs.cv_results_['params'][order[0]][\"activation\"], \n\u001b[0m\u001b[0;32m      3\u001b[0m                     \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'params'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"alpha\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                     \u001b[0mhidden_layer_sizes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'params'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"hidden_layer_sizes\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                     solver = gs.cv_results_['params'][order[0]][\"solver\"] )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'cv_results_'"
     ]
    }
   ],
   "source": [
    "# classifier with the best parameters from grid search: \n",
    "clf = MLPClassifier(activation = gs.cv_results_['params'][order[0]][\"activation\"], \n",
    "                    alpha = gs.cv_results_['params'][order[0]][\"alpha\"],\n",
    "                    hidden_layer_sizes = gs.cv_results_['params'][order[0]][\"hidden_layer_sizes\"],\n",
    "                    solver = gs.cv_results_['params'][order[0]][\"solver\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.fit(X_train,y_train)\n",
    "y_pred2 = clf.predict(X_test)\n",
    "y_pred_proba2 = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss from the best set of grid search parameters on my test set:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.62584323078537163"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"log loss from the best set of grid search parameters on my test set:\")\n",
    "log_loss(y_test, y_pred_proba2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicted class distribution of the test set (as a sanity check): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of Lows:  80.48306046360837\n",
      "percentage of Mediums:  16.315448208785867\n",
      "percentage of Highs:  3.201491327605771\n",
      "\n",
      "Number of mislabeled points out of a total 12338 points : 3437\n",
      "\n",
      "percent accuracy:  0.721429729292\n"
     ]
    }
   ],
   "source": [
    "print(\"percentage of Lows: \", len(y_pred2[y_pred2 == 1])/len(y_pred2) * 100)\n",
    "print(\"percentage of Mediums: \", len(y_pred2[y_pred2 == 2])/len(y_pred2) * 100)\n",
    "print(\"percentage of Highs: \", len(y_pred2[y_pred2 == 3])/len(y_pred2)* 100)\n",
    "\n",
    "print()\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "       % (X_test.shape[0],(y_test != y_pred2).sum()))\n",
    "\n",
    "print()\n",
    "print(\"percent accuracy: \", clf.score(X_test,y_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(823, 4), (4, 3)]"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clf.coefs_ contains the weight matrices that constitute the model parameters:\n",
    "[coef.shape for coef in clf.coefs_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next grid search attempt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs2 = GridSearchCV (  \n",
    "    \n",
    "        MLPClassifier(activation = 'tanh', solver = 'sgd'),\n",
    "                  \n",
    "        param_grid={\n",
    "        #'learning_rate': ['constant', 'invscaling', 'adaptive'],    #<- only when solver is 'sgd'\n",
    "        'alpha': 10.0 ** -np.arange(1, 7),\n",
    "        'hidden_layer_sizes': [(5,), (4,), (3,), (6,), (4,2)],\n",
    "         }, \n",
    "                  \n",
    "        scoring = 'neg_log_loss' )         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs2.fit(X_train_small,y_train_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying different datasets (move this to the top) \n",
    "Is it better to use all columns OR just the ones without interactions OR the ones without adjacency and interactions? \n",
    "(cuz neural nets automatically build the interaction terms):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61760745647523108"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit on a just the first few columns: \n",
    "\n",
    "clf.fit(X_train[:,0:47],y_train)\n",
    "y_pred = clf.predict(X_test[:,0:47])\n",
    "y_pred_proba = clf.predict_proba(X_test[:,0:47])\n",
    "y_pred_proba\n",
    "from sklearn.metrics import log_loss\n",
    "log_loss(y_test, y_pred_proba)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61062277319002833"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit on all columns: \n",
    "\n",
    "#clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred_proba = clf.predict_proba(X_test)\n",
    "y_pred_proba\n",
    "from sklearn.metrics import log_loss\n",
    "log_loss(y_test, y_pred_proba)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61891664795650025"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit on all non-interaction columns: \n",
    "\n",
    "indices = list(range(0,47))   \n",
    "indices.extend(list(range(560, 823))) \n",
    "\n",
    "clf.fit(X_train[:,indices],y_train)\n",
    "y_pred = clf.predict(X_test[:,indices])\n",
    "y_pred_proba = clf.predict_proba(X_test[:,indices])\n",
    "y_pred_proba\n",
    "from sklearn.metrics import log_loss\n",
    "log_loss(y_test, y_pred_proba)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37014, 821)"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000882497002522934"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit using no interactions and only the adjacencies that aren't higly correlated: \n",
    "\n",
    "clf.fit(X_train_no,y_train)\n",
    "y_pred_proba = clf.predict_proba(X_test_no)\n",
    "from sklearn.metrics import log_loss\n",
    "log_loss(y_test, y_pred_proba) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using cross validation:\n",
    "\n",
    "Outputs the log loss on a test test set k times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.62436891, -0.6084782 , -0.61548945, -0.59919801, -0.6109528 ,\n",
       "       -0.62454158, -0.61820943, -0.6302879 , -0.62648001, -0.63441894])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#Stratigied K-fold perserves the number of examples from each class \n",
    "cv_scores = cross_val_score(clf, X_train_all_scale[:,indices], y_train_all, scoring='neg_log_loss', cv = 10) \n",
    "cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean log loss: 0.619242524111\n"
     ]
    }
   ],
   "source": [
    "print(\"mean log loss:\", -1*np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard dev of cross val log losses : 0.01029849946\n"
     ]
    }
   ],
   "source": [
    "print(\"standard dev of cross val log losses :\",np.std(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mean log loss: 0.620394160421    all columns \n",
    "#mean log loss: 0.619242524111     minus interactions    <--- lowest loss! \n",
    "#mean log loss: 0.625933638544     minus interactions and adjacencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing Old Results: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logloss = 0.62709157338049504    \n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                   hidden_layer_sizes=(5, 2), random_state=1)\n",
    "---------------------------------------------------------      \n",
    "logloss = 0.61944758672494526    \n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-2,\n",
    "                   hidden_layer_sizes=(5, 2), random_state=1)\n",
    "---------------------------------------------------------    \n",
    "*Dataset:* no interactions, all adjacencies\n",
    "\n",
    "{'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (5,)}\n",
    "\n",
    "With a training score of:\n",
    "-0.624981302943\n",
    "\n",
    "testing log loss = 0.6271359\n",
    "\n",
    "Kaggle score:  0.62316\n",
    "\n",
    "---------------------------------------------------------    \n",
    "*Dataset:* no interactions, reduced adjacencies\n",
    "\n",
    "{'activation': 'tanh', 'solver': 'lbfgs', 'alpha': 0.001, 'hidden_layer_sizes': (5,)}\n",
    "\n",
    "With a training score of:\n",
    "-0.615696513033\n",
    "\n",
    "testing log loss = 0.6271359\n",
    "\n",
    "Kaggle score: 0.62571\n",
    "\n",
    "---------------------------------------------------------    \n",
    "*Dataset:* all interactions, all adjacencies \n",
    "\n",
    "{'activation': 'tanh', 'solver': 'sgd', 'alpha': 1.0000000000000001e-05, 'hidden_layer_sizes': (4,)}\n",
    "\n",
    "With a training score of:\n",
    "-0.624649655765\n",
    "\n",
    "testing log loss: 0.61738848385994771\n",
    "\n",
    "Kaggle score: 0.62300\n",
    "\n",
    "---------------------------------------------------------   \n",
    "*Dataset:* all interactions, all adjacencies \n",
    "\n",
    "{activation ='tanh',alpha =  1.0000000000000001e-05,hidden_layer_sizes = (4,),solver = 'sgd' }\n",
    "\n",
    "Best Mean log loss on the kth fold during training:\n",
    "0.624649655765\n",
    "\n",
    "log loss from the best set of grid search parameters on my test set:\n",
    "0.62584323078537163\n",
    "\n",
    "Kaggle score:  0.62556\n",
    "\n",
    "-------------------------------------------------------------------------\n",
    "*Dataset*: corrected scaling, small model \n",
    "\n",
    "Models details: \n",
    "\n",
    "random seed = 7, possibily with cross validation? \n",
    "#stop early if testing error gets large: \n",
    "callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "\n",
    "#Create the model: \n",
    "model = Sequential()\n",
    "\n",
    "#Input layer: \n",
    "model.add(Dense(50, input_dim= inshape, activation = 'tanh',              \n",
    "                #kernel_regularizer=regularizers.l2(0.001),\n",
    "                #activity_regularizer=regularizers.l1(0.001), \n",
    "                kernel_initializer= initializers.glorot_normal(seed=None))) \n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))     \n",
    "\n",
    "#One hidden layer: \n",
    "model.add(Dense(50,  activation = 'tanh',\n",
    "                #kernel_regularizer=regularizers.l2(0.001),\n",
    "                #activity_regularizer=regularizers.l1(0.001), \n",
    "                kernel_initializer= initializers.glorot_normal(seed=None)))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#Second hidden layer: \n",
    "model.add(Dense(50, activation = 'tanh', \n",
    "                #kernel_regularizer=regularizers.l2(0.01),\n",
    "                #activity_regularizer=regularizers.l1(0.01), \n",
    "                kernel_initializer= initializers.glorot_normal(seed=None)))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#Output layer \n",
    "model.add(Dense(3, activation='softmax',\n",
    "                #kernel_regularizer=regularizers.l2(0.01),\n",
    "                #activity_regularizer=regularizers.l1(0.01), \n",
    "                kernel_initializer= initializers.glorot_normal(seed=None)))\n",
    "\n",
    "#Setting up to optimize the weights: \n",
    "sgd = optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train_small, categorical_labels_train , epochs=150, batch_size=10)\n",
    "\n",
    "My test set loss: 0.60261446516877804\n",
    "\n",
    "Training loss:0.6401 \n",
    "\n",
    "Kaggle score: 0.61680"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a kaggle submission: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74464/74659 [============================>.] - ETA: 0s\n",
      " percentage predicted low label:\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.68023771,  0.29222378,  0.02753843],\n",
       "       [ 0.89006209,  0.09311015,  0.01682781],\n",
       "       [ 0.97496641,  0.02343635,  0.00159733],\n",
       "       ..., \n",
       "       [ 0.88118148,  0.10383328,  0.01498524],\n",
       "       [ 0.97141707,  0.02684902,  0.00173388],\n",
       "       [ 0.61404651,  0.34472221,  0.0412313 ]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_proba_holdout = model.predict_proba(X_holdout_small)\n",
    "\n",
    "#sanity check (that doesn't work for keras models)\n",
    "y_pred_holdout = model.predict(X_holdout_small)\n",
    "print(\"\\n percentage predicted low label:\")\n",
    "print(len(y_pred_holdout[y_pred_holdout== 1])/X_holdout_small.shape[0]) \n",
    "\n",
    "y_pred_proba_holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "        \"listing_id\": holdout[\"listing_id\"],\n",
    "        \"high\": y_pred_proba_holdout[:,2],\n",
    "        \"medium\":y_pred_proba_holdout[:,1],\n",
    "        \"low\": y_pred_proba_holdout[:,0]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>high</th>\n",
       "      <th>medium</th>\n",
       "      <th>low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7142618</td>\n",
       "      <td>0.038962</td>\n",
       "      <td>0.249934</td>\n",
       "      <td>0.711104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7210040</td>\n",
       "      <td>0.023934</td>\n",
       "      <td>0.108455</td>\n",
       "      <td>0.867611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7174566</td>\n",
       "      <td>0.001243</td>\n",
       "      <td>0.026781</td>\n",
       "      <td>0.971976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7191391</td>\n",
       "      <td>0.134686</td>\n",
       "      <td>0.382049</td>\n",
       "      <td>0.483265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7171695</td>\n",
       "      <td>0.012598</td>\n",
       "      <td>0.144559</td>\n",
       "      <td>0.842843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id      high    medium       low\n",
       "0     7142618  0.038962  0.249934  0.711104\n",
       "1     7210040  0.023934  0.108455  0.867611\n",
       "2     7174566  0.001243  0.026781  0.971976\n",
       "3     7191391  0.134686  0.382049  0.483265\n",
       "4     7171695  0.012598  0.144559  0.842843"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columnsTitles=[\"listing_id\",\"high\",\"medium\",\"low\"]\n",
    "submission=submission.reindex(columns=columnsTitles)\n",
    "submission.head()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission_keras2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Dropout\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import regularizers\n",
    "from keras import callbacks\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import optimizers, initializers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Keras needs the labels to be in binary 0/1 vectors for each class: \n",
    "\n",
    "categorical_labels_train = to_categorical(y_train_small-1, num_classes=None)\n",
    "categorical_labels_test = to_categorical(y_test-1, num_classes=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About this model:\n",
    " - In the input layer you need to specify the number of predictors coming in from the dataset\n",
    " - Output layer size should be the number of classes & the activaction should be softmax to get the predic_proba\n",
    " - Using dropout helps determine overfitting. 25% dropout means 25% of the hidden units are randomly removed from the model.\n",
    " - Batch Normalizatin noramlizes the activations fo the previous layer at each batch , i.e. applies a transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1.\n",
    " - Xavier initialization makes sure the weights start off at the right size so they don't shrink through each layer so too small to be useful and so they don't start to grow too massive to be useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#input layer shape:\n",
    "inshape = X_train_small.shape[1]\n",
    "\n",
    "#stop early if testing error gets large: \n",
    "callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "\n",
    "#Create the model: \n",
    "model = Sequential()\n",
    "\n",
    "#Input layer: \n",
    "model.add(Dense(50, input_dim= inshape, activation = 'tanh',              \n",
    "                #kernel_regularizer=regularizers.l2(0.001),\n",
    "                #activity_regularizer=regularizers.l1(0.001), \n",
    "                kernel_initializer= initializers.glorot_normal(seed=None))) \n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))     \n",
    "\n",
    "#One hidden layer: \n",
    "model.add(Dense(50,  activation = 'tanh',\n",
    "                #kernel_regularizer=regularizers.l2(0.001),\n",
    "                #activity_regularizer=regularizers.l1(0.001), \n",
    "                kernel_initializer= initializers.glorot_normal(seed=None)))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#Second hidden layer: \n",
    "model.add(Dense(50, activation = 'tanh', \n",
    "                #kernel_regularizer=regularizers.l2(0.01),\n",
    "                #activity_regularizer=regularizers.l1(0.01), \n",
    "                kernel_initializer= initializers.glorot_normal(seed=None)))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#Output layer \n",
    "model.add(Dense(3, activation='softmax',\n",
    "                #kernel_regularizer=regularizers.l2(0.01),\n",
    "                #activity_regularizer=regularizers.l1(0.01), \n",
    "                kernel_initializer= initializers.glorot_normal(seed=None)))\n",
    "\n",
    "#Setting up to optimize the weights: \n",
    "sgd = optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "37002/37002 [==============================] - 13s - loss: 0.7191 - acc: 0.6925    \n",
      "Epoch 2/10\n",
      "37002/37002 [==============================] - 10s - loss: 0.6763 - acc: 0.7041    \n",
      "Epoch 3/10\n",
      "37002/37002 [==============================] - 10s - loss: 0.6640 - acc: 0.7059    \n",
      "Epoch 4/10\n",
      "37002/37002 [==============================] - 11s - loss: 0.6558 - acc: 0.7101    \n",
      "Epoch 5/10\n",
      "37002/37002 [==============================] - 12s - loss: 0.6516 - acc: 0.7143    \n",
      "Epoch 6/10\n",
      "37002/37002 [==============================] - 11s - loss: 0.6509 - acc: 0.7124    \n",
      "Epoch 7/10\n",
      "37002/37002 [==============================] - 11s - loss: 0.6475 - acc: 0.7133    \n",
      "Epoch 8/10\n",
      "37002/37002 [==============================] - 11s - loss: 0.6460 - acc: 0.7133    \n",
      "Epoch 9/10\n",
      "37002/37002 [==============================] - 11s - loss: 0.6434 - acc: 0.7143    \n",
      "Epoch 10/10\n",
      "37002/37002 [==============================] - 11s - loss: 0.6401 - acc: 0.7149    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17488244198>"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Without grid search: \n",
    "model.fit(X_train_small, categorical_labels_train , epochs=10, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss on the test set:\n",
      "11872/12335 [===========================>..] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.60261446516877804"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the probabilities on the test set for the Keras model:\n",
    "\n",
    "print(\"log loss on the test set:\")\n",
    "proba = model.predict_proba(X_test_small)\n",
    "log_loss(y_test, proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras Cross Val Take2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold number: 1\n",
      "Epoch 1/150\n",
      "39469/39469 [==============================] - 8s - loss: 0.7199 - acc: 0.6917     \n",
      "Epoch 2/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6771 - acc: 0.7017     \n",
      "Epoch 3/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6650 - acc: 0.7058     \n",
      "Epoch 4/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6593 - acc: 0.7087     \n",
      "Epoch 5/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6549 - acc: 0.7109     \n",
      "Epoch 6/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6533 - acc: 0.7087     \n",
      "Epoch 7/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6508 - acc: 0.7106     \n",
      "Epoch 8/150\n",
      "39469/39469 [==============================] - 8s - loss: 0.6471 - acc: 0.7130     \n",
      "Epoch 9/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6445 - acc: 0.7111     \n",
      "Epoch 10/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6425 - acc: 0.7112     \n",
      "Epoch 11/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6406 - acc: 0.7135     \n",
      "Epoch 12/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6405 - acc: 0.7123     \n",
      "Epoch 13/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6411 - acc: 0.7135     \n",
      "Epoch 14/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6372 - acc: 0.7160     \n",
      "Epoch 15/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6371 - acc: 0.7183     \n",
      "Epoch 16/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6366 - acc: 0.7151     \n",
      "Epoch 17/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6348 - acc: 0.7169     \n",
      "Epoch 18/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6340 - acc: 0.7159     \n",
      "Epoch 19/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6354 - acc: 0.7139     \n",
      "Epoch 20/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6332 - acc: 0.7163     \n",
      "Epoch 21/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6342 - acc: 0.7148     \n",
      "Epoch 22/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6321 - acc: 0.7194     \n",
      "Epoch 23/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6312 - acc: 0.7169     \n",
      "Epoch 24/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6311 - acc: 0.7180     \n",
      "Epoch 25/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6306 - acc: 0.7173     \n",
      "Epoch 26/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6322 - acc: 0.7169     \n",
      "Epoch 27/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6295 - acc: 0.7203     \n",
      "Epoch 28/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6297 - acc: 0.7193     \n",
      "Epoch 29/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6293 - acc: 0.7184     \n",
      "Epoch 30/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6275 - acc: 0.7186     \n",
      "Epoch 31/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6295 - acc: 0.7191     \n",
      "Epoch 32/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6294 - acc: 0.7199     \n",
      "Epoch 33/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6265 - acc: 0.7202     \n",
      "Epoch 34/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6273 - acc: 0.7178     \n",
      "Epoch 35/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6292 - acc: 0.7190     \n",
      "Epoch 36/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6265 - acc: 0.7199     \n",
      "Epoch 37/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6271 - acc: 0.7202     \n",
      "Epoch 38/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6245 - acc: 0.7222     \n",
      "Epoch 39/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6276 - acc: 0.7179     \n",
      "Epoch 40/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6269 - acc: 0.7204     \n",
      "Epoch 41/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6249 - acc: 0.7202     \n",
      "Epoch 42/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6263 - acc: 0.7224     \n",
      "Epoch 43/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6263 - acc: 0.7197     \n",
      "Epoch 44/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6235 - acc: 0.7228     \n",
      "Epoch 45/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6259 - acc: 0.7196     \n",
      "Epoch 46/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6258 - acc: 0.7193     \n",
      "Epoch 47/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6248 - acc: 0.7192     \n",
      "Epoch 48/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6259 - acc: 0.7210     \n",
      "Epoch 49/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6249 - acc: 0.7221     \n",
      "Epoch 50/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6261 - acc: 0.7205     \n",
      "Epoch 51/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6235 - acc: 0.7206     \n",
      "Epoch 52/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6253 - acc: 0.7191     \n",
      "Epoch 53/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6220 - acc: 0.7240     \n",
      "Epoch 54/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6236 - acc: 0.7217     \n",
      "Epoch 55/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6242 - acc: 0.7235     \n",
      "Epoch 56/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6219 - acc: 0.7219     \n",
      "Epoch 57/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6222 - acc: 0.7226     \n",
      "Epoch 58/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6229 - acc: 0.7220     \n",
      "Epoch 59/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6213 - acc: 0.7212     \n",
      "Epoch 60/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6223 - acc: 0.7200     \n",
      "Epoch 61/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6212 - acc: 0.7228     \n",
      "Epoch 62/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6226 - acc: 0.7211     \n",
      "Epoch 63/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6226 - acc: 0.7207     \n",
      "Epoch 64/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6241 - acc: 0.7219     \n",
      "Epoch 65/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6212 - acc: 0.7215     \n",
      "Epoch 66/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6210 - acc: 0.7215     \n",
      "Epoch 67/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6223 - acc: 0.7202     \n",
      "Epoch 68/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6221 - acc: 0.7233     \n",
      "Epoch 69/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6218 - acc: 0.7196     \n",
      "Epoch 70/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6220 - acc: 0.7222     \n",
      "Epoch 71/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6210 - acc: 0.7233     \n",
      "Epoch 72/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6217 - acc: 0.7235     \n",
      "Epoch 73/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6213 - acc: 0.7227     \n",
      "Epoch 74/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6191 - acc: 0.7257     \n",
      "Epoch 75/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6215 - acc: 0.7236     \n",
      "Epoch 76/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6217 - acc: 0.7247     \n",
      "Epoch 77/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6199 - acc: 0.7249     \n",
      "Epoch 78/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6207 - acc: 0.7216     \n",
      "Epoch 79/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6187 - acc: 0.7241     \n",
      "Epoch 80/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6220 - acc: 0.7224     \n",
      "Epoch 81/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6211 - acc: 0.7243     \n",
      "Epoch 82/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6203 - acc: 0.7200     \n",
      "Epoch 83/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6195 - acc: 0.7247     \n",
      "Epoch 84/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6192 - acc: 0.7249     \n",
      "Epoch 85/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6193 - acc: 0.7253     \n",
      "Epoch 86/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6210 - acc: 0.7219     \n",
      "Epoch 87/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6181 - acc: 0.7261     \n",
      "Epoch 88/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6194 - acc: 0.7259     \n",
      "Epoch 89/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6216 - acc: 0.7237     \n",
      "Epoch 90/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6180 - acc: 0.7233     \n",
      "Epoch 91/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6191 - acc: 0.7243     \n",
      "Epoch 92/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6223 - acc: 0.7229     \n",
      "Epoch 93/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6188 - acc: 0.7227     \n",
      "Epoch 94/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6169 - acc: 0.7254     \n",
      "Epoch 95/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6200 - acc: 0.7232     \n",
      "Epoch 96/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6195 - acc: 0.7245     \n",
      "Epoch 97/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6178 - acc: 0.7239     \n",
      "Epoch 98/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6188 - acc: 0.7241     \n",
      "Epoch 99/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6202 - acc: 0.7249     \n",
      "Epoch 100/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6181 - acc: 0.7241     \n",
      "Epoch 101/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6191 - acc: 0.7240     \n",
      "Epoch 102/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6212 - acc: 0.7228     \n",
      "Epoch 103/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6181 - acc: 0.7227     \n",
      "Epoch 104/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6180 - acc: 0.7261     \n",
      "Epoch 105/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6183 - acc: 0.7261     \n",
      "Epoch 106/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6196 - acc: 0.7242     \n",
      "Epoch 107/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6200 - acc: 0.7234     \n",
      "Epoch 108/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6174 - acc: 0.7249     \n",
      "Epoch 109/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6184 - acc: 0.7250     \n",
      "Epoch 110/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6177 - acc: 0.7237     \n",
      "Epoch 111/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6181 - acc: 0.7247     \n",
      "Epoch 112/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6186 - acc: 0.7265     \n",
      "Epoch 113/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6190 - acc: 0.7233     \n",
      "Epoch 114/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6168 - acc: 0.7246     \n",
      "Epoch 115/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6178 - acc: 0.7245     \n",
      "Epoch 116/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6183 - acc: 0.7246     \n",
      "Epoch 117/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6195 - acc: 0.7243     \n",
      "Epoch 118/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6188 - acc: 0.7232     \n",
      "Epoch 119/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6178 - acc: 0.7249     \n",
      "Epoch 120/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6177 - acc: 0.7237     \n",
      "Epoch 121/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6206 - acc: 0.7246     \n",
      "Epoch 122/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6181 - acc: 0.7255     \n",
      "Epoch 123/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6191 - acc: 0.7230     \n",
      "Epoch 124/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6170 - acc: 0.7247     \n",
      "Epoch 125/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6151 - acc: 0.7255     \n",
      "Epoch 126/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6178 - acc: 0.7250     \n",
      "Epoch 127/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6187 - acc: 0.7265     \n",
      "Epoch 128/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6147 - acc: 0.7274     \n",
      "Epoch 129/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6169 - acc: 0.7236     \n",
      "Epoch 130/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6154 - acc: 0.7264     \n",
      "Epoch 131/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6180 - acc: 0.7242     \n",
      "Epoch 132/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6166 - acc: 0.7264     \n",
      "Epoch 133/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6171 - acc: 0.7244     \n",
      "Epoch 134/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6174 - acc: 0.7243     \n",
      "Epoch 135/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6176 - acc: 0.7214     \n",
      "Epoch 136/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6174 - acc: 0.7238     \n",
      "Epoch 137/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6180 - acc: 0.7267     \n",
      "Epoch 138/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6168 - acc: 0.7237     \n",
      "Epoch 139/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6163 - acc: 0.7242     \n",
      "Epoch 140/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6171 - acc: 0.7254     \n",
      "Epoch 141/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6159 - acc: 0.7257     \n",
      "Epoch 142/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6185 - acc: 0.7260     \n",
      "Epoch 143/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6151 - acc: 0.7267     \n",
      "Epoch 144/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6156 - acc: 0.7251     \n",
      "Epoch 145/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6174 - acc: 0.7256     \n",
      "Epoch 146/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6151 - acc: 0.7248     \n",
      "Epoch 147/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6174 - acc: 0.7251     \n",
      "Epoch 148/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6142 - acc: 0.7258     \n",
      "Epoch 149/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6166 - acc: 0.7262     \n",
      "Epoch 150/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6173 - acc: 0.7243     \n",
      "log loss on test k-th fold: 0.610538127599\n",
      "fold number: 2\n",
      "Epoch 1/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.7199 - acc: 0.6944     \n",
      "Epoch 2/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6755 - acc: 0.7026     \n",
      "Epoch 3/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6621 - acc: 0.7064     \n",
      "Epoch 4/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6572 - acc: 0.7094     \n",
      "Epoch 5/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6540 - acc: 0.7106     \n",
      "Epoch 6/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6530 - acc: 0.7108     \n",
      "Epoch 7/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6500 - acc: 0.7073     \n",
      "Epoch 8/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6455 - acc: 0.7106     \n",
      "Epoch 9/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6420 - acc: 0.7129     \n",
      "Epoch 10/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6437 - acc: 0.7129     \n",
      "Epoch 11/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6410 - acc: 0.7142     \n",
      "Epoch 12/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6388 - acc: 0.7145     \n",
      "Epoch 13/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6409 - acc: 0.7129     \n",
      "Epoch 14/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6359 - acc: 0.7153     \n",
      "Epoch 15/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6362 - acc: 0.7156     \n",
      "Epoch 16/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6357 - acc: 0.7175     \n",
      "Epoch 17/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6339 - acc: 0.7153     \n",
      "Epoch 18/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6360 - acc: 0.7142     \n",
      "Epoch 19/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6333 - acc: 0.7174     \n",
      "Epoch 20/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6327 - acc: 0.7160     \n",
      "Epoch 21/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6320 - acc: 0.7169     \n",
      "Epoch 22/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6329 - acc: 0.7137     \n",
      "Epoch 23/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6298 - acc: 0.7156     \n",
      "Epoch 24/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6300 - acc: 0.7166     \n",
      "Epoch 25/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6317 - acc: 0.7156     \n",
      "Epoch 26/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6279 - acc: 0.7188     \n",
      "Epoch 27/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6286 - acc: 0.7189     \n",
      "Epoch 28/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6288 - acc: 0.7183     \n",
      "Epoch 29/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6290 - acc: 0.7194     \n",
      "Epoch 30/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6296 - acc: 0.7188     \n",
      "Epoch 31/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6276 - acc: 0.7177     \n",
      "Epoch 32/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6277 - acc: 0.7160     \n",
      "Epoch 33/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6260 - acc: 0.7192     \n",
      "Epoch 34/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6302 - acc: 0.7181     \n",
      "Epoch 35/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6273 - acc: 0.7195     \n",
      "Epoch 36/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6268 - acc: 0.7191     \n",
      "Epoch 37/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6269 - acc: 0.7177     \n",
      "Epoch 38/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6266 - acc: 0.7185     \n",
      "Epoch 39/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6264 - acc: 0.7203     \n",
      "Epoch 40/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6254 - acc: 0.7200     \n",
      "Epoch 41/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6242 - acc: 0.7205     \n",
      "Epoch 42/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6242 - acc: 0.7198     \n",
      "Epoch 43/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6230 - acc: 0.7190     \n",
      "Epoch 44/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6231 - acc: 0.7201     \n",
      "Epoch 45/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6238 - acc: 0.7203     \n",
      "Epoch 46/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6242 - acc: 0.7192     \n",
      "Epoch 47/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6241 - acc: 0.7201     \n",
      "Epoch 48/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6234 - acc: 0.7207     \n",
      "Epoch 49/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6243 - acc: 0.7197     \n",
      "Epoch 50/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6236 - acc: 0.7207     \n",
      "Epoch 51/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6238 - acc: 0.7191     \n",
      "Epoch 52/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6230 - acc: 0.7212     \n",
      "Epoch 53/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6257 - acc: 0.7206     \n",
      "Epoch 54/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6209 - acc: 0.7239     \n",
      "Epoch 55/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6239 - acc: 0.7197     \n",
      "Epoch 56/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6241 - acc: 0.7195     \n",
      "Epoch 57/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6215 - acc: 0.7222     \n",
      "Epoch 58/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6229 - acc: 0.7222     \n",
      "Epoch 59/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6213 - acc: 0.7208     \n",
      "Epoch 60/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6244 - acc: 0.7201     \n",
      "Epoch 61/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6224 - acc: 0.7192     \n",
      "Epoch 62/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6228 - acc: 0.7207     \n",
      "Epoch 63/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6226 - acc: 0.7185     \n",
      "Epoch 64/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6234 - acc: 0.7189     \n",
      "Epoch 65/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6197 - acc: 0.7207     \n",
      "Epoch 66/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6215 - acc: 0.7201     \n",
      "Epoch 67/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6205 - acc: 0.7199     \n",
      "Epoch 68/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6207 - acc: 0.7227     \n",
      "Epoch 69/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6210 - acc: 0.7211     \n",
      "Epoch 70/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6203 - acc: 0.7201     \n",
      "Epoch 71/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6210 - acc: 0.7199     \n",
      "Epoch 72/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6197 - acc: 0.7227     \n",
      "Epoch 73/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6209 - acc: 0.7204     \n",
      "Epoch 74/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6196 - acc: 0.7211     \n",
      "Epoch 75/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6192 - acc: 0.7237     \n",
      "Epoch 76/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6200 - acc: 0.7220     \n",
      "Epoch 77/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6216 - acc: 0.7209     \n",
      "Epoch 78/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6216 - acc: 0.7225     \n",
      "Epoch 79/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6219 - acc: 0.7205     \n",
      "Epoch 80/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6193 - acc: 0.7220     \n",
      "Epoch 81/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6208 - acc: 0.7230     \n",
      "Epoch 82/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6173 - acc: 0.7237     \n",
      "Epoch 83/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6198 - acc: 0.7247     \n",
      "Epoch 84/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6216 - acc: 0.7215     \n",
      "Epoch 85/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6192 - acc: 0.7244     \n",
      "Epoch 86/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6181 - acc: 0.7260     \n",
      "Epoch 87/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6185 - acc: 0.7253     \n",
      "Epoch 88/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6180 - acc: 0.7209     \n",
      "Epoch 89/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6185 - acc: 0.7245     \n",
      "Epoch 90/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6181 - acc: 0.7226     \n",
      "Epoch 91/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6179 - acc: 0.7217     \n",
      "Epoch 92/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6171 - acc: 0.7230     \n",
      "Epoch 93/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6173 - acc: 0.7215     \n",
      "Epoch 94/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6185 - acc: 0.7236     \n",
      "Epoch 95/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6181 - acc: 0.7224     \n",
      "Epoch 96/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6196 - acc: 0.7221     \n",
      "Epoch 97/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6178 - acc: 0.7220     \n",
      "Epoch 98/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6179 - acc: 0.7199     \n",
      "Epoch 99/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6198 - acc: 0.7206     \n",
      "Epoch 100/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6202 - acc: 0.7228     \n",
      "Epoch 101/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6187 - acc: 0.7212     \n",
      "Epoch 102/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6159 - acc: 0.7236     \n",
      "Epoch 103/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6172 - acc: 0.7229     \n",
      "Epoch 104/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6182 - acc: 0.7225     \n",
      "Epoch 105/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6169 - acc: 0.7237     \n",
      "Epoch 106/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6178 - acc: 0.7214     \n",
      "Epoch 107/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6181 - acc: 0.7233     \n",
      "Epoch 108/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6180 - acc: 0.7234     \n",
      "Epoch 109/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6162 - acc: 0.7249     \n",
      "Epoch 110/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6172 - acc: 0.7219     \n",
      "Epoch 111/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6169 - acc: 0.7228     \n",
      "Epoch 112/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6168 - acc: 0.7238     \n",
      "Epoch 113/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6156 - acc: 0.7254     \n",
      "Epoch 114/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6155 - acc: 0.7245     \n",
      "Epoch 115/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6154 - acc: 0.7221     \n",
      "Epoch 116/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6170 - acc: 0.7218     \n",
      "Epoch 117/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6169 - acc: 0.7230     \n",
      "Epoch 118/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6175 - acc: 0.7227     \n",
      "Epoch 119/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6159 - acc: 0.7249     \n",
      "Epoch 120/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6182 - acc: 0.7227     \n",
      "Epoch 121/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6180 - acc: 0.7230     \n",
      "Epoch 122/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6159 - acc: 0.7243     \n",
      "Epoch 123/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6178 - acc: 0.7235     \n",
      "Epoch 124/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6137 - acc: 0.7261     \n",
      "Epoch 125/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6166 - acc: 0.7235     \n",
      "Epoch 126/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6158 - acc: 0.7248     \n",
      "Epoch 127/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6166 - acc: 0.7242     \n",
      "Epoch 128/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6167 - acc: 0.7235     \n",
      "Epoch 129/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6164 - acc: 0.7244     \n",
      "Epoch 130/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6141 - acc: 0.7240     \n",
      "Epoch 131/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6158 - acc: 0.7227     \n",
      "Epoch 132/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6163 - acc: 0.7255     \n",
      "Epoch 133/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6157 - acc: 0.7224     \n",
      "Epoch 134/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6169 - acc: 0.7245     \n",
      "Epoch 135/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6171 - acc: 0.7248     \n",
      "Epoch 136/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6154 - acc: 0.7226     \n",
      "Epoch 137/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6164 - acc: 0.7237     \n",
      "Epoch 138/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6172 - acc: 0.7234     \n",
      "Epoch 139/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6153 - acc: 0.7231     \n",
      "Epoch 140/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6164 - acc: 0.7233     \n",
      "Epoch 141/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6174 - acc: 0.7223     \n",
      "Epoch 142/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6158 - acc: 0.7238     \n",
      "Epoch 143/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6159 - acc: 0.7260     \n",
      "Epoch 144/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6154 - acc: 0.7256     \n",
      "Epoch 145/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6144 - acc: 0.7270     \n",
      "Epoch 146/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6162 - acc: 0.7229     \n",
      "Epoch 147/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6175 - acc: 0.7217     \n",
      "Epoch 148/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6151 - acc: 0.7221     \n",
      "Epoch 149/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6175 - acc: 0.7233     \n",
      "Epoch 150/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6159 - acc: 0.7217     \n",
      "log loss on test k-th fold: 0.619472794389\n",
      "fold number: 3\n",
      "Epoch 1/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.7150 - acc: 0.6950     \n",
      "Epoch 2/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6742 - acc: 0.7041     \n",
      "Epoch 3/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6620 - acc: 0.7074     \n",
      "Epoch 4/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6568 - acc: 0.7083     \n",
      "Epoch 5/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6532 - acc: 0.7093     \n",
      "Epoch 6/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6494 - acc: 0.7110     \n",
      "Epoch 7/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6469 - acc: 0.7117     \n",
      "Epoch 8/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6454 - acc: 0.7123     \n",
      "Epoch 9/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6430 - acc: 0.7125     \n",
      "Epoch 10/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6409 - acc: 0.7132     \n",
      "Epoch 11/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6388 - acc: 0.7153     \n",
      "Epoch 12/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6389 - acc: 0.7141     \n",
      "Epoch 13/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6369 - acc: 0.7137     \n",
      "Epoch 14/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6363 - acc: 0.7167     \n",
      "Epoch 15/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6364 - acc: 0.7177     \n",
      "Epoch 16/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6368 - acc: 0.7151     \n",
      "Epoch 17/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6350 - acc: 0.7152     \n",
      "Epoch 18/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6333 - acc: 0.7170     \n",
      "Epoch 19/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6327 - acc: 0.7154     \n",
      "Epoch 20/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6317 - acc: 0.7172     \n",
      "Epoch 21/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6319 - acc: 0.7147     \n",
      "Epoch 22/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6304 - acc: 0.7168     \n",
      "Epoch 23/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6320 - acc: 0.7170     \n",
      "Epoch 24/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6289 - acc: 0.7162     \n",
      "Epoch 25/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6274 - acc: 0.7173     \n",
      "Epoch 26/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6286 - acc: 0.7179     \n",
      "Epoch 27/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6288 - acc: 0.7160     \n",
      "Epoch 28/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6297 - acc: 0.7183     \n",
      "Epoch 29/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6313 - acc: 0.7163     \n",
      "Epoch 30/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6275 - acc: 0.7182     \n",
      "Epoch 31/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6275 - acc: 0.7177     \n",
      "Epoch 32/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6280 - acc: 0.7176     \n",
      "Epoch 33/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6261 - acc: 0.7184     \n",
      "Epoch 34/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6260 - acc: 0.7196     \n",
      "Epoch 35/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6254 - acc: 0.7215     \n",
      "Epoch 36/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6243 - acc: 0.7229     \n",
      "Epoch 37/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6264 - acc: 0.7197     \n",
      "Epoch 38/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6261 - acc: 0.7184     \n",
      "Epoch 39/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6237 - acc: 0.7201     \n",
      "Epoch 40/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6249 - acc: 0.7198     \n",
      "Epoch 41/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6236 - acc: 0.7200     \n",
      "Epoch 42/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6253 - acc: 0.7172     \n",
      "Epoch 43/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6261 - acc: 0.7197     \n",
      "Epoch 44/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6239 - acc: 0.7206     \n",
      "Epoch 45/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6245 - acc: 0.7191     \n",
      "Epoch 46/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6241 - acc: 0.7215     \n",
      "Epoch 47/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6235 - acc: 0.7203     \n",
      "Epoch 48/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6239 - acc: 0.7212     \n",
      "Epoch 49/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6231 - acc: 0.7216     \n",
      "Epoch 50/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6217 - acc: 0.7213     \n",
      "Epoch 51/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6222 - acc: 0.7219     \n",
      "Epoch 52/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6235 - acc: 0.7228     \n",
      "Epoch 53/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6221 - acc: 0.7216     \n",
      "Epoch 54/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6230 - acc: 0.7204     \n",
      "Epoch 55/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6223 - acc: 0.7221     \n",
      "Epoch 56/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6222 - acc: 0.7198     \n",
      "Epoch 57/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6227 - acc: 0.7219     \n",
      "Epoch 58/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6238 - acc: 0.7204     \n",
      "Epoch 59/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6206 - acc: 0.7229     \n",
      "Epoch 60/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6211 - acc: 0.7248     \n",
      "Epoch 61/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6202 - acc: 0.7246     \n",
      "Epoch 62/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6209 - acc: 0.7222     \n",
      "Epoch 63/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6215 - acc: 0.7249     \n",
      "Epoch 64/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6214 - acc: 0.7213     \n",
      "Epoch 65/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6210 - acc: 0.7228     \n",
      "Epoch 66/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6194 - acc: 0.7237     \n",
      "Epoch 67/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6206 - acc: 0.7222     \n",
      "Epoch 68/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6199 - acc: 0.7250     \n",
      "Epoch 69/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6208 - acc: 0.7226     \n",
      "Epoch 70/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6211 - acc: 0.7229     \n",
      "Epoch 71/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6196 - acc: 0.7246     \n",
      "Epoch 72/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6194 - acc: 0.7203     \n",
      "Epoch 73/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6215 - acc: 0.7218     \n",
      "Epoch 74/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6171 - acc: 0.7241     \n",
      "Epoch 75/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6208 - acc: 0.7219     \n",
      "Epoch 76/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6183 - acc: 0.7236     \n",
      "Epoch 77/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6194 - acc: 0.7217     \n",
      "Epoch 78/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6188 - acc: 0.7224     \n",
      "Epoch 79/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6193 - acc: 0.7251     \n",
      "Epoch 80/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6173 - acc: 0.7246     \n",
      "Epoch 81/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6180 - acc: 0.7231     \n",
      "Epoch 82/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6197 - acc: 0.7234     \n",
      "Epoch 83/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6200 - acc: 0.7230     \n",
      "Epoch 84/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6231 - acc: 0.7215     \n",
      "Epoch 85/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6197 - acc: 0.7244     \n",
      "Epoch 86/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6174 - acc: 0.7261     \n",
      "Epoch 87/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6168 - acc: 0.7241     \n",
      "Epoch 88/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6183 - acc: 0.7227     \n",
      "Epoch 89/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6155 - acc: 0.7259     \n",
      "Epoch 90/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6175 - acc: 0.7257     \n",
      "Epoch 91/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6194 - acc: 0.7240     \n",
      "Epoch 92/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6166 - acc: 0.7225     \n",
      "Epoch 93/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6185 - acc: 0.7238     \n",
      "Epoch 94/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6183 - acc: 0.7236     \n",
      "Epoch 95/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6166 - acc: 0.7239     \n",
      "Epoch 96/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6175 - acc: 0.7252     \n",
      "Epoch 97/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6195 - acc: 0.7216     \n",
      "Epoch 98/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6191 - acc: 0.7238     \n",
      "Epoch 99/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6197 - acc: 0.7223     \n",
      "Epoch 100/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6157 - acc: 0.7261     \n",
      "Epoch 101/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6174 - acc: 0.7248     \n",
      "Epoch 102/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6186 - acc: 0.7243     \n",
      "Epoch 103/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6161 - acc: 0.7246     \n",
      "Epoch 104/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6155 - acc: 0.7247     \n",
      "Epoch 105/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6146 - acc: 0.7280     \n",
      "Epoch 106/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6167 - acc: 0.7249     \n",
      "Epoch 107/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6180 - acc: 0.7247     \n",
      "Epoch 108/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6179 - acc: 0.7256     \n",
      "Epoch 109/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6175 - acc: 0.7239     \n",
      "Epoch 110/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6147 - acc: 0.7271     \n",
      "Epoch 111/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6175 - acc: 0.7257     \n",
      "Epoch 112/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6175 - acc: 0.7251     \n",
      "Epoch 113/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6168 - acc: 0.7249     \n",
      "Epoch 114/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6163 - acc: 0.7277     \n",
      "Epoch 115/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6173 - acc: 0.7239     \n",
      "Epoch 116/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6185 - acc: 0.7253     \n",
      "Epoch 117/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6177 - acc: 0.7253     \n",
      "Epoch 118/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6164 - acc: 0.7239     \n",
      "Epoch 119/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6137 - acc: 0.7271     \n",
      "Epoch 120/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6155 - acc: 0.7268     \n",
      "Epoch 121/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6143 - acc: 0.7261     \n",
      "Epoch 122/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6167 - acc: 0.7275     \n",
      "Epoch 123/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6144 - acc: 0.7267     \n",
      "Epoch 124/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6135 - acc: 0.7278     \n",
      "Epoch 125/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6162 - acc: 0.7260     \n",
      "Epoch 126/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6157 - acc: 0.7272     \n",
      "Epoch 127/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6167 - acc: 0.7242     \n",
      "Epoch 128/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6142 - acc: 0.7243     \n",
      "Epoch 129/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6155 - acc: 0.7262     \n",
      "Epoch 130/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6155 - acc: 0.7268     \n",
      "Epoch 131/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6173 - acc: 0.7259     \n",
      "Epoch 132/150\n",
      "39469/39469 [==============================] - 7s - loss: 0.6142 - acc: 0.7273     \n",
      "Epoch 133/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6171 - acc: 0.7237     \n",
      "Epoch 134/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6150 - acc: 0.7280     \n",
      "Epoch 135/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6154 - acc: 0.7278     \n",
      "Epoch 136/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6148 - acc: 0.7268     \n",
      "Epoch 137/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6166 - acc: 0.7246     \n",
      "Epoch 138/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6125 - acc: 0.7278     \n",
      "Epoch 139/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6170 - acc: 0.7254     \n",
      "Epoch 140/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6138 - acc: 0.7271     \n",
      "Epoch 141/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6138 - acc: 0.7262     \n",
      "Epoch 142/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6156 - acc: 0.7259     \n",
      "Epoch 143/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6147 - acc: 0.7236     \n",
      "Epoch 144/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6142 - acc: 0.7252     \n",
      "Epoch 145/150\n",
      "39469/39469 [==============================] - 6s - loss: 0.6148 - acc: 0.7261     \n",
      "Epoch 146/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6122 - acc: 0.7258     \n",
      "Epoch 147/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6138 - acc: 0.7259     \n",
      "Epoch 148/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6130 - acc: 0.7268     \n",
      "Epoch 149/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6150 - acc: 0.7267     \n",
      "Epoch 150/150\n",
      "39469/39469 [==============================] - 5s - loss: 0.6160 - acc: 0.7265     \n",
      "log loss on test k-th fold: 0.614531164426\n",
      "fold number: 4\n",
      "Epoch 1/150\n",
      "39470/39470 [==============================] - 6s - loss: 0.7161 - acc: 0.6945     \n",
      "Epoch 2/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6766 - acc: 0.7048     \n",
      "Epoch 3/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6632 - acc: 0.7060     \n",
      "Epoch 4/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6580 - acc: 0.7070     \n",
      "Epoch 5/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6553 - acc: 0.7072     \n",
      "Epoch 6/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6514 - acc: 0.7104     \n",
      "Epoch 7/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6478 - acc: 0.7103     \n",
      "Epoch 8/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6479 - acc: 0.7114     \n",
      "Epoch 9/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6446 - acc: 0.7123     \n",
      "Epoch 10/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6429 - acc: 0.7122     \n",
      "Epoch 11/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6418 - acc: 0.7155     \n",
      "Epoch 12/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6401 - acc: 0.7135     \n",
      "Epoch 13/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6385 - acc: 0.7166     \n",
      "Epoch 14/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6378 - acc: 0.7127     \n",
      "Epoch 15/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6360 - acc: 0.7145     \n",
      "Epoch 16/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6355 - acc: 0.7165     \n",
      "Epoch 17/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6308 - acc: 0.7186     \n",
      "Epoch 18/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6326 - acc: 0.7159     \n",
      "Epoch 19/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6356 - acc: 0.7158     \n",
      "Epoch 20/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6338 - acc: 0.7158     \n",
      "Epoch 21/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6346 - acc: 0.7174     \n",
      "Epoch 22/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6306 - acc: 0.7159     \n",
      "Epoch 23/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6318 - acc: 0.7164     \n",
      "Epoch 24/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6302 - acc: 0.7176     \n",
      "Epoch 25/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6304 - acc: 0.7187     \n",
      "Epoch 26/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6298 - acc: 0.7208     \n",
      "Epoch 27/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6315 - acc: 0.7196     \n",
      "Epoch 28/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6291 - acc: 0.7175     \n",
      "Epoch 29/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6280 - acc: 0.7173     \n",
      "Epoch 30/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6292 - acc: 0.7202     \n",
      "Epoch 31/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6282 - acc: 0.7197     \n",
      "Epoch 32/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6273 - acc: 0.7194     \n",
      "Epoch 33/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6298 - acc: 0.7191     \n",
      "Epoch 34/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6286 - acc: 0.7163     \n",
      "Epoch 35/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6276 - acc: 0.7217     \n",
      "Epoch 36/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6262 - acc: 0.7208     \n",
      "Epoch 37/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6268 - acc: 0.7199     \n",
      "Epoch 38/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6259 - acc: 0.7201     \n",
      "Epoch 39/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6253 - acc: 0.7190     \n",
      "Epoch 40/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6273 - acc: 0.7201     \n",
      "Epoch 41/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6252 - acc: 0.7192     \n",
      "Epoch 42/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6263 - acc: 0.7195     \n",
      "Epoch 43/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6251 - acc: 0.7202     \n",
      "Epoch 44/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6253 - acc: 0.7212     \n",
      "Epoch 45/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6252 - acc: 0.7232     \n",
      "Epoch 46/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6251 - acc: 0.7207     \n",
      "Epoch 47/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6247 - acc: 0.7218     \n",
      "Epoch 48/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6240 - acc: 0.7211     \n",
      "Epoch 49/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6251 - acc: 0.7188     \n",
      "Epoch 50/150\n",
      "39470/39470 [==============================] - 6s - loss: 0.6252 - acc: 0.7195     \n",
      "Epoch 51/150\n",
      "39470/39470 [==============================] - 6s - loss: 0.6247 - acc: 0.7214     \n",
      "Epoch 52/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6247 - acc: 0.7196     \n",
      "Epoch 53/150\n",
      "39470/39470 [==============================] - 6s - loss: 0.6245 - acc: 0.7224     \n",
      "Epoch 54/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6239 - acc: 0.7188     \n",
      "Epoch 55/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6233 - acc: 0.7217     \n",
      "Epoch 56/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6223 - acc: 0.7212     \n",
      "Epoch 57/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6217 - acc: 0.7212     \n",
      "Epoch 58/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6232 - acc: 0.7194     \n",
      "Epoch 59/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6239 - acc: 0.7204     \n",
      "Epoch 60/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6219 - acc: 0.7230     \n",
      "Epoch 61/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6249 - acc: 0.7223     \n",
      "Epoch 62/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6220 - acc: 0.7248     \n",
      "Epoch 63/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6216 - acc: 0.7238     \n",
      "Epoch 64/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6225 - acc: 0.7254     \n",
      "Epoch 65/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6213 - acc: 0.7208     \n",
      "Epoch 66/150\n",
      "39470/39470 [==============================] - 6s - loss: 0.6208 - acc: 0.7259     \n",
      "Epoch 67/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6224 - acc: 0.7226     \n",
      "Epoch 68/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6221 - acc: 0.7226     \n",
      "Epoch 69/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6224 - acc: 0.7216     \n",
      "Epoch 70/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6215 - acc: 0.7221     \n",
      "Epoch 71/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6205 - acc: 0.7263     \n",
      "Epoch 72/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6222 - acc: 0.7233     \n",
      "Epoch 73/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6198 - acc: 0.7234     \n",
      "Epoch 74/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6194 - acc: 0.7226     \n",
      "Epoch 75/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6205 - acc: 0.7232     \n",
      "Epoch 76/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6201 - acc: 0.7247     \n",
      "Epoch 77/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6190 - acc: 0.7259     \n",
      "Epoch 78/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6210 - acc: 0.7232     \n",
      "Epoch 79/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6192 - acc: 0.7251     \n",
      "Epoch 80/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6212 - acc: 0.7241     \n",
      "Epoch 81/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6190 - acc: 0.7225     \n",
      "Epoch 82/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6203 - acc: 0.7247     \n",
      "Epoch 83/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6201 - acc: 0.7242     \n",
      "Epoch 84/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6228 - acc: 0.7243     \n",
      "Epoch 85/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6192 - acc: 0.7244     \n",
      "Epoch 86/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6199 - acc: 0.7216     \n",
      "Epoch 87/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6194 - acc: 0.7233     \n",
      "Epoch 88/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6188 - acc: 0.7235     \n",
      "Epoch 89/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6203 - acc: 0.7243     \n",
      "Epoch 90/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6195 - acc: 0.7240     \n",
      "Epoch 91/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6172 - acc: 0.7252     \n",
      "Epoch 92/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6215 - acc: 0.7224     \n",
      "Epoch 93/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6208 - acc: 0.7211     \n",
      "Epoch 94/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6174 - acc: 0.7248     \n",
      "Epoch 95/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6205 - acc: 0.7222     \n",
      "Epoch 96/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6215 - acc: 0.7243     \n",
      "Epoch 97/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6195 - acc: 0.7240     \n",
      "Epoch 98/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6185 - acc: 0.7230     \n",
      "Epoch 99/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6197 - acc: 0.7231     \n",
      "Epoch 100/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6178 - acc: 0.7236     \n",
      "Epoch 101/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6168 - acc: 0.7250     \n",
      "Epoch 102/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6197 - acc: 0.7244     \n",
      "Epoch 103/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6180 - acc: 0.7228     \n",
      "Epoch 104/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6173 - acc: 0.7248     \n",
      "Epoch 105/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6169 - acc: 0.7274     \n",
      "Epoch 106/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6206 - acc: 0.7234     \n",
      "Epoch 107/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6177 - acc: 0.7243     \n",
      "Epoch 108/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6181 - acc: 0.7256     \n",
      "Epoch 109/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6178 - acc: 0.7225     \n",
      "Epoch 110/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6194 - acc: 0.7250     \n",
      "Epoch 111/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6174 - acc: 0.7238     \n",
      "Epoch 112/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6175 - acc: 0.7275     \n",
      "Epoch 113/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6219 - acc: 0.7232     \n",
      "Epoch 114/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6170 - acc: 0.7277     \n",
      "Epoch 115/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6208 - acc: 0.7245     \n",
      "Epoch 116/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6182 - acc: 0.7245     \n",
      "Epoch 117/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6185 - acc: 0.7228     \n",
      "Epoch 118/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6162 - acc: 0.7256     \n",
      "Epoch 119/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6152 - acc: 0.7274     \n",
      "Epoch 120/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6175 - acc: 0.7267     \n",
      "Epoch 121/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6149 - acc: 0.7270     \n",
      "Epoch 122/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6155 - acc: 0.7241     \n",
      "Epoch 123/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6159 - acc: 0.7237     \n",
      "Epoch 124/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6156 - acc: 0.7254     \n",
      "Epoch 125/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6156 - acc: 0.7275     \n",
      "Epoch 126/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6190 - acc: 0.7252     \n",
      "Epoch 127/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6196 - acc: 0.7232     \n",
      "Epoch 128/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6178 - acc: 0.7262     \n",
      "Epoch 129/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6187 - acc: 0.7256     \n",
      "Epoch 130/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6173 - acc: 0.7259     \n",
      "Epoch 131/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6166 - acc: 0.7262     \n",
      "Epoch 132/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6188 - acc: 0.7221     \n",
      "Epoch 133/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6174 - acc: 0.7258     \n",
      "Epoch 134/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6155 - acc: 0.7246     \n",
      "Epoch 135/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6162 - acc: 0.7270     \n",
      "Epoch 136/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6186 - acc: 0.7235     \n",
      "Epoch 137/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6190 - acc: 0.7225     \n",
      "Epoch 138/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6173 - acc: 0.7242     \n",
      "Epoch 139/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6159 - acc: 0.7261     \n",
      "Epoch 140/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6184 - acc: 0.7235     \n",
      "Epoch 141/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6154 - acc: 0.7265     \n",
      "Epoch 142/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6165 - acc: 0.7234     \n",
      "Epoch 143/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6160 - acc: 0.7237     \n",
      "Epoch 144/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6158 - acc: 0.7269     \n",
      "Epoch 145/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6160 - acc: 0.7243     \n",
      "Epoch 146/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6178 - acc: 0.7239     \n",
      "Epoch 147/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6168 - acc: 0.7262     \n",
      "Epoch 148/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6158 - acc: 0.7246     \n",
      "Epoch 149/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6171 - acc: 0.7240     \n",
      "Epoch 150/150\n",
      "39470/39470 [==============================] - 5s - loss: 0.6165 - acc: 0.7269     \n",
      "log loss on test k-th fold: 0.610277442665\n",
      "fold number: 5\n",
      "Epoch 1/150\n",
      "39471/39471 [==============================] - 6s - loss: 0.7158 - acc: 0.6931     \n",
      "Epoch 2/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6744 - acc: 0.7031     \n",
      "Epoch 3/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6651 - acc: 0.7059     \n",
      "Epoch 4/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6589 - acc: 0.7074     \n",
      "Epoch 5/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6543 - acc: 0.7115     \n",
      "Epoch 6/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6516 - acc: 0.7099     \n",
      "Epoch 7/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6497 - acc: 0.7109     \n",
      "Epoch 8/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6471 - acc: 0.7122     \n",
      "Epoch 9/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6424 - acc: 0.7125     \n",
      "Epoch 10/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6431 - acc: 0.7140     \n",
      "Epoch 11/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6408 - acc: 0.7119     \n",
      "Epoch 12/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6391 - acc: 0.7142     \n",
      "Epoch 13/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6375 - acc: 0.7154     \n",
      "Epoch 14/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6385 - acc: 0.7141     \n",
      "Epoch 15/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6367 - acc: 0.7149     \n",
      "Epoch 16/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6356 - acc: 0.7171     \n",
      "Epoch 17/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6350 - acc: 0.7153     \n",
      "Epoch 18/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6345 - acc: 0.7180     \n",
      "Epoch 19/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6341 - acc: 0.7173     \n",
      "Epoch 20/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6344 - acc: 0.7152     \n",
      "Epoch 21/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6335 - acc: 0.7170     \n",
      "Epoch 22/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6317 - acc: 0.7169     \n",
      "Epoch 23/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6321 - acc: 0.7165     \n",
      "Epoch 24/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6293 - acc: 0.7172     \n",
      "Epoch 25/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6313 - acc: 0.7194     \n",
      "Epoch 26/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6301 - acc: 0.7175     \n",
      "Epoch 27/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6294 - acc: 0.7175     \n",
      "Epoch 28/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6308 - acc: 0.7189     \n",
      "Epoch 29/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6300 - acc: 0.7189     \n",
      "Epoch 30/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6293 - acc: 0.7200     \n",
      "Epoch 31/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6280 - acc: 0.7184     \n",
      "Epoch 32/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6259 - acc: 0.7204     \n",
      "Epoch 33/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6293 - acc: 0.7180     \n",
      "Epoch 34/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6263 - acc: 0.7205     \n",
      "Epoch 35/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6291 - acc: 0.7181     \n",
      "Epoch 36/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6284 - acc: 0.7189     \n",
      "Epoch 37/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6271 - acc: 0.7185     \n",
      "Epoch 38/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6255 - acc: 0.7199     \n",
      "Epoch 39/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6256 - acc: 0.7216     \n",
      "Epoch 40/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6265 - acc: 0.7211     \n",
      "Epoch 41/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6241 - acc: 0.7234     \n",
      "Epoch 42/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6254 - acc: 0.7221     \n",
      "Epoch 43/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6248 - acc: 0.7220     \n",
      "Epoch 44/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6245 - acc: 0.7208     \n",
      "Epoch 45/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6248 - acc: 0.7228     \n",
      "Epoch 46/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6270 - acc: 0.7193     \n",
      "Epoch 47/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6250 - acc: 0.7189     \n",
      "Epoch 48/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6238 - acc: 0.7211     \n",
      "Epoch 49/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6257 - acc: 0.7185     \n",
      "Epoch 50/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6247 - acc: 0.7200     \n",
      "Epoch 51/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6244 - acc: 0.7186     \n",
      "Epoch 52/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6251 - acc: 0.7238     \n",
      "Epoch 53/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6233 - acc: 0.7217     \n",
      "Epoch 54/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6245 - acc: 0.7215     \n",
      "Epoch 55/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6231 - acc: 0.7239     \n",
      "Epoch 56/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6244 - acc: 0.7249     \n",
      "Epoch 57/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6231 - acc: 0.7183     \n",
      "Epoch 58/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6248 - acc: 0.7196     \n",
      "Epoch 59/150\n",
      "39471/39471 [==============================] - 6s - loss: 0.6249 - acc: 0.7203     \n",
      "Epoch 60/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6211 - acc: 0.7224     \n",
      "Epoch 61/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6240 - acc: 0.7205     \n",
      "Epoch 62/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6245 - acc: 0.7200     \n",
      "Epoch 63/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6227 - acc: 0.7212     \n",
      "Epoch 64/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6203 - acc: 0.7230     \n",
      "Epoch 65/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6237 - acc: 0.7217     \n",
      "Epoch 66/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6227 - acc: 0.7217     \n",
      "Epoch 67/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6237 - acc: 0.7200     \n",
      "Epoch 68/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6213 - acc: 0.7228     \n",
      "Epoch 69/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6220 - acc: 0.7222     \n",
      "Epoch 70/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6221 - acc: 0.7196     \n",
      "Epoch 71/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6236 - acc: 0.7208     \n",
      "Epoch 72/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6225 - acc: 0.7218     \n",
      "Epoch 73/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6235 - acc: 0.7227     \n",
      "Epoch 74/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6220 - acc: 0.7207     \n",
      "Epoch 75/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6204 - acc: 0.7216     \n",
      "Epoch 76/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6199 - acc: 0.7236     \n",
      "Epoch 77/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6240 - acc: 0.7206     \n",
      "Epoch 78/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6222 - acc: 0.7188     \n",
      "Epoch 79/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6206 - acc: 0.7222     \n",
      "Epoch 80/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6220 - acc: 0.7231     \n",
      "Epoch 81/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6218 - acc: 0.7209     \n",
      "Epoch 82/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6211 - acc: 0.7235     \n",
      "Epoch 83/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6210 - acc: 0.7224     \n",
      "Epoch 84/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6216 - acc: 0.7243     \n",
      "Epoch 85/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6228 - acc: 0.7231     \n",
      "Epoch 86/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6208 - acc: 0.7213     \n",
      "Epoch 87/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6214 - acc: 0.7211     \n",
      "Epoch 88/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6205 - acc: 0.7243     \n",
      "Epoch 89/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6219 - acc: 0.7207     \n",
      "Epoch 90/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6186 - acc: 0.7238     \n",
      "Epoch 91/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6188 - acc: 0.7237     \n",
      "Epoch 92/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6204 - acc: 0.7227     \n",
      "Epoch 93/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6202 - acc: 0.7213     \n",
      "Epoch 94/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6204 - acc: 0.7253     \n",
      "Epoch 95/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6194 - acc: 0.7224     \n",
      "Epoch 96/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6179 - acc: 0.7235     \n",
      "Epoch 97/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6207 - acc: 0.7224     \n",
      "Epoch 98/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6202 - acc: 0.7223     \n",
      "Epoch 99/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6172 - acc: 0.7255     \n",
      "Epoch 100/150\n",
      "39471/39471 [==============================] - 6s - loss: 0.6180 - acc: 0.7268     \n",
      "Epoch 101/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6197 - acc: 0.7226     \n",
      "Epoch 102/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6189 - acc: 0.7246     \n",
      "Epoch 103/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6206 - acc: 0.7232     \n",
      "Epoch 104/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6205 - acc: 0.7239     \n",
      "Epoch 105/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6179 - acc: 0.7242     \n",
      "Epoch 106/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6172 - acc: 0.7236     \n",
      "Epoch 107/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6195 - acc: 0.7229     \n",
      "Epoch 108/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6188 - acc: 0.7239     \n",
      "Epoch 109/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6203 - acc: 0.7229     \n",
      "Epoch 110/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6183 - acc: 0.7239     \n",
      "Epoch 111/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6190 - acc: 0.7242     \n",
      "Epoch 112/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6173 - acc: 0.7230     \n",
      "Epoch 113/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6192 - acc: 0.7224     \n",
      "Epoch 114/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6186 - acc: 0.7231     \n",
      "Epoch 115/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6190 - acc: 0.7217     \n",
      "Epoch 116/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6197 - acc: 0.7224     \n",
      "Epoch 117/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6194 - acc: 0.7233     \n",
      "Epoch 118/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6180 - acc: 0.7261     \n",
      "Epoch 119/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6190 - acc: 0.7244     \n",
      "Epoch 120/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6175 - acc: 0.7228     \n",
      "Epoch 121/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6167 - acc: 0.7243     \n",
      "Epoch 122/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6164 - acc: 0.7243     \n",
      "Epoch 123/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6170 - acc: 0.7243     \n",
      "Epoch 124/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6198 - acc: 0.7231     \n",
      "Epoch 125/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6178 - acc: 0.7236     \n",
      "Epoch 126/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6169 - acc: 0.7229     \n",
      "Epoch 127/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6200 - acc: 0.7220     \n",
      "Epoch 128/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6170 - acc: 0.7234     \n",
      "Epoch 129/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6172 - acc: 0.7220     \n",
      "Epoch 130/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6174 - acc: 0.7255     \n",
      "Epoch 131/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6185 - acc: 0.7251     \n",
      "Epoch 132/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6186 - acc: 0.7241     \n",
      "Epoch 133/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6167 - acc: 0.7252     \n",
      "Epoch 134/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6198 - acc: 0.7219     \n",
      "Epoch 135/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6183 - acc: 0.7225     \n",
      "Epoch 136/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6176 - acc: 0.7232     \n",
      "Epoch 137/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6162 - acc: 0.7230     \n",
      "Epoch 138/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6174 - acc: 0.7261     \n",
      "Epoch 139/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6191 - acc: 0.7242     \n",
      "Epoch 140/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6167 - acc: 0.7240     \n",
      "Epoch 141/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6194 - acc: 0.7227     \n",
      "Epoch 142/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6180 - acc: 0.7232     \n",
      "Epoch 143/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6171 - acc: 0.7270     \n",
      "Epoch 144/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6169 - acc: 0.7230     \n",
      "Epoch 145/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6159 - acc: 0.7260     \n",
      "Epoch 146/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6188 - acc: 0.7226     \n",
      "Epoch 147/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6184 - acc: 0.7235     \n",
      "Epoch 148/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6182 - acc: 0.7242     \n",
      "Epoch 149/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6170 - acc: 0.7251     \n",
      "Epoch 150/150\n",
      "39471/39471 [==============================] - 5s - loss: 0.6174 - acc: 0.7241     \n",
      "log loss on test k-th fold: 0.61750358006\n",
      "\n",
      "mean testing log loss:\n",
      "0.614464621828 +/- 0.00366800528139\n"
     ]
    }
   ],
   "source": [
    "# define 5-fold cross validation test harness\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\n",
    "cvscores = []\n",
    "i=1\n",
    "\n",
    "for train, test in kfold.split(X_train_all_small, y_train_all_small):\n",
    "    print(\"fold number:\", i)\n",
    "    model = Sequential()\n",
    "    \n",
    "    inshape = X_train_small.shape[1]\n",
    "\n",
    "    #Input layer: \n",
    "    model.add(Dense(50, input_dim= inshape, activation = 'tanh',              \n",
    "                #kernel_regularizer=regularizers.l2(0.001),\n",
    "                #activity_regularizer=regularizers.l1(0.001), \n",
    "                kernel_initializer= initializers.glorot_normal(seed=None))) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))     \n",
    "\n",
    "    #One hidden layer: \n",
    "    model.add(Dense(50,  activation = 'tanh',\n",
    "                #kernel_regularizer=regularizers.l2(0.001),\n",
    "                #activity_regularizer=regularizers.l1(0.001), \n",
    "                kernel_initializer= initializers.glorot_normal(seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    #Second hidden layer: \n",
    "    model.add(Dense(50, activation = 'tanh', \n",
    "                #kernel_regularizer=regularizers.l2(0.01),\n",
    "                #activity_regularizer=regularizers.l1(0.01), \n",
    "                kernel_initializer= initializers.glorot_normal(seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    #Output layer \n",
    "    model.add(Dense(3, activation='softmax',\n",
    "                #kernel_regularizer=regularizers.l2(0.01),\n",
    "                #activity_regularizer=regularizers.l1(0.01), \n",
    "                kernel_initializer= initializers.glorot_normal(seed=None)))\n",
    "\n",
    "    #Setting up to optimize the weights: \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    categorical_labels = to_categorical(y_train_all_small[train]-1, num_classes=None)\n",
    "    model.fit(X_train_all_small[train],categorical_labels, epochs=150, batch_size=10, verbose=1)\n",
    "    \n",
    "    # evaluate the model\n",
    "    categorical_labels_test = to_categorical(y_train_all_small[test]-1, num_classes=None)\n",
    "    scores = model.evaluate(X_train_all_small[test], categorical_labels_test, verbose=0)\n",
    "    print(\"log loss on test k-th fold:\", scores[0])\n",
    "    cvscores.append(scores[0])\n",
    "    \n",
    "    i+=1\n",
    "\n",
    "print()\n",
    "print(\"mean testing log loss:\")\n",
    "print(numpy.mean(cvscores), \"+/-\", numpy.std(cvscores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss on the test set:\n",
      "10752/12335 [=========================>....] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.58680211244944525"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "print(\"log loss on the test set:\")\n",
    "proba = model.predict_proba(X_test_small)\n",
    "log_loss(y_test, proba)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle score: 0.62309"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras Cross Val Take 3  (Standardizing all the binary colums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold number: 1\n",
      "Epoch 1/100\n",
      "39469/39469 [==============================] - 8s - loss: 0.7158 - acc: 0.6898     \n",
      "Epoch 2/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6750 - acc: 0.7014     \n",
      "Epoch 3/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6614 - acc: 0.7071     \n",
      "Epoch 4/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6561 - acc: 0.7094     \n",
      "Epoch 5/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6516 - acc: 0.7092     \n",
      "Epoch 6/100\n",
      "39469/39469 [==============================] - 8s - loss: 0.6494 - acc: 0.7143     \n",
      "Epoch 7/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6468 - acc: 0.7141     \n",
      "Epoch 8/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6445 - acc: 0.7122     \n",
      "Epoch 9/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6435 - acc: 0.7125     \n",
      "Epoch 10/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6391 - acc: 0.7164     \n",
      "Epoch 11/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6361 - acc: 0.7171     \n",
      "Epoch 12/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6368 - acc: 0.7165     \n",
      "Epoch 13/100\n",
      "39469/39469 [==============================] - 8s - loss: 0.6370 - acc: 0.7180     \n",
      "Epoch 14/100\n",
      "39469/39469 [==============================] - 8s - loss: 0.6353 - acc: 0.7177     \n",
      "Epoch 15/100\n",
      "39469/39469 [==============================] - 8s - loss: 0.6331 - acc: 0.7165     \n",
      "Epoch 16/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6335 - acc: 0.7170     \n",
      "Epoch 17/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6324 - acc: 0.7201     \n",
      "Epoch 18/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6300 - acc: 0.7207     \n",
      "Epoch 19/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6300 - acc: 0.7204     \n",
      "Epoch 20/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6293 - acc: 0.7199     \n",
      "Epoch 21/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6289 - acc: 0.7190     \n",
      "Epoch 22/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6288 - acc: 0.7192     \n",
      "Epoch 23/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6272 - acc: 0.7187     \n",
      "Epoch 24/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6272 - acc: 0.7214     \n",
      "Epoch 25/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6292 - acc: 0.7206     \n",
      "Epoch 26/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6264 - acc: 0.7213     \n",
      "Epoch 27/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6285 - acc: 0.7181     \n",
      "Epoch 28/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6243 - acc: 0.7224     \n",
      "Epoch 29/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6248 - acc: 0.7233     \n",
      "Epoch 30/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6246 - acc: 0.7206     \n",
      "Epoch 31/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6255 - acc: 0.7225     \n",
      "Epoch 32/100\n",
      "39469/39469 [==============================] - 8s - loss: 0.6231 - acc: 0.7222     \n",
      "Epoch 33/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6234 - acc: 0.7235     \n",
      "Epoch 34/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6239 - acc: 0.7200     \n",
      "Epoch 35/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6240 - acc: 0.7218     \n",
      "Epoch 36/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6244 - acc: 0.7218     \n",
      "Epoch 37/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6235 - acc: 0.7222     \n",
      "Epoch 38/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6232 - acc: 0.7212     \n",
      "Epoch 39/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6210 - acc: 0.7244     \n",
      "Epoch 40/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6223 - acc: 0.7219     \n",
      "Epoch 41/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6230 - acc: 0.7231     \n",
      "Epoch 42/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6229 - acc: 0.7243     \n",
      "Epoch 43/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6213 - acc: 0.7227     \n",
      "Epoch 44/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6205 - acc: 0.7263     \n",
      "Epoch 45/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6210 - acc: 0.7234     \n",
      "Epoch 46/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6200 - acc: 0.7212     \n",
      "Epoch 47/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6198 - acc: 0.7236     \n",
      "Epoch 48/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6194 - acc: 0.7230     \n",
      "Epoch 49/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6196 - acc: 0.7239     \n",
      "Epoch 50/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6219 - acc: 0.7226     \n",
      "Epoch 51/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6220 - acc: 0.7239     \n",
      "Epoch 52/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6203 - acc: 0.7253     \n",
      "Epoch 53/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6208 - acc: 0.7218     \n",
      "Epoch 54/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6178 - acc: 0.7250     \n",
      "Epoch 55/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6180 - acc: 0.7252     \n",
      "Epoch 56/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6195 - acc: 0.7250     \n",
      "Epoch 57/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6196 - acc: 0.7249     \n",
      "Epoch 58/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6156 - acc: 0.7244     \n",
      "Epoch 59/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6169 - acc: 0.7244     \n",
      "Epoch 60/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6189 - acc: 0.7250     \n",
      "Epoch 61/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6184 - acc: 0.7263     \n",
      "Epoch 62/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6197 - acc: 0.7231     \n",
      "Epoch 63/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6171 - acc: 0.7261     \n",
      "Epoch 64/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6193 - acc: 0.7233     \n",
      "Epoch 65/100\n",
      "39469/39469 [==============================] - 8s - loss: 0.6171 - acc: 0.7250     \n",
      "Epoch 66/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6176 - acc: 0.7236     \n",
      "Epoch 67/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6180 - acc: 0.7267     \n",
      "Epoch 68/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6183 - acc: 0.7235     \n",
      "Epoch 69/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6166 - acc: 0.7236     \n",
      "Epoch 70/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6170 - acc: 0.7238     \n",
      "Epoch 71/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6194 - acc: 0.7260     \n",
      "Epoch 72/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6174 - acc: 0.7246     \n",
      "Epoch 73/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6168 - acc: 0.7261     \n",
      "Epoch 74/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6173 - acc: 0.7245     \n",
      "Epoch 75/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6163 - acc: 0.7250     \n",
      "Epoch 76/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6152 - acc: 0.7274     \n",
      "Epoch 77/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6187 - acc: 0.7246     \n",
      "Epoch 78/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6160 - acc: 0.7274     \n",
      "Epoch 79/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6162 - acc: 0.7266     \n",
      "Epoch 80/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6161 - acc: 0.7252     \n",
      "Epoch 81/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6150 - acc: 0.7258     \n",
      "Epoch 82/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6160 - acc: 0.7242     \n",
      "Epoch 83/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6151 - acc: 0.7278     \n",
      "Epoch 84/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6171 - acc: 0.7254     \n",
      "Epoch 85/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6152 - acc: 0.7251     \n",
      "Epoch 86/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6152 - acc: 0.7261     \n",
      "Epoch 87/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6160 - acc: 0.7252     \n",
      "Epoch 88/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6156 - acc: 0.7268     \n",
      "Epoch 89/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6155 - acc: 0.7283     \n",
      "Epoch 90/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6155 - acc: 0.7268     \n",
      "Epoch 91/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6159 - acc: 0.7260     \n",
      "Epoch 92/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6133 - acc: 0.7277     \n",
      "Epoch 93/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6145 - acc: 0.7246     \n",
      "Epoch 94/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6149 - acc: 0.7281     \n",
      "Epoch 95/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6150 - acc: 0.7268     \n",
      "Epoch 96/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6156 - acc: 0.7262     \n",
      "Epoch 97/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6145 - acc: 0.7248     \n",
      "Epoch 98/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6144 - acc: 0.7253     \n",
      "Epoch 99/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6144 - acc: 0.7261     \n",
      "Epoch 100/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6147 - acc: 0.7266     \n",
      "log loss on test k-th fold: 0.61167664899\n",
      "fold number: 2\n",
      "Epoch 1/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.7199 - acc: 0.6881     \n",
      "Epoch 2/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6743 - acc: 0.7014     \n",
      "Epoch 3/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6613 - acc: 0.7063     \n",
      "Epoch 4/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6561 - acc: 0.7110     \n",
      "Epoch 5/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6528 - acc: 0.7101     \n",
      "Epoch 6/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6482 - acc: 0.7124     \n",
      "Epoch 7/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6457 - acc: 0.7141     \n",
      "Epoch 8/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6429 - acc: 0.7153     \n",
      "Epoch 9/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6412 - acc: 0.7139     \n",
      "Epoch 10/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6393 - acc: 0.7155     \n",
      "Epoch 11/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6364 - acc: 0.7157     \n",
      "Epoch 12/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6366 - acc: 0.7140     \n",
      "Epoch 13/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6350 - acc: 0.7156     \n",
      "Epoch 14/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6341 - acc: 0.7137     \n",
      "Epoch 15/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6347 - acc: 0.7158     \n",
      "Epoch 16/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6316 - acc: 0.7185     \n",
      "Epoch 17/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6312 - acc: 0.7162     \n",
      "Epoch 18/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6315 - acc: 0.7188     \n",
      "Epoch 19/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6293 - acc: 0.7184     \n",
      "Epoch 20/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6271 - acc: 0.7186     \n",
      "Epoch 21/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6296 - acc: 0.7169     \n",
      "Epoch 22/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6264 - acc: 0.7203     \n",
      "Epoch 23/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6253 - acc: 0.7173     \n",
      "Epoch 24/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6257 - acc: 0.7209     \n",
      "Epoch 25/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6265 - acc: 0.7182     \n",
      "Epoch 26/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6255 - acc: 0.7209     \n",
      "Epoch 27/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6253 - acc: 0.7211     \n",
      "Epoch 28/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6273 - acc: 0.7188     \n",
      "Epoch 29/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6242 - acc: 0.7206     \n",
      "Epoch 30/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6254 - acc: 0.7193     \n",
      "Epoch 31/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6224 - acc: 0.7195     \n",
      "Epoch 32/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6222 - acc: 0.7207     \n",
      "Epoch 33/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6227 - acc: 0.7216     \n",
      "Epoch 34/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6220 - acc: 0.7214     \n",
      "Epoch 35/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6230 - acc: 0.7220     \n",
      "Epoch 36/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6216 - acc: 0.7235     \n",
      "Epoch 37/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6210 - acc: 0.7213     \n",
      "Epoch 38/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6227 - acc: 0.7216     \n",
      "Epoch 39/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6199 - acc: 0.7222     \n",
      "Epoch 40/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6216 - acc: 0.7225     \n",
      "Epoch 41/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6189 - acc: 0.7235     \n",
      "Epoch 42/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6219 - acc: 0.7203     \n",
      "Epoch 43/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6204 - acc: 0.7227     \n",
      "Epoch 44/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6209 - acc: 0.7221     \n",
      "Epoch 45/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6187 - acc: 0.7228     \n",
      "Epoch 46/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6194 - acc: 0.7219     \n",
      "Epoch 47/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6184 - acc: 0.7224     \n",
      "Epoch 48/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6216 - acc: 0.7219     \n",
      "Epoch 49/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6187 - acc: 0.7219     \n",
      "Epoch 50/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6183 - acc: 0.7255     \n",
      "Epoch 51/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6183 - acc: 0.7223     \n",
      "Epoch 52/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6176 - acc: 0.7222     \n",
      "Epoch 53/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6178 - acc: 0.7224     \n",
      "Epoch 54/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6167 - acc: 0.7217     \n",
      "Epoch 55/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6188 - acc: 0.7222     \n",
      "Epoch 56/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6177 - acc: 0.7235     \n",
      "Epoch 57/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6183 - acc: 0.7215     \n",
      "Epoch 58/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6179 - acc: 0.7233     \n",
      "Epoch 59/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6169 - acc: 0.7212     \n",
      "Epoch 60/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6162 - acc: 0.7238     \n",
      "Epoch 61/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6158 - acc: 0.7246     \n",
      "Epoch 62/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6165 - acc: 0.7226     \n",
      "Epoch 63/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6181 - acc: 0.7226     \n",
      "Epoch 64/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6189 - acc: 0.7210     \n",
      "Epoch 65/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6140 - acc: 0.7265     \n",
      "Epoch 66/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6169 - acc: 0.7234     \n",
      "Epoch 67/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6141 - acc: 0.7218     \n",
      "Epoch 68/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6167 - acc: 0.7216     \n",
      "Epoch 69/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6169 - acc: 0.7242     \n",
      "Epoch 70/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6147 - acc: 0.7267     \n",
      "Epoch 71/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6137 - acc: 0.7263     \n",
      "Epoch 72/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6145 - acc: 0.7262     \n",
      "Epoch 73/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6162 - acc: 0.7248     \n",
      "Epoch 74/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6152 - acc: 0.7246     \n",
      "Epoch 75/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6150 - acc: 0.7240     \n",
      "Epoch 76/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6136 - acc: 0.7257     \n",
      "Epoch 77/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6154 - acc: 0.7253     \n",
      "Epoch 78/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6156 - acc: 0.7252     \n",
      "Epoch 79/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6128 - acc: 0.7255     \n",
      "Epoch 80/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6140 - acc: 0.7256     \n",
      "Epoch 81/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6166 - acc: 0.7220     \n",
      "Epoch 82/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6151 - acc: 0.7239     \n",
      "Epoch 83/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6151 - acc: 0.7249     \n",
      "Epoch 84/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6128 - acc: 0.7255     \n",
      "Epoch 85/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6159 - acc: 0.7255     \n",
      "Epoch 86/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6144 - acc: 0.7248     \n",
      "Epoch 87/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6148 - acc: 0.7265     \n",
      "Epoch 88/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6128 - acc: 0.7266     \n",
      "Epoch 89/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6139 - acc: 0.7248     \n",
      "Epoch 90/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6138 - acc: 0.7234     \n",
      "Epoch 91/100\n",
      "39469/39469 [==============================] - 7s - loss: 0.6158 - acc: 0.7239     \n",
      "Epoch 92/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6135 - acc: 0.7256     \n",
      "Epoch 93/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6159 - acc: 0.7237     \n",
      "Epoch 94/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6139 - acc: 0.7251     \n",
      "Epoch 95/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6139 - acc: 0.7272     \n",
      "Epoch 96/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6159 - acc: 0.7244     \n",
      "Epoch 97/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6129 - acc: 0.7266     \n",
      "Epoch 98/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6138 - acc: 0.7240     \n",
      "Epoch 99/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6116 - acc: 0.7287     \n",
      "Epoch 100/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6126 - acc: 0.7263     \n",
      "log loss on test k-th fold: 0.616046484999\n",
      "fold number: 3\n",
      "Epoch 1/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.7169 - acc: 0.6889     \n",
      "Epoch 2/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6715 - acc: 0.7039     \n",
      "Epoch 3/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6602 - acc: 0.7093     \n",
      "Epoch 4/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6584 - acc: 0.7096     \n",
      "Epoch 5/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6511 - acc: 0.7106     \n",
      "Epoch 6/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6473 - acc: 0.7120     \n",
      "Epoch 7/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6453 - acc: 0.7145     \n",
      "Epoch 8/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6447 - acc: 0.7128     \n",
      "Epoch 9/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6423 - acc: 0.7157     \n",
      "Epoch 10/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6400 - acc: 0.7156     \n",
      "Epoch 11/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6392 - acc: 0.7155     \n",
      "Epoch 12/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6381 - acc: 0.7166     \n",
      "Epoch 13/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6364 - acc: 0.7172     \n",
      "Epoch 14/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6366 - acc: 0.7139     \n",
      "Epoch 15/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6331 - acc: 0.7180     \n",
      "Epoch 16/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6345 - acc: 0.7185     \n",
      "Epoch 17/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6345 - acc: 0.7166     \n",
      "Epoch 18/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6336 - acc: 0.7155     \n",
      "Epoch 19/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6308 - acc: 0.7185     \n",
      "Epoch 20/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6315 - acc: 0.7164     \n",
      "Epoch 21/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6301 - acc: 0.7204     \n",
      "Epoch 22/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6272 - acc: 0.7202     \n",
      "Epoch 23/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6276 - acc: 0.7195     \n",
      "Epoch 24/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6297 - acc: 0.7201     \n",
      "Epoch 25/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6274 - acc: 0.7200     \n",
      "Epoch 26/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6276 - acc: 0.7201     \n",
      "Epoch 27/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6280 - acc: 0.7192     \n",
      "Epoch 28/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6263 - acc: 0.7203     \n",
      "Epoch 29/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6260 - acc: 0.7207     \n",
      "Epoch 30/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6262 - acc: 0.7231     \n",
      "Epoch 31/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6270 - acc: 0.7212     \n",
      "Epoch 32/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6253 - acc: 0.7218     \n",
      "Epoch 33/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6248 - acc: 0.7202     \n",
      "Epoch 34/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6241 - acc: 0.7215     \n",
      "Epoch 35/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6243 - acc: 0.7228     \n",
      "Epoch 36/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6232 - acc: 0.7224     \n",
      "Epoch 37/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6229 - acc: 0.7204     \n",
      "Epoch 38/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6227 - acc: 0.7223     \n",
      "Epoch 39/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6205 - acc: 0.7212     \n",
      "Epoch 40/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6232 - acc: 0.7212     \n",
      "Epoch 41/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6221 - acc: 0.7216     \n",
      "Epoch 42/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6219 - acc: 0.7240     \n",
      "Epoch 43/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6197 - acc: 0.7228     \n",
      "Epoch 44/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6222 - acc: 0.7192     \n",
      "Epoch 45/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6215 - acc: 0.7218     \n",
      "Epoch 46/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6206 - acc: 0.7238     \n",
      "Epoch 47/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6202 - acc: 0.7224     \n",
      "Epoch 48/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6194 - acc: 0.7237     \n",
      "Epoch 49/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6191 - acc: 0.7248     \n",
      "Epoch 50/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6202 - acc: 0.7221     \n",
      "Epoch 51/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6182 - acc: 0.7239     \n",
      "Epoch 52/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6188 - acc: 0.7247     \n",
      "Epoch 53/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6179 - acc: 0.7259     \n",
      "Epoch 54/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6184 - acc: 0.7236     \n",
      "Epoch 55/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6207 - acc: 0.7229     \n",
      "Epoch 56/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6200 - acc: 0.7267     \n",
      "Epoch 57/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6179 - acc: 0.7222     \n",
      "Epoch 58/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6196 - acc: 0.7247     \n",
      "Epoch 59/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6197 - acc: 0.7228     \n",
      "Epoch 60/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6178 - acc: 0.7251     \n",
      "Epoch 61/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6172 - acc: 0.7267     \n",
      "Epoch 62/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6189 - acc: 0.7248     \n",
      "Epoch 63/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6159 - acc: 0.7276     \n",
      "Epoch 64/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6163 - acc: 0.7256     \n",
      "Epoch 65/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6170 - acc: 0.7257     \n",
      "Epoch 66/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6177 - acc: 0.7249     \n",
      "Epoch 67/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6180 - acc: 0.7234     \n",
      "Epoch 68/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6176 - acc: 0.7242     \n",
      "Epoch 69/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6171 - acc: 0.7238     \n",
      "Epoch 70/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6166 - acc: 0.7255     \n",
      "Epoch 71/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6170 - acc: 0.7240     \n",
      "Epoch 72/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6158 - acc: 0.7257     \n",
      "Epoch 73/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6168 - acc: 0.7235     \n",
      "Epoch 74/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6168 - acc: 0.7262     \n",
      "Epoch 75/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6153 - acc: 0.7229     \n",
      "Epoch 76/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6176 - acc: 0.7249     \n",
      "Epoch 77/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6169 - acc: 0.7231     \n",
      "Epoch 78/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6156 - acc: 0.7271     \n",
      "Epoch 79/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6160 - acc: 0.7241     \n",
      "Epoch 80/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6163 - acc: 0.7242     \n",
      "Epoch 81/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6141 - acc: 0.7271     \n",
      "Epoch 82/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6163 - acc: 0.7264     \n",
      "Epoch 83/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6156 - acc: 0.7282     \n",
      "Epoch 84/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6158 - acc: 0.7245     \n",
      "Epoch 85/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6157 - acc: 0.7266     \n",
      "Epoch 86/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6150 - acc: 0.7250     \n",
      "Epoch 87/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6165 - acc: 0.7261     \n",
      "Epoch 88/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6163 - acc: 0.7261     \n",
      "Epoch 89/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6154 - acc: 0.7259     \n",
      "Epoch 90/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6148 - acc: 0.7253     \n",
      "Epoch 91/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6151 - acc: 0.7241     \n",
      "Epoch 92/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6168 - acc: 0.7261     \n",
      "Epoch 93/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6145 - acc: 0.7278     \n",
      "Epoch 94/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6162 - acc: 0.7269     \n",
      "Epoch 95/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6149 - acc: 0.7256     \n",
      "Epoch 96/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6146 - acc: 0.7264     \n",
      "Epoch 97/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6140 - acc: 0.7259     \n",
      "Epoch 98/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6129 - acc: 0.7271     \n",
      "Epoch 99/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6149 - acc: 0.7258     \n",
      "Epoch 100/100\n",
      "39469/39469 [==============================] - 6s - loss: 0.6156 - acc: 0.7264     \n",
      "log loss on test k-th fold: 0.615172642854\n",
      "fold number: 4\n",
      "Epoch 1/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.7245 - acc: 0.6865     \n",
      "Epoch 2/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6743 - acc: 0.7017     \n",
      "Epoch 3/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6628 - acc: 0.7061     \n",
      "Epoch 4/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6569 - acc: 0.7076     \n",
      "Epoch 5/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6544 - acc: 0.7104     \n",
      "Epoch 6/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6490 - acc: 0.7104     \n",
      "Epoch 7/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6473 - acc: 0.7139     \n",
      "Epoch 8/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6435 - acc: 0.7140     \n",
      "Epoch 9/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6416 - acc: 0.7126     \n",
      "Epoch 10/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6406 - acc: 0.7156     \n",
      "Epoch 11/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6378 - acc: 0.7153     \n",
      "Epoch 12/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6371 - acc: 0.7147     \n",
      "Epoch 13/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6368 - acc: 0.7180     \n",
      "Epoch 14/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6348 - acc: 0.7174     \n",
      "Epoch 15/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6345 - acc: 0.7180     \n",
      "Epoch 16/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6346 - acc: 0.7203     \n",
      "Epoch 17/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6334 - acc: 0.7182     \n",
      "Epoch 18/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6307 - acc: 0.7206     \n",
      "Epoch 19/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6331 - acc: 0.7192     \n",
      "Epoch 20/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6305 - acc: 0.7188     \n",
      "Epoch 21/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6299 - acc: 0.7206     \n",
      "Epoch 22/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6317 - acc: 0.7195     \n",
      "Epoch 23/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6273 - acc: 0.7202     \n",
      "Epoch 24/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6307 - acc: 0.7185     \n",
      "Epoch 25/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6293 - acc: 0.7208     \n",
      "Epoch 26/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6264 - acc: 0.7197     \n",
      "Epoch 27/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6279 - acc: 0.7189     \n",
      "Epoch 28/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6268 - acc: 0.7214     \n",
      "Epoch 29/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6270 - acc: 0.7202     \n",
      "Epoch 30/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6254 - acc: 0.7185     \n",
      "Epoch 31/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6243 - acc: 0.7219     \n",
      "Epoch 32/100\n",
      "39470/39470 [==============================] - 7s - loss: 0.6261 - acc: 0.7212     \n",
      "Epoch 33/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6240 - acc: 0.7215     \n",
      "Epoch 34/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6234 - acc: 0.7211     \n",
      "Epoch 35/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6235 - acc: 0.7232     \n",
      "Epoch 36/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6254 - acc: 0.7209     \n",
      "Epoch 37/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6240 - acc: 0.7212     \n",
      "Epoch 38/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6240 - acc: 0.7219     \n",
      "Epoch 39/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6218 - acc: 0.7241     \n",
      "Epoch 40/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6232 - acc: 0.7228     \n",
      "Epoch 41/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6228 - acc: 0.7205     \n",
      "Epoch 42/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6237 - acc: 0.7192     \n",
      "Epoch 43/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6238 - acc: 0.7208     \n",
      "Epoch 44/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6229 - acc: 0.7210     \n",
      "Epoch 45/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6206 - acc: 0.7234     \n",
      "Epoch 46/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6216 - acc: 0.7232     \n",
      "Epoch 47/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6199 - acc: 0.7217     \n",
      "Epoch 48/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6207 - acc: 0.7205     \n",
      "Epoch 49/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6208 - acc: 0.7233     \n",
      "Epoch 50/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6200 - acc: 0.7216     \n",
      "Epoch 51/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6187 - acc: 0.7242     \n",
      "Epoch 52/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6200 - acc: 0.7233     \n",
      "Epoch 53/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6200 - acc: 0.7238     \n",
      "Epoch 54/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6180 - acc: 0.7233     \n",
      "Epoch 55/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6184 - acc: 0.7240     \n",
      "Epoch 56/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6194 - acc: 0.7228     \n",
      "Epoch 57/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6195 - acc: 0.7238     \n",
      "Epoch 58/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6193 - acc: 0.7231     \n",
      "Epoch 59/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6190 - acc: 0.7222     \n",
      "Epoch 60/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6196 - acc: 0.7246     \n",
      "Epoch 61/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6212 - acc: 0.7226     \n",
      "Epoch 62/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6168 - acc: 0.7240     \n",
      "Epoch 63/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6204 - acc: 0.7253     \n",
      "Epoch 64/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6184 - acc: 0.7267     \n",
      "Epoch 65/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6174 - acc: 0.7252     \n",
      "Epoch 66/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6188 - acc: 0.7242     \n",
      "Epoch 67/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6158 - acc: 0.7247     \n",
      "Epoch 68/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6164 - acc: 0.7247     \n",
      "Epoch 69/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6175 - acc: 0.7254     \n",
      "Epoch 70/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6176 - acc: 0.7270     \n",
      "Epoch 71/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6163 - acc: 0.7223     \n",
      "Epoch 72/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6169 - acc: 0.7251     \n",
      "Epoch 73/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6174 - acc: 0.7240     \n",
      "Epoch 74/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6165 - acc: 0.7265     \n",
      "Epoch 75/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6164 - acc: 0.7260     \n",
      "Epoch 76/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6170 - acc: 0.7247     \n",
      "Epoch 77/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6188 - acc: 0.7237     \n",
      "Epoch 78/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6172 - acc: 0.7251     \n",
      "Epoch 79/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6164 - acc: 0.7238     \n",
      "Epoch 80/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6163 - acc: 0.7251     \n",
      "Epoch 81/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6159 - acc: 0.7237     \n",
      "Epoch 82/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6165 - acc: 0.7243     \n",
      "Epoch 83/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6152 - acc: 0.7255     \n",
      "Epoch 84/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6161 - acc: 0.7235     \n",
      "Epoch 85/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6149 - acc: 0.7247     \n",
      "Epoch 86/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6163 - acc: 0.7267     \n",
      "Epoch 87/100\n",
      "39470/39470 [==============================] - 7s - loss: 0.6136 - acc: 0.7253     \n",
      "Epoch 88/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6150 - acc: 0.7263     \n",
      "Epoch 89/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6148 - acc: 0.7260     \n",
      "Epoch 90/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6155 - acc: 0.7253     \n",
      "Epoch 91/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6161 - acc: 0.7254     \n",
      "Epoch 92/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6163 - acc: 0.7254     \n",
      "Epoch 93/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6149 - acc: 0.7259     \n",
      "Epoch 94/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6139 - acc: 0.7266     \n",
      "Epoch 95/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6135 - acc: 0.7272     \n",
      "Epoch 96/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6150 - acc: 0.7244     \n",
      "Epoch 97/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6149 - acc: 0.7248     \n",
      "Epoch 98/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6161 - acc: 0.7235     \n",
      "Epoch 99/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6164 - acc: 0.7258     \n",
      "Epoch 100/100\n",
      "39470/39470 [==============================] - 6s - loss: 0.6148 - acc: 0.7255     \n",
      "log loss on test k-th fold: 0.617581181542\n",
      "fold number: 5\n",
      "Epoch 1/100\n",
      "39471/39471 [==============================] - 7s - loss: 0.7224 - acc: 0.6883     \n",
      "Epoch 2/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6736 - acc: 0.7037     \n",
      "Epoch 3/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6615 - acc: 0.7069     \n",
      "Epoch 4/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6578 - acc: 0.7059     \n",
      "Epoch 5/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6519 - acc: 0.7113     \n",
      "Epoch 6/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6485 - acc: 0.7112     \n",
      "Epoch 7/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6461 - acc: 0.7138     \n",
      "Epoch 8/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6452 - acc: 0.7145     \n",
      "Epoch 9/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6432 - acc: 0.7156     \n",
      "Epoch 10/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6405 - acc: 0.7151     \n",
      "Epoch 11/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6395 - acc: 0.7183     \n",
      "Epoch 12/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6385 - acc: 0.7157     \n",
      "Epoch 13/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6384 - acc: 0.7168     \n",
      "Epoch 14/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6348 - acc: 0.7185     \n",
      "Epoch 15/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6335 - acc: 0.7188     \n",
      "Epoch 16/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6338 - acc: 0.7177     \n",
      "Epoch 17/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6334 - acc: 0.7190     \n",
      "Epoch 18/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6321 - acc: 0.7168     \n",
      "Epoch 19/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6310 - acc: 0.7170     \n",
      "Epoch 20/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6287 - acc: 0.7197     \n",
      "Epoch 21/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6292 - acc: 0.7199     \n",
      "Epoch 22/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6300 - acc: 0.7197     \n",
      "Epoch 23/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6284 - acc: 0.7210     \n",
      "Epoch 24/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6267 - acc: 0.7221     \n",
      "Epoch 25/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6269 - acc: 0.7213     \n",
      "Epoch 26/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6280 - acc: 0.7213     \n",
      "Epoch 27/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6274 - acc: 0.7192     \n",
      "Epoch 28/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6265 - acc: 0.7192     \n",
      "Epoch 29/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6252 - acc: 0.7220     \n",
      "Epoch 30/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6258 - acc: 0.7196     \n",
      "Epoch 31/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6260 - acc: 0.7185     \n",
      "Epoch 32/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6243 - acc: 0.7215     \n",
      "Epoch 33/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6257 - acc: 0.7222     \n",
      "Epoch 34/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6242 - acc: 0.7212     \n",
      "Epoch 35/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6229 - acc: 0.7229     \n",
      "Epoch 36/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6234 - acc: 0.7218     \n",
      "Epoch 37/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6238 - acc: 0.7229     \n",
      "Epoch 38/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6222 - acc: 0.7206     \n",
      "Epoch 39/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6229 - acc: 0.7239     \n",
      "Epoch 40/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6249 - acc: 0.7178     \n",
      "Epoch 41/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6228 - acc: 0.7230     \n",
      "Epoch 42/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6218 - acc: 0.7233     \n",
      "Epoch 43/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6223 - acc: 0.7230     \n",
      "Epoch 44/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6218 - acc: 0.7231     \n",
      "Epoch 45/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6198 - acc: 0.7236     \n",
      "Epoch 46/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6198 - acc: 0.7242     \n",
      "Epoch 47/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6172 - acc: 0.7263     \n",
      "Epoch 48/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6226 - acc: 0.7200     \n",
      "Epoch 49/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6228 - acc: 0.7211     \n",
      "Epoch 50/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6198 - acc: 0.7250     \n",
      "Epoch 51/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6210 - acc: 0.7252     \n",
      "Epoch 52/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6208 - acc: 0.7238     \n",
      "Epoch 53/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6204 - acc: 0.7233     \n",
      "Epoch 54/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6196 - acc: 0.7245     \n",
      "Epoch 55/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6189 - acc: 0.7248     \n",
      "Epoch 56/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6192 - acc: 0.7229     \n",
      "Epoch 57/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6181 - acc: 0.7245     \n",
      "Epoch 58/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6202 - acc: 0.7256     \n",
      "Epoch 59/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6224 - acc: 0.7237     \n",
      "Epoch 60/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6212 - acc: 0.7234     \n",
      "Epoch 61/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6189 - acc: 0.7231     \n",
      "Epoch 62/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6180 - acc: 0.7254     \n",
      "Epoch 63/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6192 - acc: 0.7236     \n",
      "Epoch 64/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6172 - acc: 0.7258     \n",
      "Epoch 65/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6178 - acc: 0.7243     \n",
      "Epoch 66/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6188 - acc: 0.7247     \n",
      "Epoch 67/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6191 - acc: 0.7244     \n",
      "Epoch 68/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6212 - acc: 0.7220     \n",
      "Epoch 69/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6168 - acc: 0.7248     \n",
      "Epoch 70/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6172 - acc: 0.7262     \n",
      "Epoch 71/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6182 - acc: 0.7239     \n",
      "Epoch 72/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6171 - acc: 0.7254     \n",
      "Epoch 73/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6179 - acc: 0.7266     \n",
      "Epoch 74/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6163 - acc: 0.7254     \n",
      "Epoch 75/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6165 - acc: 0.7255     \n",
      "Epoch 76/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6168 - acc: 0.7277     \n",
      "Epoch 77/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6173 - acc: 0.7258     \n",
      "Epoch 78/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6169 - acc: 0.7267     \n",
      "Epoch 79/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6174 - acc: 0.7260     \n",
      "Epoch 80/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6157 - acc: 0.7255     \n",
      "Epoch 81/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6186 - acc: 0.7240     \n",
      "Epoch 82/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6151 - acc: 0.7257     \n",
      "Epoch 83/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6165 - acc: 0.7262     \n",
      "Epoch 84/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6168 - acc: 0.7258     \n",
      "Epoch 85/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6168 - acc: 0.7262     \n",
      "Epoch 86/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6172 - acc: 0.7271     \n",
      "Epoch 87/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6152 - acc: 0.7248     \n",
      "Epoch 88/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6152 - acc: 0.7263     \n",
      "Epoch 89/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6185 - acc: 0.7245     \n",
      "Epoch 90/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6158 - acc: 0.7261     \n",
      "Epoch 91/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6134 - acc: 0.7266     \n",
      "Epoch 92/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6175 - acc: 0.7248     \n",
      "Epoch 93/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6163 - acc: 0.7270     \n",
      "Epoch 94/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6147 - acc: 0.7275     \n",
      "Epoch 95/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6161 - acc: 0.7254     \n",
      "Epoch 96/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6155 - acc: 0.7268     \n",
      "Epoch 97/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6137 - acc: 0.7274     \n",
      "Epoch 98/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6151 - acc: 0.7249     \n",
      "Epoch 99/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6149 - acc: 0.7244     \n",
      "Epoch 100/100\n",
      "39471/39471 [==============================] - 6s - loss: 0.6142 - acc: 0.7256     \n",
      "log loss on test k-th fold: 0.613683275161\n",
      "\n",
      "mean testing log loss:\n",
      "0.614832046709 +/- 0.00202112935045\n"
     ]
    }
   ],
   "source": [
    "# define 5-fold cross validation test harness\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\n",
    "cvscores3 = []\n",
    "i=1\n",
    "\n",
    "scale_stand = StandardScaler()\n",
    "\n",
    "small_scaled_bin = scale_stand.fit_transform(X_train_all_small)\n",
    "\n",
    "for train, test in kfold.split(small_scaled_bin, y_train_all_small):\n",
    "    print(\"fold number:\", i)\n",
    "    model3 = Sequential()\n",
    "    \n",
    "    inshape = small_scaled_bin.shape[1]\n",
    "\n",
    "    #Input layer: \n",
    "    model3.add(Dense(50, input_dim= inshape, activation = 'tanh',         \n",
    "                #kernel_regularizer=regularizers.l2(0.001),\n",
    "                #activity_regularizer=regularizers.l1(0.001), \n",
    "                kernel_initializer= initializers.glorot_normal(seed=None)))\n",
    "                     \n",
    "    #model3.add(BatchNormalization())\n",
    "    model3.add(Dropout(0.5))     \n",
    "\n",
    "    #One hidden layer: \n",
    "    model3.add(Dense(50,  activation = 'tanh',\n",
    "                #kernel_regularizer=regularizers.l2(0.001),\n",
    "                #activity_regularizer=regularizers.l1(0.001), \n",
    "                kernel_initializer= initializers.glorot_normal(seed=None)))\n",
    "    #model3.add(BatchNormalization())\n",
    "    model3.add(Dropout(0.25))\n",
    "\n",
    "    #Second hidden layer: \n",
    "    model3.add(Dense(50, activation = 'tanh', \n",
    "                #kernel_regularizer=regularizers.l2(0.01),\n",
    "                #activity_regularizer=regularizers.l1(0.01), \n",
    "                kernel_initializer= initializers.glorot_normal(seed=None)))\n",
    "    #model3.add(BatchNormalization())\n",
    "    model3.add(Dropout(0.25))\n",
    "\n",
    "    #Output layer \n",
    "    model3.add(Dense(3, activation='softmax',\n",
    "                #kernel_regularizer=regularizers.l2(0.01),\n",
    "                #activity_regularizer=regularizers.l1(0.01), \n",
    "                kernel_initializer= initializers.glorot_normal(seed=None)))\n",
    "\n",
    "    #Setting up to optimize the weights: \n",
    "    model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    categorical_labels = to_categorical(y_train_all_small[train]-1, num_classes=None)\n",
    "    model3.fit(small_scaled_bin[train],categorical_labels, epochs=100, batch_size=10, verbose=1)\n",
    "    \n",
    "    # evaluate the model\n",
    "    categorical_labels_test = to_categorical(y_train_all_small[test]-1, num_classes=None)\n",
    "    scores = model3.evaluate(small_scaled_bin[test], categorical_labels_test, verbose=0)\n",
    "    print(\"log loss on test k-th fold:\", scores[0])\n",
    "    cvscores3.append(scores[0])\n",
    "    \n",
    "    i+=1\n",
    "\n",
    "print()\n",
    "print(\"mean testing log loss:\")\n",
    "print(numpy.mean(cvscores3), \"+/-\", numpy.std(cvscores3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation: Scaling the binary predictors doesn't help or hurt the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras Cross Val Take 4 (all variables scaled, using Batch Normalization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold number: 1\n",
      "Epoch 1/100\n",
      "39469/39469 [==============================] - 18s - loss: 0.7626 - acc: 0.6768    \n",
      "Epoch 2/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.7152 - acc: 0.6927    \n",
      "Epoch 3/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.7085 - acc: 0.6941    \n",
      "Epoch 4/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.7015 - acc: 0.6952    \n",
      "Epoch 5/100\n",
      "39469/39469 [==============================] - 16s - loss: 0.6953 - acc: 0.6948    \n",
      "Epoch 6/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6902 - acc: 0.6974    \n",
      "Epoch 7/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6896 - acc: 0.6960    \n",
      "Epoch 8/100\n",
      "39469/39469 [==============================] - 16s - loss: 0.6858 - acc: 0.6995    \n",
      "Epoch 9/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6826 - acc: 0.6997    \n",
      "Epoch 10/100\n",
      "39469/39469 [==============================] - 16s - loss: 0.6814 - acc: 0.6986    \n",
      "Epoch 11/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6831 - acc: 0.6989    \n",
      "Epoch 12/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6790 - acc: 0.6997    \n",
      "Epoch 13/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6804 - acc: 0.6993    \n",
      "Epoch 14/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6785 - acc: 0.7005    \n",
      "Epoch 15/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6775 - acc: 0.7019    \n",
      "Epoch 16/100\n",
      "39469/39469 [==============================] - 16s - loss: 0.6762 - acc: 0.7009    \n",
      "Epoch 17/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6734 - acc: 0.7035    \n",
      "Epoch 18/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6757 - acc: 0.7039    \n",
      "Epoch 19/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6737 - acc: 0.7011    \n",
      "Epoch 20/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6749 - acc: 0.7034    \n",
      "Epoch 21/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6733 - acc: 0.7032    \n",
      "Epoch 22/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6716 - acc: 0.7034    \n",
      "Epoch 23/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6708 - acc: 0.7053    \n",
      "Epoch 24/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6709 - acc: 0.7056    \n",
      "Epoch 25/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6713 - acc: 0.7034    \n",
      "Epoch 26/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6699 - acc: 0.7055    \n",
      "Epoch 27/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6703 - acc: 0.7050    \n",
      "Epoch 28/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6718 - acc: 0.7036    \n",
      "Epoch 29/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6669 - acc: 0.7043    \n",
      "Epoch 30/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6675 - acc: 0.7067    \n",
      "Epoch 31/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6688 - acc: 0.7040    \n",
      "Epoch 32/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6670 - acc: 0.7046    \n",
      "Epoch 33/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6660 - acc: 0.7064    \n",
      "Epoch 34/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6658 - acc: 0.7086    \n",
      "Epoch 35/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6657 - acc: 0.7057    \n",
      "Epoch 36/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6649 - acc: 0.7087    \n",
      "Epoch 37/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6638 - acc: 0.7054    \n",
      "Epoch 38/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6658 - acc: 0.7067    \n",
      "Epoch 39/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6642 - acc: 0.7054    \n",
      "Epoch 40/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6649 - acc: 0.7094    \n",
      "Epoch 41/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6636 - acc: 0.7068    \n",
      "Epoch 42/100\n",
      "39469/39469 [==============================] - 16s - loss: 0.6648 - acc: 0.7091    \n",
      "Epoch 43/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6644 - acc: 0.7069    \n",
      "Epoch 44/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6610 - acc: 0.7099    \n",
      "Epoch 45/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6610 - acc: 0.7071    \n",
      "Epoch 46/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6630 - acc: 0.7086    \n",
      "Epoch 47/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6615 - acc: 0.7105    \n",
      "Epoch 48/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6647 - acc: 0.7066    \n",
      "Epoch 49/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6577 - acc: 0.7097    \n",
      "Epoch 50/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6632 - acc: 0.7095    \n",
      "Epoch 51/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6611 - acc: 0.7109    \n",
      "Epoch 52/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6596 - acc: 0.7081    \n",
      "Epoch 53/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6646 - acc: 0.7086    \n",
      "Epoch 54/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6609 - acc: 0.7082    \n",
      "Epoch 55/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6587 - acc: 0.7126    \n",
      "Epoch 56/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6592 - acc: 0.7091    \n",
      "Epoch 57/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6607 - acc: 0.7080    \n",
      "Epoch 58/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6594 - acc: 0.7086    \n",
      "Epoch 59/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6588 - acc: 0.7092    \n",
      "Epoch 60/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6605 - acc: 0.7103    \n",
      "Epoch 61/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6579 - acc: 0.7114    \n",
      "Epoch 62/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6598 - acc: 0.7081    \n",
      "Epoch 63/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6603 - acc: 0.7113    \n",
      "Epoch 64/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6601 - acc: 0.7097    \n",
      "Epoch 65/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6581 - acc: 0.7109    \n",
      "Epoch 66/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6604 - acc: 0.7100    \n",
      "Epoch 67/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6571 - acc: 0.7106    \n",
      "Epoch 68/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6572 - acc: 0.7115    \n",
      "Epoch 69/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6550 - acc: 0.7103    \n",
      "Epoch 70/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6580 - acc: 0.7122    \n",
      "Epoch 71/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6579 - acc: 0.7104    \n",
      "Epoch 72/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6566 - acc: 0.7128    \n",
      "Epoch 73/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6558 - acc: 0.7098    \n",
      "Epoch 74/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6554 - acc: 0.7128    \n",
      "Epoch 75/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6570 - acc: 0.7109    \n",
      "Epoch 76/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6559 - acc: 0.7110    \n",
      "Epoch 77/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6557 - acc: 0.7119    \n",
      "Epoch 78/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6561 - acc: 0.7103    \n",
      "Epoch 79/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6558 - acc: 0.7113    \n",
      "Epoch 80/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6537 - acc: 0.7118    \n",
      "Epoch 81/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6549 - acc: 0.7100    \n",
      "Epoch 82/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6557 - acc: 0.7114    \n",
      "Epoch 83/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6542 - acc: 0.7100    \n",
      "Epoch 84/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6575 - acc: 0.7111    \n",
      "Epoch 85/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6541 - acc: 0.7132    \n",
      "Epoch 86/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6535 - acc: 0.7135    \n",
      "Epoch 87/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6558 - acc: 0.7128    \n",
      "Epoch 88/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6545 - acc: 0.7125    \n",
      "Epoch 89/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6530 - acc: 0.7114    \n",
      "Epoch 90/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6544 - acc: 0.7128    \n",
      "Epoch 91/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6545 - acc: 0.7109    \n",
      "Epoch 92/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6535 - acc: 0.7141    \n",
      "Epoch 93/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6529 - acc: 0.7122    \n",
      "Epoch 94/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6516 - acc: 0.7144    \n",
      "Epoch 95/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6526 - acc: 0.7149    \n",
      "Epoch 96/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6534 - acc: 0.7138    \n",
      "Epoch 97/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6560 - acc: 0.7115    \n",
      "Epoch 98/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6524 - acc: 0.7138    \n",
      "Epoch 99/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6506 - acc: 0.7140    \n",
      "Epoch 100/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6521 - acc: 0.7139    \n",
      "log loss on test k-th fold: 0.620526380296\n",
      "fold number: 2\n",
      "Epoch 1/100\n",
      "39469/39469 [==============================] - 18s - loss: 0.7691 - acc: 0.6756    \n",
      "Epoch 2/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.7169 - acc: 0.6930    \n",
      "Epoch 3/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.7073 - acc: 0.6949    \n",
      "Epoch 4/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.7011 - acc: 0.6953    \n",
      "Epoch 5/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6934 - acc: 0.6948    \n",
      "Epoch 6/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6904 - acc: 0.6994    \n",
      "Epoch 7/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6876 - acc: 0.6984    \n",
      "Epoch 8/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6833 - acc: 0.6989    \n",
      "Epoch 9/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6807 - acc: 0.6996    \n",
      "Epoch 10/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6812 - acc: 0.6992    \n",
      "Epoch 11/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6806 - acc: 0.6983    \n",
      "Epoch 12/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6782 - acc: 0.6992    \n",
      "Epoch 13/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6762 - acc: 0.7009    \n",
      "Epoch 14/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6726 - acc: 0.7066    \n",
      "Epoch 15/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6771 - acc: 0.7022    \n",
      "Epoch 16/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6764 - acc: 0.7019    \n",
      "Epoch 17/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6748 - acc: 0.7018    \n",
      "Epoch 18/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6760 - acc: 0.7001    \n",
      "Epoch 19/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6748 - acc: 0.7017    \n",
      "Epoch 20/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6733 - acc: 0.7032    \n",
      "Epoch 21/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6709 - acc: 0.7042    \n",
      "Epoch 22/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6716 - acc: 0.7030    \n",
      "Epoch 23/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6709 - acc: 0.7064    \n",
      "Epoch 24/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6683 - acc: 0.7059    \n",
      "Epoch 25/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6679 - acc: 0.7042    \n",
      "Epoch 26/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6655 - acc: 0.7049    \n",
      "Epoch 27/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6682 - acc: 0.7058    \n",
      "Epoch 28/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6696 - acc: 0.7038    \n",
      "Epoch 29/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6670 - acc: 0.7063    \n",
      "Epoch 30/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6685 - acc: 0.7068    \n",
      "Epoch 31/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6667 - acc: 0.7068    \n",
      "Epoch 32/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6675 - acc: 0.7089    \n",
      "Epoch 33/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6648 - acc: 0.7059    \n",
      "Epoch 34/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6674 - acc: 0.7076    \n",
      "Epoch 35/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6661 - acc: 0.7079    \n",
      "Epoch 36/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6666 - acc: 0.7060    \n",
      "Epoch 37/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6648 - acc: 0.7080    \n",
      "Epoch 38/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6652 - acc: 0.7057    \n",
      "Epoch 39/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6653 - acc: 0.7084    \n",
      "Epoch 40/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6653 - acc: 0.7078    \n",
      "Epoch 41/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6637 - acc: 0.7073    \n",
      "Epoch 42/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6620 - acc: 0.7084    \n",
      "Epoch 43/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6628 - acc: 0.7089    \n",
      "Epoch 44/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6635 - acc: 0.7076    \n",
      "Epoch 45/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6601 - acc: 0.7113    \n",
      "Epoch 46/100\n",
      "33640/39469 [========================>.....] - ETA: 2s - loss: 0.6645 - acc: 0.7087"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-128-4dedc4b8cb91>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mcategorical_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train_all_small\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     \u001b[0mmodel4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msmall_scaled_bin\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcategorical_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;31m# evaluate the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    843\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    844\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 845\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    846\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1483\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1484\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1485\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1487\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1138\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1140\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1141\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2071\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2072\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m-> 2073\u001b[1;33m                               feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m   2074\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2075\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define 5-fold cross validation test harness\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\n",
    "cvscores4 = []\n",
    "i=1\n",
    "\n",
    "scale_stand = StandardScaler()\n",
    "\n",
    "small_scaled_bin = scale_stand.fit_transform(X_train_all_small)\n",
    "\n",
    "for train, test in kfold.split(small_scaled_bin, y_train_all_small):\n",
    "    print(\"fold number:\", i)\n",
    "    model4 = Sequential()\n",
    "    \n",
    "    inshape = small_scaled_bin.shape[1]\n",
    "\n",
    "    #Input layer: \n",
    "    model4.add(Dense(50, input_dim= inshape, activation = 'tanh',         \n",
    "                #kernel_regularizer=regularizers.l2(0.001),\n",
    "                #activity_regularizer=regularizers.l1(0.001), \n",
    "                kernel_initializer= initializers.glorot_normal(seed=None)))\n",
    "                     \n",
    "    model4.add(BatchNormalization())\n",
    "    model4.add(Dropout(0.5))     \n",
    "\n",
    "    #One hidden layer: \n",
    "    model4.add(Dense(50,  activation = 'tanh',\n",
    "                #kernel_regularizer=regularizers.l2(0.001),\n",
    "                #activity_regularizer=regularizers.l1(0.001), \n",
    "                kernel_initializer= initializers.glorot_normal(seed=None)))\n",
    "    model4.add(BatchNormalization())\n",
    "    model4.add(Dropout(0.25))\n",
    "\n",
    "    #Second hidden layer: \n",
    "    model4.add(Dense(50, activation = 'tanh', \n",
    "                #kernel_regularizer=regularizers.l2(0.01),\n",
    "                #activity_regularizer=regularizers.l1(0.01), \n",
    "                kernel_initializer= initializers.glorot_normal(seed=None)))\n",
    "    model4.add(BatchNormalization())\n",
    "    model4.add(Dropout(0.25))\n",
    "\n",
    "    #Output layer \n",
    "    model4.add(Dense(3, activation='softmax',\n",
    "                #kernel_regularizer=regularizers.l2(0.01),\n",
    "                #activity_regularizer=regularizers.l1(0.01), \n",
    "                kernel_initializer= initializers.glorot_normal(seed=None)))\n",
    "\n",
    "    #Setting up to optimize the weights: \n",
    "    model4.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    categorical_labels = to_categorical(y_train_all_small[train]-1, num_classes=None)\n",
    "    model4.fit(small_scaled_bin[train],categorical_labels, epochs=100, batch_size=10, verbose=1)\n",
    "    \n",
    "    # evaluate the model\n",
    "    categorical_labels_test = to_categorical(y_train_all_small[test]-1, num_classes=None)\n",
    "    scores = model4.evaluate(small_scaled_bin[test], categorical_labels_test, verbose=0)\n",
    "    print(\"log loss on test k-th fold:\", scores[0])\n",
    "    cvscores4.append(scores[0])\n",
    "    \n",
    "    i+=1\n",
    "\n",
    "print()\n",
    "print(\"mean testing log loss:\")\n",
    "print(numpy.mean(cvscores4), \"+/-\", numpy.std(cvscores4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".......Changed the above (which was hte best run) from adam to sgd, results  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras Cross Val Take 5 (Adding some regularization and early stopping):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold number: 1\n",
      "Epoch 1/100\n",
      "39469/39469 [==============================] - 18s - loss: 0.8751 - acc: 0.6615    \n",
      "Epoch 2/100\n",
      "39469/39469 [==============================] - 16s - loss: 0.7466 - acc: 0.6913    \n",
      "Epoch 3/100\n",
      "39469/39469 [==============================] - 17s - loss: 0.7229 - acc: 0.6952    \n",
      "Epoch 4/100\n",
      "39469/39469 [==============================] - 17s - loss: 0.7159 - acc: 0.6972    \n",
      "Epoch 5/100\n",
      "39469/39469 [==============================] - 17s - loss: 0.7107 - acc: 0.6970    \n",
      "Epoch 6/100\n",
      "39469/39469 [==============================] - 16s - loss: 0.7089 - acc: 0.6986    \n",
      "Epoch 7/100\n",
      "39469/39469 [==============================] - 17s - loss: 0.7074 - acc: 0.6981    \n",
      "Epoch 8/100\n",
      "39469/39469 [==============================] - 16s - loss: 0.7068 - acc: 0.6993    \n",
      "Epoch 9/100\n",
      "39469/39469 [==============================] - 16s - loss: 0.7070 - acc: 0.6995    \n",
      "Epoch 10/100\n",
      "39469/39469 [==============================] - 17s - loss: 0.7042 - acc: 0.6986    \n",
      "Epoch 11/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.7022 - acc: 0.6990    \n",
      "Epoch 12/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.7053 - acc: 0.6991    \n",
      "Epoch 13/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.7017 - acc: 0.6968    \n",
      "Epoch 14/100\n",
      "39469/39469 [==============================] - 13s - loss: 0.7025 - acc: 0.6967    \n",
      "Epoch 15/100\n",
      "39469/39469 [==============================] - 13s - loss: 0.7038 - acc: 0.6979    \n",
      "Epoch 16/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.7020 - acc: 0.6986    \n",
      "Epoch 17/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.7013 - acc: 0.6972    \n",
      "Epoch 18/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6993 - acc: 0.6986    \n",
      "Epoch 19/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6987 - acc: 0.6977    \n",
      "Epoch 20/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6990 - acc: 0.6990    \n",
      "Epoch 21/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6979 - acc: 0.6979    \n",
      "Epoch 22/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6978 - acc: 0.7016    \n",
      "Epoch 23/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6982 - acc: 0.6974    \n",
      "Epoch 24/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6984 - acc: 0.6975    \n",
      "Epoch 25/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.7011 - acc: 0.6955    \n",
      "Epoch 26/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6996 - acc: 0.6969    \n",
      "Epoch 27/100\n",
      "39469/39469 [==============================] - 10s - loss: 0.6979 - acc: 0.7010    \n",
      "Epoch 28/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6959 - acc: 0.6982    \n",
      "Epoch 29/100\n",
      "39469/39469 [==============================] - 10s - loss: 0.6961 - acc: 0.6982    \n",
      "Epoch 30/100\n",
      "39469/39469 [==============================] - 10s - loss: 0.6974 - acc: 0.7000    \n",
      "Epoch 31/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6981 - acc: 0.6989    \n",
      "Epoch 32/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6964 - acc: 0.6986    \n",
      "Epoch 33/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6999 - acc: 0.6969    \n",
      "Epoch 34/100\n",
      "39469/39469 [==============================] - 13s - loss: 0.6979 - acc: 0.6997    \n",
      "Epoch 35/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6962 - acc: 0.6999    \n",
      "Epoch 36/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6977 - acc: 0.6985    \n",
      "Epoch 37/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6948 - acc: 0.6973    \n",
      "Epoch 38/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6976 - acc: 0.6991    \n",
      "Epoch 39/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6957 - acc: 0.7002    \n",
      "Epoch 40/100\n",
      "39469/39469 [==============================] - 13s - loss: 0.6962 - acc: 0.7011    \n",
      "Epoch 41/100\n",
      "39469/39469 [==============================] - 13s - loss: 0.6972 - acc: 0.6993    \n",
      "Epoch 42/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6973 - acc: 0.6984    \n",
      "Epoch 43/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6956 - acc: 0.6990    \n",
      "Epoch 44/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6949 - acc: 0.7003    \n",
      "Epoch 45/100\n",
      "39469/39469 [==============================] - 13s - loss: 0.6954 - acc: 0.6982    \n",
      "Epoch 46/100\n",
      "39469/39469 [==============================] - 13s - loss: 0.6940 - acc: 0.6998    \n",
      "Epoch 47/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6948 - acc: 0.6995    \n",
      "Epoch 48/100\n",
      "39469/39469 [==============================] - 10s - loss: 0.6957 - acc: 0.6984    \n",
      "Epoch 49/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6953 - acc: 0.6990    \n",
      "Epoch 50/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6965 - acc: 0.6990    \n",
      "Epoch 51/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6956 - acc: 0.6980    \n",
      "Epoch 52/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6960 - acc: 0.6992    \n",
      "Epoch 53/100\n",
      "39469/39469 [==============================] - 10s - loss: 0.6959 - acc: 0.7000    \n",
      "Epoch 54/100\n",
      "39469/39469 [==============================] - 10s - loss: 0.6948 - acc: 0.6998    \n",
      "Epoch 55/100\n",
      "39469/39469 [==============================] - 13s - loss: 0.6941 - acc: 0.6997    \n",
      "Epoch 56/100\n",
      "39469/39469 [==============================] - 13s - loss: 0.6956 - acc: 0.6994    \n",
      "Epoch 57/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6926 - acc: 0.6984    \n",
      "Epoch 58/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6938 - acc: 0.7001    \n",
      "Epoch 59/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6941 - acc: 0.6992    \n",
      "Epoch 60/100\n",
      "39469/39469 [==============================] - 16s - loss: 0.6929 - acc: 0.7000    \n",
      "Epoch 61/100\n",
      "39469/39469 [==============================] - 16s - loss: 0.6922 - acc: 0.7007    \n",
      "Epoch 62/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6947 - acc: 0.6988    \n",
      "Epoch 63/100\n",
      "39469/39469 [==============================] - 16s - loss: 0.6923 - acc: 0.7020    \n",
      "Epoch 64/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6924 - acc: 0.6999    \n",
      "Epoch 65/100\n",
      "39469/39469 [==============================] - 16s - loss: 0.6959 - acc: 0.7017    \n",
      "Epoch 66/100\n",
      "39469/39469 [==============================] - 16s - loss: 0.6939 - acc: 0.7000    \n",
      "Epoch 67/100\n",
      "39469/39469 [==============================] - 16s - loss: 0.6952 - acc: 0.6990    \n",
      "Epoch 68/100\n",
      "39469/39469 [==============================] - 16s - loss: 0.6941 - acc: 0.6996    \n",
      "Epoch 69/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6927 - acc: 0.7009    \n",
      "Epoch 70/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6925 - acc: 0.7003    \n",
      "Epoch 71/100\n",
      "39469/39469 [==============================] - 16s - loss: 0.6943 - acc: 0.7000    \n",
      "Epoch 72/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6946 - acc: 0.7031    \n",
      "Epoch 73/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6963 - acc: 0.6987    \n",
      "Epoch 74/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6916 - acc: 0.7014    \n",
      "Epoch 75/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.6954 - acc: 0.7003    \n",
      "Epoch 76/100\n",
      "39469/39469 [==============================] - 16s - loss: 0.6926 - acc: 0.7025    \n",
      "Epoch 77/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6934 - acc: 0.7001    \n",
      "Epoch 78/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6948 - acc: 0.7017    \n",
      "Epoch 79/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6933 - acc: 0.7011    \n",
      "Epoch 80/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6928 - acc: 0.7008    \n",
      "Epoch 81/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6946 - acc: 0.7012    \n",
      "Epoch 82/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6906 - acc: 0.6996    \n",
      "Epoch 83/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6925 - acc: 0.7010    \n",
      "Epoch 84/100\n",
      "39469/39469 [==============================] - 17s - loss: 0.6927 - acc: 0.7016    \n",
      "Epoch 85/100\n",
      "39469/39469 [==============================] - 16s - loss: 0.6926 - acc: 0.7012    \n",
      "Epoch 86/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6933 - acc: 0.7009    \n",
      "Epoch 87/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6949 - acc: 0.7018    \n",
      "Epoch 88/100\n",
      "39469/39469 [==============================] - 13s - loss: 0.6907 - acc: 0.7025    \n",
      "Epoch 89/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6925 - acc: 0.7021    \n",
      "Epoch 90/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6940 - acc: 0.7001    \n",
      "Epoch 91/100\n",
      "39469/39469 [==============================] - 13s - loss: 0.6935 - acc: 0.7017    \n",
      "Epoch 92/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6953 - acc: 0.7005    \n",
      "Epoch 93/100\n",
      "39469/39469 [==============================] - 13s - loss: 0.6927 - acc: 0.7015    \n",
      "Epoch 94/100\n",
      "39469/39469 [==============================] - 13s - loss: 0.6941 - acc: 0.7035    \n",
      "Epoch 95/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6904 - acc: 0.7025    \n",
      "Epoch 96/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6911 - acc: 0.7010    \n",
      "Epoch 97/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6913 - acc: 0.7015    \n",
      "Epoch 98/100\n",
      "39469/39469 [==============================] - 13s - loss: 0.6917 - acc: 0.7029    \n",
      "Epoch 99/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6920 - acc: 0.7004    \n",
      "Epoch 100/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6926 - acc: 0.7024    \n",
      "log loss on test k-th fold: 0.6577045198\n",
      "fold number: 2\n",
      "Epoch 1/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.8558 - acc: 0.6664    \n",
      "Epoch 2/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.7422 - acc: 0.6955    \n",
      "Epoch 3/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.7198 - acc: 0.6953    \n",
      "Epoch 4/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.7131 - acc: 0.6967    \n",
      "Epoch 5/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.7099 - acc: 0.6964    \n",
      "Epoch 6/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.7089 - acc: 0.6982    \n",
      "Epoch 7/100\n",
      "39469/39469 [==============================] - 13s - loss: 0.7074 - acc: 0.6976    \n",
      "Epoch 8/100\n",
      "39469/39469 [==============================] - 13s - loss: 0.7048 - acc: 0.6974    \n",
      "Epoch 9/100\n",
      "39469/39469 [==============================] - 13s - loss: 0.7059 - acc: 0.6974    \n",
      "Epoch 10/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.7022 - acc: 0.6992    \n",
      "Epoch 11/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.7023 - acc: 0.6966    \n",
      "Epoch 12/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.7031 - acc: 0.6974    \n",
      "Epoch 13/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.7021 - acc: 0.6983    \n",
      "Epoch 14/100\n",
      "39469/39469 [==============================] - 13s - loss: 0.7008 - acc: 0.6975    \n",
      "Epoch 15/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.7009 - acc: 0.6984    \n",
      "Epoch 16/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6995 - acc: 0.6993    \n",
      "Epoch 17/100\n",
      "39469/39469 [==============================] - 13s - loss: 0.7009 - acc: 0.6997    \n",
      "Epoch 18/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6997 - acc: 0.6980    \n",
      "Epoch 19/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6992 - acc: 0.6976    \n",
      "Epoch 20/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6995 - acc: 0.6993    \n",
      "Epoch 21/100\n",
      "39469/39469 [==============================] - 13s - loss: 0.6996 - acc: 0.6998    \n",
      "Epoch 22/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6971 - acc: 0.6976    \n",
      "Epoch 23/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6987 - acc: 0.6977    \n",
      "Epoch 24/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6979 - acc: 0.6964    \n",
      "Epoch 25/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6962 - acc: 0.6974    \n",
      "Epoch 26/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6955 - acc: 0.7003    \n",
      "Epoch 27/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6967 - acc: 0.6996    \n",
      "Epoch 28/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6956 - acc: 0.7008    \n",
      "Epoch 29/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6963 - acc: 0.7002    \n",
      "Epoch 30/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6971 - acc: 0.6992    \n",
      "Epoch 31/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6955 - acc: 0.6994    \n",
      "Epoch 32/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6950 - acc: 0.6980    \n",
      "Epoch 33/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6945 - acc: 0.6992    \n",
      "Epoch 34/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6974 - acc: 0.6956    \n",
      "Epoch 35/100\n",
      "39469/39469 [==============================] - 13s - loss: 0.6964 - acc: 0.6993    \n",
      "Epoch 36/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6968 - acc: 0.6983    \n",
      "Epoch 37/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6928 - acc: 0.7000    \n",
      "Epoch 38/100\n",
      "39469/39469 [==============================] - 13s - loss: 0.6974 - acc: 0.6985    \n",
      "Epoch 39/100\n",
      "39469/39469 [==============================] - 13s - loss: 0.6952 - acc: 0.7007    \n",
      "Epoch 40/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6953 - acc: 0.6983    \n",
      "Epoch 41/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6947 - acc: 0.7026    \n",
      "Epoch 42/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6956 - acc: 0.6978    \n",
      "Epoch 43/100\n",
      "39469/39469 [==============================] - 13s - loss: 0.6942 - acc: 0.7004    \n",
      "Epoch 44/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6951 - acc: 0.6986    \n",
      "Epoch 45/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6944 - acc: 0.6990    \n",
      "Epoch 46/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6948 - acc: 0.6993    \n",
      "Epoch 47/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6934 - acc: 0.7001    \n",
      "Epoch 48/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6941 - acc: 0.7011    \n",
      "Epoch 49/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6923 - acc: 0.7001    \n",
      "Epoch 50/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6962 - acc: 0.6992    \n",
      "Epoch 51/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6916 - acc: 0.7003    \n",
      "Epoch 52/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6953 - acc: 0.6999    \n",
      "Epoch 53/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6933 - acc: 0.7013    \n",
      "Epoch 54/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6935 - acc: 0.7004    \n",
      "Epoch 55/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6954 - acc: 0.6996    \n",
      "Epoch 56/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6920 - acc: 0.7023    \n",
      "Epoch 57/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6951 - acc: 0.6989    \n",
      "Epoch 58/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6929 - acc: 0.7011    \n",
      "Epoch 59/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6923 - acc: 0.7003    \n",
      "Epoch 60/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6924 - acc: 0.7014    \n",
      "Epoch 61/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6934 - acc: 0.7009    \n",
      "Epoch 62/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6917 - acc: 0.7005    \n",
      "Epoch 63/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6908 - acc: 0.7027    \n",
      "Epoch 64/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6926 - acc: 0.7020    \n",
      "Epoch 65/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6904 - acc: 0.7012    \n",
      "Epoch 66/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6911 - acc: 0.7015    \n",
      "Epoch 67/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6935 - acc: 0.6985    \n",
      "Epoch 68/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6931 - acc: 0.7005    \n",
      "Epoch 69/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6929 - acc: 0.7000    \n",
      "Epoch 70/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6921 - acc: 0.7001    \n",
      "Epoch 71/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6915 - acc: 0.7016    \n",
      "Epoch 72/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6913 - acc: 0.7005    \n",
      "Epoch 73/100\n",
      "39469/39469 [==============================] - 13s - loss: 0.6916 - acc: 0.7006    \n",
      "Epoch 74/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6916 - acc: 0.6995    \n",
      "Epoch 75/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6941 - acc: 0.6997    \n",
      "Epoch 76/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6910 - acc: 0.7028    \n",
      "Epoch 77/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6934 - acc: 0.7007    \n",
      "Epoch 78/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6935 - acc: 0.7004    \n",
      "Epoch 79/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6921 - acc: 0.7010    \n",
      "Epoch 80/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6928 - acc: 0.7007    \n",
      "Epoch 81/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6890 - acc: 0.7002    \n",
      "Epoch 82/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6924 - acc: 0.7004    \n",
      "Epoch 83/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6930 - acc: 0.7008    \n",
      "Epoch 84/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6900 - acc: 0.7024    \n",
      "Epoch 85/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6914 - acc: 0.7009    \n",
      "Epoch 86/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6897 - acc: 0.7015    \n",
      "Epoch 87/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6907 - acc: 0.7007    \n",
      "Epoch 88/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6899 - acc: 0.6994    \n",
      "Epoch 89/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6918 - acc: 0.6997    \n",
      "Epoch 90/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6893 - acc: 0.7019    \n",
      "Epoch 91/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6910 - acc: 0.7010    \n",
      "Epoch 92/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6886 - acc: 0.7038    \n",
      "Epoch 93/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6905 - acc: 0.7005    \n",
      "Epoch 94/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6915 - acc: 0.7009    \n",
      "Epoch 95/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6897 - acc: 0.7013    \n",
      "Epoch 96/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6902 - acc: 0.7014    \n",
      "Epoch 97/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6910 - acc: 0.7002    \n",
      "Epoch 98/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6914 - acc: 0.6997    \n",
      "Epoch 99/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6919 - acc: 0.7000    \n",
      "Epoch 100/100\n",
      "39469/39469 [==============================] - 14s - loss: 0.6900 - acc: 0.7043    \n",
      "log loss on test k-th fold: 0.658527246167\n",
      "fold number: 3\n",
      "Epoch 1/100\n",
      "39469/39469 [==============================] - 15s - loss: 0.8671 - acc: 0.6631    \n",
      "Epoch 2/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.7438 - acc: 0.6939    \n",
      "Epoch 3/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.7212 - acc: 0.6957    \n",
      "Epoch 4/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.7129 - acc: 0.6976    \n",
      "Epoch 5/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.7127 - acc: 0.6966    \n",
      "Epoch 6/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.7100 - acc: 0.6993    \n",
      "Epoch 7/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.7072 - acc: 0.6971    \n",
      "Epoch 8/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.7029 - acc: 0.6998    \n",
      "Epoch 9/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.7027 - acc: 0.6993    \n",
      "Epoch 10/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.7041 - acc: 0.6993    \n",
      "Epoch 11/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.7047 - acc: 0.6977    \n",
      "Epoch 12/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.7025 - acc: 0.7008    \n",
      "Epoch 13/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.7013 - acc: 0.6983    \n",
      "Epoch 14/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.7005 - acc: 0.7004    \n",
      "Epoch 15/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6990 - acc: 0.6990    \n",
      "Epoch 16/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.7007 - acc: 0.6987    \n",
      "Epoch 17/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6996 - acc: 0.6982    \n",
      "Epoch 18/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.7019 - acc: 0.6984    \n",
      "Epoch 19/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6985 - acc: 0.6998    \n",
      "Epoch 20/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6979 - acc: 0.6980    \n",
      "Epoch 21/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.7025 - acc: 0.6992    \n",
      "Epoch 22/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6966 - acc: 0.7006    \n",
      "Epoch 23/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6973 - acc: 0.7000    \n",
      "Epoch 24/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6981 - acc: 0.7011    \n",
      "Epoch 25/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6971 - acc: 0.7013    \n",
      "Epoch 26/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6988 - acc: 0.6992    \n",
      "Epoch 27/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6997 - acc: 0.7007    \n",
      "Epoch 28/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6983 - acc: 0.7000    \n",
      "Epoch 29/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6966 - acc: 0.7002    \n",
      "Epoch 30/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6960 - acc: 0.6991    \n",
      "Epoch 31/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6961 - acc: 0.6996    \n",
      "Epoch 32/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.7003 - acc: 0.7013    \n",
      "Epoch 33/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6963 - acc: 0.6998    \n",
      "Epoch 34/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6975 - acc: 0.6998    \n",
      "Epoch 35/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6971 - acc: 0.6996    \n",
      "Epoch 36/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6983 - acc: 0.7003    \n",
      "Epoch 37/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6956 - acc: 0.6993    \n",
      "Epoch 38/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6979 - acc: 0.6998    \n",
      "Epoch 39/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6965 - acc: 0.6992    \n",
      "Epoch 40/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6964 - acc: 0.7005    \n",
      "Epoch 41/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6947 - acc: 0.7011    \n",
      "Epoch 42/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6941 - acc: 0.7015    \n",
      "Epoch 43/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6973 - acc: 0.7011    \n",
      "Epoch 44/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6950 - acc: 0.7021    \n",
      "Epoch 45/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6959 - acc: 0.6984    \n",
      "Epoch 46/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6956 - acc: 0.7007    \n",
      "Epoch 47/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6930 - acc: 0.7002    \n",
      "Epoch 48/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6932 - acc: 0.6992    \n",
      "Epoch 49/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6931 - acc: 0.7005    \n",
      "Epoch 50/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6975 - acc: 0.7004    \n",
      "Epoch 51/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6936 - acc: 0.7002    \n",
      "Epoch 52/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6930 - acc: 0.7010    \n",
      "Epoch 53/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6978 - acc: 0.7002    \n",
      "Epoch 54/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6934 - acc: 0.6992    \n",
      "Epoch 55/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6934 - acc: 0.7025    \n",
      "Epoch 56/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6949 - acc: 0.6989    \n",
      "Epoch 57/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6919 - acc: 0.7029    \n",
      "Epoch 58/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6935 - acc: 0.7012    \n",
      "Epoch 59/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6927 - acc: 0.6995    \n",
      "Epoch 60/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6930 - acc: 0.7028    \n",
      "Epoch 61/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6943 - acc: 0.7002    \n",
      "Epoch 62/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6920 - acc: 0.7016    \n",
      "Epoch 63/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6948 - acc: 0.6999    \n",
      "Epoch 64/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6953 - acc: 0.6997    \n",
      "Epoch 65/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6918 - acc: 0.7020    \n",
      "Epoch 66/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6951 - acc: 0.6990    \n",
      "Epoch 67/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6913 - acc: 0.7048    \n",
      "Epoch 68/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6931 - acc: 0.7011    \n",
      "Epoch 69/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6953 - acc: 0.7007    \n",
      "Epoch 70/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6923 - acc: 0.7019    \n",
      "Epoch 71/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6906 - acc: 0.7024    \n",
      "Epoch 72/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6906 - acc: 0.7022    \n",
      "Epoch 73/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6926 - acc: 0.7003    \n",
      "Epoch 74/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6933 - acc: 0.7022    \n",
      "Epoch 75/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6934 - acc: 0.7002    \n",
      "Epoch 76/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6927 - acc: 0.7017    \n",
      "Epoch 77/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6918 - acc: 0.7006    \n",
      "Epoch 78/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6937 - acc: 0.7019    \n",
      "Epoch 79/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6901 - acc: 0.7018    \n",
      "Epoch 80/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6927 - acc: 0.7023    \n",
      "Epoch 81/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6917 - acc: 0.7005    \n",
      "Epoch 82/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6930 - acc: 0.7027    \n",
      "Epoch 83/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6912 - acc: 0.6998    \n",
      "Epoch 84/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6911 - acc: 0.7027    \n",
      "Epoch 85/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6885 - acc: 0.7017    \n",
      "Epoch 86/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6915 - acc: 0.7020    \n",
      "Epoch 87/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6925 - acc: 0.7024    \n",
      "Epoch 88/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6910 - acc: 0.7029    \n",
      "Epoch 89/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6907 - acc: 0.7012    \n",
      "Epoch 90/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6922 - acc: 0.7030    \n",
      "Epoch 91/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6892 - acc: 0.7018    \n",
      "Epoch 92/100\n",
      "39469/39469 [==============================] - 12s - loss: 0.6910 - acc: 0.7011    \n",
      "Epoch 93/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6903 - acc: 0.7049    \n",
      "Epoch 94/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6908 - acc: 0.7026    \n",
      "Epoch 95/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6900 - acc: 0.7029    \n",
      "Epoch 96/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6907 - acc: 0.7030    \n",
      "Epoch 97/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6919 - acc: 0.7046    \n",
      "Epoch 98/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6907 - acc: 0.7033    \n",
      "Epoch 99/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6902 - acc: 0.7031    \n",
      "Epoch 100/100\n",
      "39469/39469 [==============================] - 11s - loss: 0.6905 - acc: 0.7044    \n",
      "log loss on test k-th fold: 0.657908238981\n",
      "fold number: 4\n",
      "Epoch 1/100\n",
      "39470/39470 [==============================] - 15s - loss: 0.8511 - acc: 0.6690    \n",
      "Epoch 2/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.7444 - acc: 0.6921    \n",
      "Epoch 3/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.7241 - acc: 0.6948    \n",
      "Epoch 4/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.7152 - acc: 0.6964    \n",
      "Epoch 5/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.7111 - acc: 0.6976    \n",
      "Epoch 6/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.7090 - acc: 0.6982    \n",
      "Epoch 7/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.7073 - acc: 0.6954    \n",
      "Epoch 8/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.7063 - acc: 0.6993    \n",
      "Epoch 9/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.7051 - acc: 0.7006    \n",
      "Epoch 10/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.7044 - acc: 0.6972    \n",
      "Epoch 11/100\n",
      "39470/39470 [==============================] - 16s - loss: 0.7036 - acc: 0.6968    \n",
      "Epoch 12/100\n",
      "39470/39470 [==============================] - 13s - loss: 0.7027 - acc: 0.6991    \n",
      "Epoch 13/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.7018 - acc: 0.6978    \n",
      "Epoch 14/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.7014 - acc: 0.6987    \n",
      "Epoch 15/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.7014 - acc: 0.6967    \n",
      "Epoch 16/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.7021 - acc: 0.6978    \n",
      "Epoch 17/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.7002 - acc: 0.6987    \n",
      "Epoch 18/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.7023 - acc: 0.6974    \n",
      "Epoch 19/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.7010 - acc: 0.6998    \n",
      "Epoch 20/100\n",
      "39470/39470 [==============================] - 13s - loss: 0.6982 - acc: 0.7001    \n",
      "Epoch 21/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6997 - acc: 0.6989    \n",
      "Epoch 22/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6998 - acc: 0.6987    \n",
      "Epoch 23/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6990 - acc: 0.6978    \n",
      "Epoch 24/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6989 - acc: 0.7001    \n",
      "Epoch 25/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6972 - acc: 0.6981    \n",
      "Epoch 26/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6997 - acc: 0.6985    \n",
      "Epoch 27/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6971 - acc: 0.6998    \n",
      "Epoch 28/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6987 - acc: 0.7001    \n",
      "Epoch 29/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6977 - acc: 0.6976    \n",
      "Epoch 30/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6983 - acc: 0.6980    \n",
      "Epoch 31/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6984 - acc: 0.6980    \n",
      "Epoch 32/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6974 - acc: 0.6995    \n",
      "Epoch 33/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6988 - acc: 0.6969    \n",
      "Epoch 34/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6980 - acc: 0.6976    \n",
      "Epoch 35/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6977 - acc: 0.7006    \n",
      "Epoch 36/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6988 - acc: 0.6996    \n",
      "Epoch 37/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6981 - acc: 0.6984    \n",
      "Epoch 38/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6989 - acc: 0.6988    \n",
      "Epoch 39/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6996 - acc: 0.6986    \n",
      "Epoch 40/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6964 - acc: 0.6964    \n",
      "Epoch 41/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6968 - acc: 0.6991    \n",
      "Epoch 42/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6969 - acc: 0.6985    \n",
      "Epoch 43/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6971 - acc: 0.7008    \n",
      "Epoch 44/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6983 - acc: 0.6993    \n",
      "Epoch 45/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6979 - acc: 0.6974    \n",
      "Epoch 46/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6950 - acc: 0.7001    \n",
      "Epoch 47/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6963 - acc: 0.6978    \n",
      "Epoch 48/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6940 - acc: 0.6984    \n",
      "Epoch 49/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6971 - acc: 0.6995    \n",
      "Epoch 50/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6953 - acc: 0.7005    \n",
      "Epoch 51/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6977 - acc: 0.6987    \n",
      "Epoch 52/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6949 - acc: 0.7008    \n",
      "Epoch 53/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6954 - acc: 0.6990    \n",
      "Epoch 54/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6952 - acc: 0.6983    \n",
      "Epoch 55/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6967 - acc: 0.6987    \n",
      "Epoch 56/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6941 - acc: 0.6996    \n",
      "Epoch 57/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6931 - acc: 0.7015    \n",
      "Epoch 58/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6958 - acc: 0.6984    \n",
      "Epoch 59/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6957 - acc: 0.6994    \n",
      "Epoch 60/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6949 - acc: 0.6991    \n",
      "Epoch 61/100\n",
      "39470/39470 [==============================] - 13s - loss: 0.6934 - acc: 0.7015    \n",
      "Epoch 62/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6947 - acc: 0.7015    \n",
      "Epoch 63/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6954 - acc: 0.6995    \n",
      "Epoch 64/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6923 - acc: 0.7035    \n",
      "Epoch 65/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6952 - acc: 0.7015    \n",
      "Epoch 66/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6944 - acc: 0.6985    \n",
      "Epoch 67/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6933 - acc: 0.7001    \n",
      "Epoch 68/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6937 - acc: 0.7000    \n",
      "Epoch 69/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6937 - acc: 0.7016    \n",
      "Epoch 70/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6928 - acc: 0.7032    \n",
      "Epoch 71/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6913 - acc: 0.7011    \n",
      "Epoch 72/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6932 - acc: 0.7005    \n",
      "Epoch 73/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6923 - acc: 0.7026    \n",
      "Epoch 74/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6959 - acc: 0.6990    \n",
      "Epoch 75/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6930 - acc: 0.6999    \n",
      "Epoch 76/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6935 - acc: 0.7006    \n",
      "Epoch 77/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6925 - acc: 0.7016    \n",
      "Epoch 78/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6927 - acc: 0.7003    \n",
      "Epoch 79/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6941 - acc: 0.6993    \n",
      "Epoch 80/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6902 - acc: 0.7014    \n",
      "Epoch 81/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6920 - acc: 0.7006    \n",
      "Epoch 82/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6922 - acc: 0.7010    \n",
      "Epoch 83/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6920 - acc: 0.7015    \n",
      "Epoch 84/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6932 - acc: 0.7013    \n",
      "Epoch 85/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6919 - acc: 0.7017    \n",
      "Epoch 86/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6931 - acc: 0.6997    \n",
      "Epoch 87/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6927 - acc: 0.7011    \n",
      "Epoch 88/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6933 - acc: 0.7005    \n",
      "Epoch 89/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6915 - acc: 0.7002    \n",
      "Epoch 90/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6925 - acc: 0.7003    \n",
      "Epoch 91/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6964 - acc: 0.7010    \n",
      "Epoch 92/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6934 - acc: 0.7007    \n",
      "Epoch 93/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6946 - acc: 0.7003    \n",
      "Epoch 94/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6919 - acc: 0.7005    \n",
      "Epoch 95/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6926 - acc: 0.7009    \n",
      "Epoch 96/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6935 - acc: 0.7003    \n",
      "Epoch 97/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6926 - acc: 0.7005    \n",
      "Epoch 98/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6912 - acc: 0.7001    \n",
      "Epoch 99/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6935 - acc: 0.7008    \n",
      "Epoch 100/100\n",
      "39470/39470 [==============================] - 12s - loss: 0.6936 - acc: 0.7010    \n",
      "log loss on test k-th fold: 0.658421972832\n",
      "fold number: 5\n",
      "Epoch 1/100\n",
      "39471/39471 [==============================] - 15s - loss: 0.8545 - acc: 0.6665    \n",
      "Epoch 2/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.7427 - acc: 0.6948    \n",
      "Epoch 3/100\n",
      "39471/39471 [==============================] - 13s - loss: 0.7211 - acc: 0.6953    \n",
      "Epoch 4/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.7133 - acc: 0.6970    \n",
      "Epoch 5/100\n",
      "39471/39471 [==============================] - 13s - loss: 0.7122 - acc: 0.6950    \n",
      "Epoch 6/100\n",
      "39471/39471 [==============================] - 13s - loss: 0.7095 - acc: 0.6965    \n",
      "Epoch 7/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.7092 - acc: 0.6995    \n",
      "Epoch 8/100\n",
      "39471/39471 [==============================] - 13s - loss: 0.7069 - acc: 0.6976    \n",
      "Epoch 9/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.7067 - acc: 0.6981    \n",
      "Epoch 10/100\n",
      "39471/39471 [==============================] - 13s - loss: 0.7039 - acc: 0.6974    \n",
      "Epoch 11/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.7049 - acc: 0.6997    \n",
      "Epoch 12/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.7009 - acc: 0.7005    \n",
      "Epoch 13/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.7032 - acc: 0.6981    \n",
      "Epoch 14/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.7022 - acc: 0.6989    \n",
      "Epoch 15/100\n",
      "39471/39471 [==============================] - 13s - loss: 0.7012 - acc: 0.6993    \n",
      "Epoch 16/100\n",
      "39471/39471 [==============================] - 13s - loss: 0.7015 - acc: 0.6995    \n",
      "Epoch 17/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.7026 - acc: 0.6978    \n",
      "Epoch 18/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.7003 - acc: 0.6989    \n",
      "Epoch 19/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.7002 - acc: 0.6990    \n",
      "Epoch 20/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6991 - acc: 0.7011    \n",
      "Epoch 21/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6990 - acc: 0.6998    \n",
      "Epoch 22/100\n",
      "39471/39471 [==============================] - 13s - loss: 0.6999 - acc: 0.6961    \n",
      "Epoch 23/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6981 - acc: 0.6988    \n",
      "Epoch 24/100\n",
      "39471/39471 [==============================] - 13s - loss: 0.6989 - acc: 0.6987    \n",
      "Epoch 25/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6987 - acc: 0.6995    \n",
      "Epoch 26/100\n",
      "39471/39471 [==============================] - 13s - loss: 0.6994 - acc: 0.6983    \n",
      "Epoch 27/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6957 - acc: 0.7010    \n",
      "Epoch 28/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6986 - acc: 0.7001    \n",
      "Epoch 29/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6965 - acc: 0.7016    \n",
      "Epoch 30/100\n",
      "39471/39471 [==============================] - 13s - loss: 0.7003 - acc: 0.6982    \n",
      "Epoch 31/100\n",
      "39471/39471 [==============================] - 13s - loss: 0.6998 - acc: 0.6995    \n",
      "Epoch 32/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6984 - acc: 0.6994    \n",
      "Epoch 33/100\n",
      "39471/39471 [==============================] - 13s - loss: 0.6981 - acc: 0.6992    \n",
      "Epoch 34/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6985 - acc: 0.7006    \n",
      "Epoch 35/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6987 - acc: 0.6985    \n",
      "Epoch 36/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6952 - acc: 0.6995    \n",
      "Epoch 37/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6983 - acc: 0.7008    \n",
      "Epoch 38/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6958 - acc: 0.7009    \n",
      "Epoch 39/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6973 - acc: 0.6999    \n",
      "Epoch 40/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6972 - acc: 0.7010    \n",
      "Epoch 41/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6980 - acc: 0.6975    \n",
      "Epoch 42/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6968 - acc: 0.6997    \n",
      "Epoch 43/100\n",
      "39471/39471 [==============================] - 13s - loss: 0.6974 - acc: 0.6988    \n",
      "Epoch 44/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6979 - acc: 0.6987    \n",
      "Epoch 45/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6965 - acc: 0.6993    \n",
      "Epoch 46/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6949 - acc: 0.6995    \n",
      "Epoch 47/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6957 - acc: 0.6990    \n",
      "Epoch 48/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6985 - acc: 0.6983    \n",
      "Epoch 49/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6990 - acc: 0.6998    \n",
      "Epoch 50/100\n",
      "39471/39471 [==============================] - 13s - loss: 0.6960 - acc: 0.6999    \n",
      "Epoch 51/100\n",
      "39471/39471 [==============================] - 13s - loss: 0.6967 - acc: 0.7021    \n",
      "Epoch 52/100\n",
      "39471/39471 [==============================] - 13s - loss: 0.6959 - acc: 0.7020    \n",
      "Epoch 53/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6970 - acc: 0.6993    \n",
      "Epoch 54/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6956 - acc: 0.7000    \n",
      "Epoch 55/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6955 - acc: 0.7007    \n",
      "Epoch 56/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6961 - acc: 0.7003    \n",
      "Epoch 57/100\n",
      "39471/39471 [==============================] - 13s - loss: 0.6976 - acc: 0.7014    \n",
      "Epoch 58/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6954 - acc: 0.7017    \n",
      "Epoch 59/100\n",
      "39471/39471 [==============================] - 13s - loss: 0.6941 - acc: 0.7019    \n",
      "Epoch 60/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6939 - acc: 0.7011    \n",
      "Epoch 61/100\n",
      "39471/39471 [==============================] - 13s - loss: 0.6959 - acc: 0.6999    \n",
      "Epoch 62/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6956 - acc: 0.7013    \n",
      "Epoch 63/100\n",
      "39471/39471 [==============================] - 13s - loss: 0.6939 - acc: 0.7028    \n",
      "Epoch 64/100\n",
      "39471/39471 [==============================] - 13s - loss: 0.6944 - acc: 0.7032    \n",
      "Epoch 65/100\n",
      "39471/39471 [==============================] - 13s - loss: 0.6915 - acc: 0.7012    \n",
      "Epoch 66/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6941 - acc: 0.7018    \n",
      "Epoch 67/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6963 - acc: 0.7000    \n",
      "Epoch 68/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6932 - acc: 0.7034    \n",
      "Epoch 69/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6931 - acc: 0.7016    \n",
      "Epoch 70/100\n",
      "39471/39471 [==============================] - 13s - loss: 0.6925 - acc: 0.7033    \n",
      "Epoch 71/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6921 - acc: 0.7024    \n",
      "Epoch 72/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6937 - acc: 0.7010    \n",
      "Epoch 73/100\n",
      "39471/39471 [==============================] - 13s - loss: 0.6953 - acc: 0.6995    \n",
      "Epoch 74/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6950 - acc: 0.7019    \n",
      "Epoch 75/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6934 - acc: 0.7028    \n",
      "Epoch 76/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6935 - acc: 0.7030    \n",
      "Epoch 77/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6910 - acc: 0.7018    \n",
      "Epoch 78/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6963 - acc: 0.6992    \n",
      "Epoch 79/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6913 - acc: 0.7018    \n",
      "Epoch 80/100\n",
      "39471/39471 [==============================] - 13s - loss: 0.6950 - acc: 0.7011    \n",
      "Epoch 81/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6931 - acc: 0.7019    \n",
      "Epoch 82/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6929 - acc: 0.7009    \n",
      "Epoch 83/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6933 - acc: 0.7023    \n",
      "Epoch 84/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6928 - acc: 0.7004    \n",
      "Epoch 85/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6919 - acc: 0.7024    \n",
      "Epoch 86/100\n",
      "39471/39471 [==============================] - 13s - loss: 0.6913 - acc: 0.7019    \n",
      "Epoch 87/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6936 - acc: 0.7027    \n",
      "Epoch 88/100\n",
      "39471/39471 [==============================] - 13s - loss: 0.6924 - acc: 0.7023    \n",
      "Epoch 89/100\n",
      "39471/39471 [==============================] - 13s - loss: 0.6926 - acc: 0.7049    \n",
      "Epoch 90/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6930 - acc: 0.7028    \n",
      "Epoch 91/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6935 - acc: 0.6995    \n",
      "Epoch 92/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6915 - acc: 0.6993    \n",
      "Epoch 93/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6943 - acc: 0.7001    \n",
      "Epoch 94/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6935 - acc: 0.7012    \n",
      "Epoch 95/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6904 - acc: 0.7024    \n",
      "Epoch 96/100\n",
      "39471/39471 [==============================] - 13s - loss: 0.6923 - acc: 0.7010    \n",
      "Epoch 97/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6938 - acc: 0.7012    \n",
      "Epoch 98/100\n",
      "39471/39471 [==============================] - 13s - loss: 0.6922 - acc: 0.7036    \n",
      "Epoch 99/100\n",
      "39471/39471 [==============================] - 13s - loss: 0.6932 - acc: 0.7030    \n",
      "Epoch 100/100\n",
      "39471/39471 [==============================] - 12s - loss: 0.6927 - acc: 0.7015    \n",
      "log loss on test k-th fold: 0.654593988205\n",
      "\n",
      "mean testing log loss:\n",
      "0.657431193197 +/- 0.00145155011469\n"
     ]
    }
   ],
   "source": [
    "# define 5-fold cross validation test \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\n",
    "cvscores5 = []\n",
    "i=1\n",
    "\n",
    "scale_stand = StandardScaler()\n",
    "\n",
    "small_scaled_bin = scale_stand.fit_transform(X_train_all_small)\n",
    "\n",
    "for train, test in kfold.split(small_scaled_bin, y_train_all_small):\n",
    "    print(\"fold number:\", i)\n",
    "    model5 = Sequential()\n",
    "    \n",
    "    inshape = small_scaled_bin.shape[1]\n",
    "\n",
    "    #Input layer: \n",
    "    model5.add(Dense(50, input_dim= inshape, activation = 'tanh',         \n",
    "                kernel_regularizer=regularizers.l2(0.001),\n",
    "                #activity_regularizer=regularizers.l1(0.001), \n",
    "                kernel_initializer= initializers.glorot_normal(seed=None)))\n",
    "                     \n",
    "    model5.add(BatchNormalization())\n",
    "    model5.add(Dropout(0.5))     \n",
    "\n",
    "    #One hidden layer: \n",
    "    model5.add(Dense(50,  activation = 'tanh',\n",
    "                kernel_regularizer=regularizers.l2(0.001),\n",
    "                #activity_regularizer=regularizers.l1(0.001), \n",
    "                kernel_initializer= initializers.glorot_normal(seed=None)))\n",
    "    model5.add(BatchNormalization())\n",
    "    model5.add(Dropout(0.5))\n",
    "\n",
    "    #Second hidden layer: \n",
    "    model5.add(Dense(50, activation = 'tanh', \n",
    "                #kernel_regularizer=regularizers.l2(0.01),\n",
    "                #activity_regularizer=regularizers.l1(0.01), \n",
    "                kernel_initializer= initializers.glorot_normal(seed=None)))\n",
    "    model5.add(BatchNormalization())\n",
    "    model5.add(Dropout(0.5))\n",
    "\n",
    "    #Output layer \n",
    "    model5.add(Dense(3, activation='softmax',\n",
    "                #kernel_regularizer=regularizers.l2(0.01),\n",
    "                #activity_regularizer=regularizers.l1(0.01), \n",
    "                kernel_initializer= initializers.glorot_normal(seed=None)))\n",
    "\n",
    "    #Setting up to optimize the weights: \n",
    "    model5.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    categorical_labels = to_categorical(y_train_all_small[train]-1, num_classes=None)\n",
    "    model5.fit(small_scaled_bin[train],categorical_labels, epochs=100, batch_size=10, verbose=1)\n",
    "    \n",
    "    # evaluate the model\n",
    "    categorical_labels_test = to_categorical(y_train_all_small[test]-1, num_classes=None)\n",
    "    scores = model5.evaluate(small_scaled_bin[test], categorical_labels_test, verbose=0)\n",
    "    print(\"log loss on test k-th fold:\", scores[0])\n",
    "    cvscores5.append(scores[0])\n",
    "    \n",
    "    i+=1\n",
    "\n",
    "print()\n",
    "print(\"mean testing log loss:\")\n",
    "print(numpy.mean(cvscores5), \"+/-\", numpy.std(cvscores5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation: Too much regularization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras Cross Val Take 6 (no location columns):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold number: 1\n",
      "Epoch 1/100\n",
      "32891/32891 [==============================] - 13s - loss: 0.7672 - acc: 0.6687    \n",
      "Epoch 2/100\n",
      "32891/32891 [==============================] - 9s - loss: 0.7013 - acc: 0.6944     \n",
      "Epoch 3/100\n",
      "32891/32891 [==============================] - 9s - loss: 0.6906 - acc: 0.6985     \n",
      "Epoch 4/100\n",
      "32891/32891 [==============================] - 9s - loss: 0.6847 - acc: 0.6995     \n",
      "Epoch 5/100\n",
      "32891/32891 [==============================] - 9s - loss: 0.6795 - acc: 0.7017     \n",
      "Epoch 6/100\n",
      "32891/32891 [==============================] - 8s - loss: 0.6771 - acc: 0.7044     \n",
      "Epoch 7/100\n",
      "32891/32891 [==============================] - 8s - loss: 0.6763 - acc: 0.7014     \n",
      "Epoch 8/100\n",
      "32891/32891 [==============================] - 8s - loss: 0.6711 - acc: 0.7049     \n",
      "Epoch 9/100\n",
      "32891/32891 [==============================] - 8s - loss: 0.6727 - acc: 0.7054     \n",
      "Epoch 10/100\n",
      "32891/32891 [==============================] - 8s - loss: 0.6694 - acc: 0.7053     \n",
      "Epoch 11/100\n",
      "32891/32891 [==============================] - 9s - loss: 0.6684 - acc: 0.7058     \n",
      "Epoch 12/100\n",
      "32891/32891 [==============================] - 12s - loss: 0.6657 - acc: 0.7066    \n",
      "Epoch 13/100\n",
      "32891/32891 [==============================] - 12s - loss: 0.6664 - acc: 0.7040    \n",
      "Epoch 14/100\n",
      "32891/32891 [==============================] - 12s - loss: 0.6640 - acc: 0.7070    \n",
      "Epoch 15/100\n",
      "32891/32891 [==============================] - 12s - loss: 0.6647 - acc: 0.7053    \n",
      "Epoch 16/100\n",
      "32891/32891 [==============================] - 12s - loss: 0.6631 - acc: 0.7079    \n",
      "Epoch 17/100\n",
      "32891/32891 [==============================] - 12s - loss: 0.6603 - acc: 0.7105    \n",
      "Epoch 18/100\n",
      "32891/32891 [==============================] - 12s - loss: 0.6619 - acc: 0.7068    \n",
      "Epoch 19/100\n",
      "32891/32891 [==============================] - 12s - loss: 0.6591 - acc: 0.7062    \n",
      "Epoch 20/100\n",
      "32891/32891 [==============================] - 12s - loss: 0.6587 - acc: 0.7071    \n",
      "Epoch 21/100\n",
      "32891/32891 [==============================] - 12s - loss: 0.6609 - acc: 0.7066    \n",
      "Epoch 22/100\n",
      "32891/32891 [==============================] - 12s - loss: 0.6591 - acc: 0.7088    \n",
      "Epoch 23/100\n",
      "32891/32891 [==============================] - 12s - loss: 0.6596 - acc: 0.7079    \n",
      "Epoch 24/100\n",
      "32891/32891 [==============================] - 12s - loss: 0.6578 - acc: 0.7072    \n",
      "Epoch 25/100\n",
      "32891/32891 [==============================] - 12s - loss: 0.6578 - acc: 0.7064    \n",
      "Epoch 26/100\n",
      "32891/32891 [==============================] - 12s - loss: 0.6566 - acc: 0.7085    \n",
      "Epoch 27/100\n",
      "32891/32891 [==============================] - 12s - loss: 0.6578 - acc: 0.7082    \n",
      "Epoch 28/100\n",
      "32891/32891 [==============================] - 12s - loss: 0.6532 - acc: 0.7094    \n",
      "Epoch 29/100\n",
      "32891/32891 [==============================] - 12s - loss: 0.6589 - acc: 0.7072    \n",
      "Epoch 30/100\n",
      "32891/32891 [==============================] - 12s - loss: 0.6545 - acc: 0.7097    \n",
      "Epoch 31/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6556 - acc: 0.7087    \n",
      "Epoch 32/100\n",
      "32891/32891 [==============================] - 12s - loss: 0.6557 - acc: 0.7100    \n",
      "Epoch 33/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6566 - acc: 0.7096    \n",
      "Epoch 34/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6533 - acc: 0.7106    \n",
      "Epoch 35/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6533 - acc: 0.7128    \n",
      "Epoch 36/100\n",
      "32891/32891 [==============================] - 12s - loss: 0.6491 - acc: 0.7112    \n",
      "Epoch 37/100\n",
      "32891/32891 [==============================] - 12s - loss: 0.6512 - acc: 0.7102    \n",
      "Epoch 38/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6517 - acc: 0.7108    \n",
      "Epoch 39/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6498 - acc: 0.7114    \n",
      "Epoch 40/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6510 - acc: 0.7116    \n",
      "Epoch 41/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6475 - acc: 0.7127    \n",
      "Epoch 42/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6482 - acc: 0.7124    \n",
      "Epoch 43/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6492 - acc: 0.7133    \n",
      "Epoch 44/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6509 - acc: 0.7124    \n",
      "Epoch 45/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6518 - acc: 0.7115    \n",
      "Epoch 46/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6505 - acc: 0.7111    \n",
      "Epoch 47/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6473 - acc: 0.7097    \n",
      "Epoch 48/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6483 - acc: 0.7121    \n",
      "Epoch 49/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6484 - acc: 0.7130    \n",
      "Epoch 50/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6489 - acc: 0.7124    \n",
      "Epoch 51/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6482 - acc: 0.7130    \n",
      "Epoch 52/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6509 - acc: 0.7125    \n",
      "Epoch 53/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6482 - acc: 0.7132    \n",
      "Epoch 54/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6492 - acc: 0.7134    \n",
      "Epoch 55/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6466 - acc: 0.7121    \n",
      "Epoch 56/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6491 - acc: 0.7146    \n",
      "Epoch 57/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6480 - acc: 0.7136    \n",
      "Epoch 58/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6452 - acc: 0.7126    \n",
      "Epoch 59/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6482 - acc: 0.7133    \n",
      "Epoch 60/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6468 - acc: 0.7137    \n",
      "Epoch 61/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6451 - acc: 0.7118    \n",
      "Epoch 62/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6464 - acc: 0.7117    \n",
      "Epoch 63/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6440 - acc: 0.7153    \n",
      "Epoch 64/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6484 - acc: 0.7121    \n",
      "Epoch 65/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6452 - acc: 0.7136    \n",
      "Epoch 66/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6432 - acc: 0.7144    \n",
      "Epoch 67/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6480 - acc: 0.7108    \n",
      "Epoch 68/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6453 - acc: 0.7157    \n",
      "Epoch 69/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6470 - acc: 0.7125    \n",
      "Epoch 70/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6454 - acc: 0.7131    \n",
      "Epoch 71/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6436 - acc: 0.7154    \n",
      "Epoch 72/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6452 - acc: 0.7124    \n",
      "Epoch 73/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6436 - acc: 0.7156    \n",
      "Epoch 74/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6450 - acc: 0.7157    \n",
      "Epoch 75/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6442 - acc: 0.7150    \n",
      "Epoch 76/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6440 - acc: 0.7134    \n",
      "Epoch 77/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6432 - acc: 0.7149    \n",
      "Epoch 78/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6450 - acc: 0.7128    \n",
      "Epoch 79/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6414 - acc: 0.7138    \n",
      "Epoch 80/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6426 - acc: 0.7134    \n",
      "Epoch 81/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6422 - acc: 0.7157    \n",
      "Epoch 82/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6431 - acc: 0.7122    \n",
      "Epoch 83/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6425 - acc: 0.7146    \n",
      "Epoch 84/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6419 - acc: 0.7162    \n",
      "Epoch 85/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6441 - acc: 0.7117    \n",
      "Epoch 86/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6416 - acc: 0.7166    \n",
      "Epoch 87/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6441 - acc: 0.7153    \n",
      "Epoch 88/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6409 - acc: 0.7170    \n",
      "Epoch 89/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6441 - acc: 0.7157    \n",
      "Epoch 90/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6397 - acc: 0.7149    \n",
      "Epoch 91/100\n",
      "32891/32891 [==============================] - 11s - loss: 0.6411 - acc: 0.7142    \n",
      "Epoch 92/100\n",
      "24780/32891 [=====================>........] - ETA: 2s - loss: 0.6372 - acc: 0.7172"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-201b5140c992>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mcategorical_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train_all_small\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     \u001b[0mmodel6\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msmall_scaled_bin\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcategorical_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;31m# evaluate the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    843\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    844\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 845\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    846\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1483\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1484\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1485\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1487\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1138\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1140\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1141\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2071\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2072\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m-> 2073\u001b[1;33m                               feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m   2074\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2075\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define 5-fold cross validation test harness\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=7)\n",
    "cvscores6 = []\n",
    "i=1\n",
    "\n",
    "scale_stand = StandardScaler()\n",
    "\n",
    "small_scaled_bin = scale_stand.fit_transform(X_train_all_small[:, :58])\n",
    "\n",
    "for train, test in kfold.split(small_scaled_bin, y_train_all_small):\n",
    "    print(\"fold number:\", i)\n",
    "    model6 = Sequential()\n",
    "    \n",
    "    inshape = small_scaled_bin.shape[1]\n",
    "\n",
    "    #Input layer: \n",
    "    model6.add(Dense(50, input_dim= inshape, activation = 'tanh',         \n",
    "                #kernel_regularizer=regularizers.l2(0.001),\n",
    "                #activity_regularizer=regularizers.l1(0.001), \n",
    "                kernel_initializer= initializers.glorot_normal(seed=None)))\n",
    "                     \n",
    "    model6.add(BatchNormalization())\n",
    "    model6.add(Dropout(0.5))     \n",
    "\n",
    "    #One hidden layer: \n",
    "    model6.add(Dense(50,  activation = 'tanh',\n",
    "                #kernel_regularizer=regularizers.l2(0.001),\n",
    "                #activity_regularizer=regularizers.l1(0.001), \n",
    "                kernel_initializer= initializers.glorot_normal(seed=None)))\n",
    "    model6.add(BatchNormalization())\n",
    "    model6.add(Dropout(0.25))\n",
    "\n",
    "    #Second hidden layer: \n",
    "    model6.add(Dense(50, activation = 'tanh', \n",
    "                #kernel_regularizer=regularizers.l2(0.01),\n",
    "                #activity_regularizer=regularizers.l1(0.01), \n",
    "                kernel_initializer= initializers.glorot_normal(seed=None)))\n",
    "    model6.add(BatchNormalization())\n",
    "    model6.add(Dropout(0.25))\n",
    "\n",
    "    #Output layer \n",
    "    model6.add(Dense(3, activation='softmax',\n",
    "                #kernel_regularizer=regularizers.l2(0.01),\n",
    "                #activity_regularizer=regularizers.l1(0.01), \n",
    "                kernel_initializer= initializers.glorot_normal(seed=None)))\n",
    "\n",
    "    #Setting up to optimize the weights: \n",
    "    model6.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    categorical_labels = to_categorical(y_train_all_small[train]-1, num_classes=None)\n",
    "    model6.fit(small_scaled_bin[train],categorical_labels, epochs=100, batch_size=10, verbose=1)\n",
    "    \n",
    "    # evaluate the model\n",
    "    categorical_labels_test = to_categorical(y_train_all_small[test]-1, num_classes=None)\n",
    "    scores = model6.evaluate(small_scaled_bin[test], categorical_labels_test, verbose=0)\n",
    "    print(\"log loss on test k-th fold:\", scores[0])\n",
    "    cvscores6.append(scores[0])\n",
    "    \n",
    "    i+=1\n",
    "\n",
    "print()\n",
    "print(\"mean testing log loss:\")\n",
    "print(numpy.mean(cvscores6), \"+/-\", numpy.std(cvscores6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation: The locations column adds a lot of information. Log losses are .02 points higher without it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras Cross Val Take 7 (full dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold number: 1\n",
      "Epoch 1/5\n",
      "32891/32891 [==============================] - 26s - loss: 0.8484 - acc: 0.6589    \n",
      "Epoch 2/5\n",
      "32891/32891 [==============================] - 22s - loss: 0.7627 - acc: 0.6945    \n",
      "Epoch 3/5\n",
      "32891/32891 [==============================] - 23s - loss: 0.7551 - acc: 0.6944    \n",
      "Epoch 4/5\n",
      "32891/32891 [==============================] - 23s - loss: 0.7519 - acc: 0.6937    \n",
      "Epoch 5/5\n",
      "32891/32891 [==============================] - 22s - loss: 0.7449 - acc: 0.6938    \n",
      "log loss on test k-th fold: 0.747604794497\n",
      "fold number: 2\n",
      "Epoch 1/5\n",
      "32891/32891 [==============================] - 26s - loss: 0.8606 - acc: 0.6548    \n",
      "Epoch 2/5\n",
      "32891/32891 [==============================] - 22s - loss: 0.7637 - acc: 0.6935    \n",
      "Epoch 3/5\n",
      "32891/32891 [==============================] - 23s - loss: 0.7560 - acc: 0.6941    \n",
      "Epoch 4/5\n",
      "14510/32891 [============>.................] - ETA: 12s - loss: 0.7429 - acc: 0.6977"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-126-706ef5210135>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[0mcategorical_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train_all\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m     \u001b[0mmodel7\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscaled_bin\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcategorical_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;31m# evaluate the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    843\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    844\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 845\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    846\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1483\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1484\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1485\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1487\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1138\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1140\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1141\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2071\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2072\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m-> 2073\u001b[1;33m                               feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m   2074\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2075\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define 5-fold cross validation test harness\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=7)\n",
    "cvscores7 = []\n",
    "i=1\n",
    "\n",
    "scale_stand = StandardScaler()\n",
    "\n",
    "scaled_bin = scale_stand.fit_transform(X_train_all)\n",
    "\n",
    "for train, test in kfold.split(scaled_bin, y_train_all):\n",
    "    print(\"fold number:\", i)\n",
    "    model7 = Sequential()\n",
    "    \n",
    "    #stop early if testing error gets large: \n",
    "    callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "\n",
    "    \n",
    "    inshape = scaled_bin.shape[1]\n",
    "\n",
    "    #Input layer: \n",
    "    model7.add(Dense(20, input_dim= inshape, activation = 'tanh',         \n",
    "                kernel_regularizer=regularizers.l2(0.00001),\n",
    "                activity_regularizer=regularizers.l1(0.00001), \n",
    "                kernel_initializer= initializers.glorot_normal(seed=None)))\n",
    "                     \n",
    "    model7.add(BatchNormalization())\n",
    "    model7.add(Dropout(0.5))     \n",
    "\n",
    "    #One hidden layer: \n",
    "    model7.add(Dense(20,  activation = 'tanh',\n",
    "                kernel_regularizer=regularizers.l2(0.00001),\n",
    "                activity_regularizer=regularizers.l1(0.00001), \n",
    "                kernel_initializer= initializers.glorot_normal(seed=None)))\n",
    "    model7.add(BatchNormalization())\n",
    "    model7.add(Dropout(0.5))\n",
    "\n",
    "    #Second hidden layer: \n",
    "    model7.add(Dense(20, activation = 'tanh', \n",
    "                #kernel_regularizer=regularizers.l2(0.01),\n",
    "                activity_regularizer=regularizers.l1(0.0001), \n",
    "                kernel_initializer= initializers.glorot_normal(seed=None)))\n",
    "    model7.add(BatchNormalization())\n",
    "    model7.add(Dropout(0.5))\n",
    "    \n",
    "    #Third hidden layer: \n",
    "    model7.add(Dense(20, activation = 'tanh', \n",
    "                kernel_regularizer=regularizers.l2(0.00001),\n",
    "                activity_regularizer=regularizers.l1(0.00001), \n",
    "                kernel_initializer= initializers.glorot_normal(seed=None)))\n",
    "    model7.add(BatchNormalization())\n",
    "    model7.add(Dropout(0.5))\n",
    "\n",
    "    #Output layer \n",
    "    model7.add(Dense(3, activation='softmax',\n",
    "                kernel_regularizer=regularizers.l2(0.0001),\n",
    "                activity_regularizer=regularizers.l1(0.00001), \n",
    "                kernel_initializer= initializers.glorot_normal(seed=None)))\n",
    "\n",
    "    #Setting up to optimize the weights: \n",
    "    model7.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    categorical_labels = to_categorical(y_train_all[train]-1, num_classes=None)\n",
    "    model7.fit(scaled_bin[train],categorical_labels, epochs=5, batch_size=10, verbose=1)\n",
    "    \n",
    "    # evaluate the model\n",
    "    categorical_labels_test = to_categorical(y_train_all[test]-1, num_classes=None)\n",
    "    scores = model7.evaluate(scaled_bin[test], categorical_labels_test, verbose=0)\n",
    "    \n",
    "    print(\"log loss on test k-th fold:\", scores[0])\n",
    "    cvscores7.append(scores[0])\n",
    "    \n",
    "    i+=1\n",
    "\n",
    "print()\n",
    "print(\"mean testing log loss:\")\n",
    "print(numpy.mean(cvscores7), \"+/-\", numpy.std(cvscores7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32891, 3)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49337,)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16446, 837)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_bin[test].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16446, 58)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_scaled_bin[test].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras grid search: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using Grid Search to tune the parameters: \n",
    "\n",
    "def create_model(optimizer='rmsprop', init='glorot_uniform'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim= X_train.shape[1], activation='tanh'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(X_train.shape[1], activation='tanh'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(3, activation='sigmoid'))\n",
    "    model.compile(loss='categorical_crossentropy', metrics = ['accuracy'], optimizer= 'sgd')\n",
    "    return model\n",
    " \n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# grid search epochs, batch size and optimizer\n",
    "optimizers = ['rmsprop', 'adam']\n",
    "init = ['glorot_uniform', 'normal', 'uniform']\n",
    "batches = [5, 10, 20]\n",
    "\n",
    "param_grid = dict(optimizer=optimizers, batch_size=batches, init=init)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-364-122aec6f4913>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgrid_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategorical_labels\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    943\u001b[0m             \u001b[0mtrain\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtest\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m         \"\"\"\n\u001b[1;32m--> 945\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    946\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, groups, parameter_iterable)\u001b[0m\n\u001b[0;32m    562\u001b[0m                                   \u001b[0mreturn_times\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m--> 564\u001b[1;33m           \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    565\u001b[0m           for train, test in cv_iter)\n\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 758\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    606\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 608\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    609\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 571\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[0mscore_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mfit_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m             \u001b[0mtrain_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_score\u001b[1;34m(estimator, X_test, y_test, scorer)\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'item'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py\u001b[0m in \u001b[0;36m_passthrough_scorer\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_passthrough_scorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m     \u001b[1;34m\"\"\"Function that wraps estimator.score\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[0;32m    278\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight)\u001b[0m\n\u001b[0;32m    872\u001b[0m                                    \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m                                    \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 874\u001b[1;33m                                    sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    875\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight)\u001b[0m\n\u001b[0;32m   1527\u001b[0m         return self._test_loop(f, ins,\n\u001b[0;32m   1528\u001b[0m                                \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1529\u001b[1;33m                                verbose=verbose)\n\u001b[0m\u001b[0;32m   1530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1531\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_test_loop\u001b[1;34m(self, f, ins, batch_size, verbose)\u001b[0m\n\u001b[0;32m   1254\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1256\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1257\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1258\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2071\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2072\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m-> 2073\u001b[1;33m                               feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m   2074\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2075\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    936\u001b[0m                 ' to a larger type (e.g. int64).')\n\u001b[0;32m    937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 938\u001b[1;33m           \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    939\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    940\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lara\\AppData\\Roaming\\Python\\Python35\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \"\"\"\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#grid_result = grid.fit(X_train, categorical_labels )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Areas of extension:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dynamic sampling to balance the classes"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
